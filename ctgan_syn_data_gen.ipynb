{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPJUMSxQuPRsstgCTGE94En",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Attabeezy/sequential-crm-for-dce/blob/main/ctgan_syn_data_gen.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sdv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iBKrl7DGrasL",
        "outputId": "5c3f8c6e-c7f7-4d9c-8b97-2fbdac4d0b26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sdv\n",
            "  Downloading sdv-1.29.0-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting boto3<2.0.0,>=1.28 (from sdv)\n",
            "  Downloading boto3-1.40.76-py3-none-any.whl.metadata (6.8 kB)\n",
            "Collecting botocore<2.0.0,>=1.31 (from sdv)\n",
            "  Downloading botocore-1.40.76-py3-none-any.whl.metadata (5.9 kB)\n",
            "Requirement already satisfied: cloudpickle>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from sdv) (3.1.2)\n",
            "Requirement already satisfied: graphviz>=0.13.2 in /usr/local/lib/python3.12/dist-packages (from sdv) (0.21)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from sdv) (2.0.2)\n",
            "Requirement already satisfied: pandas>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from sdv) (2.2.2)\n",
            "Requirement already satisfied: tqdm>=4.29 in /usr/local/lib/python3.12/dist-packages (from sdv) (4.67.1)\n",
            "Collecting copulas>=0.12.1 (from sdv)\n",
            "  Downloading copulas-0.12.3-py3-none-any.whl.metadata (9.5 kB)\n",
            "Collecting ctgan>=0.11.0 (from sdv)\n",
            "  Downloading ctgan-0.11.1-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting deepecho>=0.7.0 (from sdv)\n",
            "  Downloading deepecho-0.7.0-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting rdt>=1.18.2 (from sdv)\n",
            "  Downloading rdt-1.18.2-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting sdmetrics>=0.21.0 (from sdv)\n",
            "  Downloading sdmetrics-0.24.0-py3-none-any.whl.metadata (9.3 kB)\n",
            "Requirement already satisfied: platformdirs>=4.0 in /usr/local/lib/python3.12/dist-packages (from sdv) (4.5.0)\n",
            "Requirement already satisfied: pyyaml>=6.0.1 in /usr/local/lib/python3.12/dist-packages (from sdv) (6.0.3)\n",
            "Collecting jmespath<2.0.0,>=0.7.1 (from boto3<2.0.0,>=1.28->sdv)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
            "Collecting s3transfer<0.15.0,>=0.14.0 (from boto3<2.0.0,>=1.28->sdv)\n",
            "  Downloading s3transfer-0.14.0-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.12/dist-packages (from botocore<2.0.0,>=1.31->sdv) (2.9.0.post0)\n",
            "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /usr/local/lib/python3.12/dist-packages (from botocore<2.0.0,>=1.31->sdv) (2.5.0)\n",
            "Requirement already satisfied: plotly>=5.10.0 in /usr/local/lib/python3.12/dist-packages (from copulas>=0.12.1->sdv) (5.24.1)\n",
            "Requirement already satisfied: scipy>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from copulas>=0.12.1->sdv) (1.16.3)\n",
            "Requirement already satisfied: torch>=2.3.0 in /usr/local/lib/python3.12/dist-packages (from ctgan>=0.11.0->sdv) (2.8.0+cu126)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.1.1->sdv) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.1.1->sdv) (2025.2)\n",
            "Requirement already satisfied: scikit-learn>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from rdt>=1.18.2->sdv) (1.6.1)\n",
            "Collecting Faker!=37.11.0,>=17 (from rdt>=1.18.2->sdv)\n",
            "  Downloading faker-38.0.0-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.12/dist-packages (from plotly>=5.10.0->copulas>=0.12.1->sdv) (8.5.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from plotly>=5.10.0->copulas>=0.12.1->sdv) (25.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<2.0.0,>=1.31->sdv) (1.17.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.3.1->rdt>=1.18.2->sdv) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.3.1->rdt>=1.18.2->sdv) (3.6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->ctgan>=0.11.0->sdv) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->ctgan>=0.11.0->sdv) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->ctgan>=0.11.0->sdv) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->ctgan>=0.11.0->sdv) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->ctgan>=0.11.0->sdv) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->ctgan>=0.11.0->sdv) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->ctgan>=0.11.0->sdv) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->ctgan>=0.11.0->sdv) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->ctgan>=0.11.0->sdv) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->ctgan>=0.11.0->sdv) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->ctgan>=0.11.0->sdv) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->ctgan>=0.11.0->sdv) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->ctgan>=0.11.0->sdv) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->ctgan>=0.11.0->sdv) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->ctgan>=0.11.0->sdv) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->ctgan>=0.11.0->sdv) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->ctgan>=0.11.0->sdv) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->ctgan>=0.11.0->sdv) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->ctgan>=0.11.0->sdv) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->ctgan>=0.11.0->sdv) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->ctgan>=0.11.0->sdv) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->ctgan>=0.11.0->sdv) (3.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.3.0->ctgan>=0.11.0->sdv) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.3.0->ctgan>=0.11.0->sdv) (3.0.3)\n",
            "Downloading sdv-1.29.0-py3-none-any.whl (196 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.6/196.6 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading boto3-1.40.76-py3-none-any.whl (139 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.4/139.4 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading botocore-1.40.76-py3-none-any.whl (14.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.2/14.2 MB\u001b[0m \u001b[31m95.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading copulas-0.12.3-py3-none-any.whl (52 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.7/52.7 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ctgan-0.11.1-py3-none-any.whl (25 kB)\n",
            "Downloading deepecho-0.7.0-py3-none-any.whl (27 kB)\n",
            "Downloading rdt-1.18.2-py3-none-any.whl (74 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.3/74.3 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sdmetrics-0.24.0-py3-none-any.whl (198 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m198.3/198.3 kB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading faker-38.0.0-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m76.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Downloading s3transfer-0.14.0-py3-none-any.whl (85 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.7/85.7 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: jmespath, Faker, botocore, s3transfer, rdt, copulas, sdmetrics, deepecho, ctgan, boto3, sdv\n",
            "Successfully installed Faker-38.0.0 boto3-1.40.76 botocore-1.40.76 copulas-0.12.3 ctgan-0.11.1 deepecho-0.7.0 jmespath-1.0.1 rdt-1.18.2 s3transfer-0.14.0 sdmetrics-0.24.0 sdv-1.29.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "CTGAN synthetic data generator using SDV's CTGANSynthesizer and metadata API\n",
        "Updated to use sdv >= 1.0 (sdv.single_table.CTGANSynthesizer)\n",
        "\n",
        "Usage:\n",
        "    pip install pandas scikit-learn sdv openpyxl\n",
        "\n",
        "    python ctgan_sdv_generator_new.py --input MomoStatementReport.xlsx --sheet Sheet1 \\\n",
        "        --n-samples 10000 --epochs 300 --output synthetic.csv\n",
        "\n",
        "Notes:\n",
        "- This script uses SDV's SingleTableMetadata to infer column types. You may still\n",
        "  override columns if autodetection isn't perfect.\n",
        "- The script generates independent rows (CTGAN). For sequence-aware generation,\n",
        "  use a sequence model (not included here).\n",
        "\"\"\"\n",
        "\n",
        "import argparse\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sdv.single_table import CTGANSynthesizer\n",
        "from sdv.metadata import SingleTableMetadata"
      ],
      "metadata": {
        "id": "JwUepJgGriu2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Helpers"
      ],
      "metadata": {
        "id": "1c2EJUkpuOZi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def detect_datetime_columns(df, sample_n=1000):\n",
        "    dt_cols = []\n",
        "    for col in df.columns:\n",
        "        if np.issubdtype(df[col].dtype, np.datetime64):\n",
        "            dt_cols.append(col)\n",
        "            continue\n",
        "        if df[col].dtype == object:\n",
        "            # Ensure sample size does not exceed the number of available non-null elements\n",
        "            non_null_count = len(df[col].dropna())\n",
        "            actual_sample_size = min(non_null_count, sample_n)\n",
        "            if actual_sample_size == 0: # Skip if no non-null values to sample\n",
        "                continue\n",
        "            sample = df[col].dropna().astype(str).sample(actual_sample_size, random_state=1)\n",
        "            parsed = sample.apply(lambda x: pd.to_datetime(x, errors='coerce')).notna().mean()\n",
        "            if parsed > 0.9:\n",
        "                dt_cols.append(col)\n",
        "    return dt_cols\n",
        "\n",
        "\n",
        "def group_rare_levels(s, top_k=50, min_freq=0.01):\n",
        "    freqs = s.value_counts()\n",
        "    n = len(s)\n",
        "    keep = set(freqs.head(top_k).index)\n",
        "    keep2 = set(freqs[freqs / n >= min_freq].index)\n",
        "    final_keep = keep.union(keep2)\n",
        "    return s.where(s.isin(final_keep), other='__OTHER__')"
      ],
      "metadata": {
        "id": "w42ufHC1ujZo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing"
      ],
      "metadata": {
        "id": "czCSNpfzumtD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_dataframe(df):\n",
        "    df = df.copy()\n",
        "\n",
        "    # detect and convert datetime columns\n",
        "    dt_cols = detect_datetime_columns(df)\n",
        "    created_cols = []\n",
        "    for c in dt_cols:\n",
        "        df[c] = pd.to_datetime(df[c], errors='coerce')\n",
        "        # Use .astype('int64') instead of .view('int64') to avoid FutureWarning\n",
        "        df[c + '__unix'] = df[c].astype('int64') // 10**9\n",
        "        df[c + '__hour'] = df[c].dt.hour.astype('Int64')\n",
        "        df[c + '__weekday'] = df[c].dt.dayofweek.astype('Int64')\n",
        "        df[c + '__month'] = df[c].dt.month.astype('Int64')\n",
        "        created_cols += [c + '__unix', c + '__hour', c + '__weekday', c + '__month']\n",
        "        df = df.drop(columns=[c])\n",
        "\n",
        "    # auto-detect categorical columns: object dtype or small-cardinality integer\n",
        "    candidate_cats = []\n",
        "    n = len(df)\n",
        "    # Use pd.api.types.is_integer_dtype for robustness with pandas nullable integer types\n",
        "    import pandas.api.types as ptypes\n",
        "    for col in df.columns:\n",
        "        if df[col].dtype == object:\n",
        "            candidate_cats.append(col)\n",
        "        elif ptypes.is_integer_dtype(df[col].dtype): # Changed from np.issubdtype\n",
        "            nunique = df[col].nunique()\n",
        "            if nunique <= 50 or (nunique / max(1, n)) <= 0.05:\n",
        "                candidate_cats.append(col)\n",
        "\n",
        "    # convert candidate cats to string and group rare levels\n",
        "    for c in candidate_cats:\n",
        "        df[c] = df[c].astype(str)\n",
        "        df[c] = group_rare_levels(df[c])\n",
        "\n",
        "    # Impute missing values: categorical -> '__NA__', numeric -> median\n",
        "    categorical_columns = [c for c in candidate_cats if c in df.columns]\n",
        "    for c in df.columns:\n",
        "        if c in categorical_columns:\n",
        "            df[c] = df[c].fillna('__NA__')\n",
        "        else:\n",
        "            if df[c].isna().any():\n",
        "                try:\n",
        "                    df[c] = df[c].fillna(df[c].median())\n",
        "                except Exception:\n",
        "                    df[c] = df[c].fillna(0)\n",
        "\n",
        "    return df, created_cols, categorical_columns"
      ],
      "metadata": {
        "id": "Rd_Dv9nPupIb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Metadata & CTGAN"
      ],
      "metadata": {
        "id": "ePYzkszOuwL7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_metadata(df, categorical_columns):\n",
        "    \"\"\"Build SDV SingleTableMetadata and annotate detected categorical columns.\n",
        "    Returns metadata object.\n",
        "    \"\"\"\n",
        "    metadata = SingleTableMetadata()\n",
        "    metadata.detect_from_dataframe(df)\n",
        "\n",
        "    # Force categorical columns to be discrete/string in metadata\n",
        "    for c in categorical_columns:\n",
        "        if c in metadata.get_columns():\n",
        "            metadata.update_column(c, sdtype='categorical')\n",
        "    return metadata\n",
        "\n",
        "\n",
        "def train_ctgan_with_metadata(df, metadata, epochs=300):\n",
        "    \"\"\"Train CTGANSynthesizer using provided metadata.\"\"\"\n",
        "    model = CTGANSynthesizer(metadata=metadata, epochs=epochs)\n",
        "    model.fit(df)\n",
        "    return model"
      ],
      "metadata": {
        "id": "U0O0yv3iu0kA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sampling & Evaluation"
      ],
      "metadata": {
        "id": "IOoeBSEvu3Mo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sample_ctgan(model, n_samples=10000, conditions=None):\n",
        "    if conditions:\n",
        "        # SDV supports conditional sampling via sample_conditions in some versions.\n",
        "        # The API expects a list of condition dicts describing desired column values.\n",
        "        # Try direct call; fallback to rejection sampling.\n",
        "        try:\n",
        "            # example: conditions = [{'column': 'tx_type', 'value': 'withdrawal'}]\n",
        "            return model.sample_conditions(conditions)\n",
        "        except Exception:\n",
        "            pool = model.sample(n_samples * 4)\n",
        "            for k, v in conditions.items():\n",
        "                pool = pool[pool[k] == v]\n",
        "            return pool.head(n_samples).reset_index(drop=True)\n",
        "    return model.sample(num_rows=n_samples)\n",
        "\n",
        "\n",
        "def quick_tstr(synth, holdout, label_col='label'):\n",
        "    if label_col not in synth.columns or label_col not in holdout.columns:\n",
        "        print('Label missing; skipping TSTR')\n",
        "        return None\n",
        "    from sklearn.ensemble import RandomForestClassifier\n",
        "    from sklearn.metrics import roc_auc_score\n",
        "\n",
        "    Xs = pd.get_dummies(synth.drop(columns=[label_col]))\n",
        "    ys = synth[label_col].astype(int)\n",
        "    Xr = pd.get_dummies(holdout.drop(columns=[label_col]))\n",
        "    yr = holdout[label_col].astype(int)\n",
        "\n",
        "    for c in Xs.columns.difference(Xr.columns):\n",
        "        Xr[c] = 0\n",
        "    for c in Xr.columns.difference(Xs.columns):\n",
        "        Xs[c] = 0\n",
        "    Xs = Xs[Xr.columns]\n",
        "\n",
        "    clf = RandomForestClassifier(n_estimators=200, random_state=0)\n",
        "    clf.fit(Xs, ys)\n",
        "    preds = clf.predict_proba(Xr)[:,1]\n",
        "    auc = roc_auc_score(yr, preds)\n",
        "    print('TSTR AUC:', auc)\n",
        "    return auc"
      ],
      "metadata": {
        "id": "JPAyYc8_u9xI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CLI"
      ],
      "metadata": {
        "id": "lhDtrWrDvBK3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main(args):\n",
        "    # Load\n",
        "    if args.sheet is None:\n",
        "        df = pd.read_excel(args.input)\n",
        "    else:\n",
        "        df = pd.read_excel(args.input, sheet_name=args.sheet)\n",
        "    print('Loaded', df.shape)\n",
        "\n",
        "    # Preprocess\n",
        "    proc, created_cols, cat_cols = preprocess_dataframe(df)\n",
        "    print('Created time cols:', created_cols)\n",
        "    print('Detected categorical cols:', cat_cols)\n",
        "\n",
        "    # Split for holdout\n",
        "    train_df, holdout_df = train_test_split(proc, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Build metadata\n",
        "    metadata = SingleTableMetadata()\n",
        "    metadata.detect_from_dataframe(train_df)\n",
        "    for c in cat_cols:\n",
        "        try:\n",
        "            metadata.update_column(c, sdtype='categorical')\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    # Train CTGAN\n",
        "    print('Training CTGAN (this may take a few minutes)...')\n",
        "    model = CTGANSynthesizer(metadata=metadata, epochs=args.epochs)\n",
        "    model.fit(train_df)\n",
        "\n",
        "    # Save model via SDV API\n",
        "    try:\n",
        "        model.save(args.model_out)\n",
        "        print('Model saved to', args.model_out)\n",
        "    except Exception:\n",
        "        # fallback: pickle\n",
        "        import pickle\n",
        "        with open(args.model_out, 'wb') as f:\n",
        "            pickle.dump(model, f)\n",
        "        print('Model pickled to', args.model_out)\n",
        "\n",
        "    # Sample\n",
        "    synth = sample_ctgan(model, n_samples=args.n_samples)\n",
        "    synth.to_csv(args.output, index=False)\n",
        "    print('Synthetic saved to', args.output)\n",
        "\n",
        "    # Optional TSTR\n",
        "    if args.label_col:\n",
        "        quick_tstr(synth, holdout_df, label_col=args.label_col)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    import argparse\n",
        "\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--input', required=True)\n",
        "    parser.add_argument('--sheet', default=None)\n",
        "    parser.add_argument('--n-samples', type=int, default=10000)\n",
        "    parser.add_argument('--epochs', type=int, default=300)\n",
        "    parser.add_argument('--output', default='synthetic.csv')\n",
        "    parser.add_argument('--model-out', default='ctgan_sdv_new.pkl')\n",
        "    parser.add_argument('--label-col', default=None)\n",
        "\n",
        "    # For Colab execution, we will create a dummy args object\n",
        "    # You need to replace 'YOUR_INPUT_FILE.xlsx' with the actual path to your file\n",
        "    class DummyArgs:\n",
        "        def __init__(self):\n",
        "            self.input = 'MomoStatementReport.xlsx' # <--- REPLACE WITH YOUR FILE PATH\n",
        "            self.sheet = None\n",
        "            self.n_samples = 10000\n",
        "            self.epochs = 300\n",
        "            self.output = 'synthetic.csv'\n",
        "            self.model_out = 'ctgan_sdv_new.pkl'\n",
        "            self.label_col = None\n",
        "\n",
        "    args_for_colab = DummyArgs()\n",
        "    main(args_for_colab) # Call the main function with the dummy arguments"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7uLP6Qk5uZR8",
        "outputId": "a9496106-c7d1-4f8a-9dc2-f495956f19e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded (105, 16)\n",
            "Created time cols: ['TRANSACTION DATE__unix', 'TRANSACTION DATE__hour', 'TRANSACTION DATE__weekday', 'TRANSACTION DATE__month']\n",
            "Detected categorical cols: ['FROM ACCT', 'FROM NAME', 'FROM NO.', 'TRANS. TYPE', 'TO NO.', 'TO NAME', 'TO ACCT', 'REF', 'OVA', 'TRANSACTION DATE__unix', 'TRANSACTION DATE__hour', 'TRANSACTION DATE__weekday', 'TRANSACTION DATE__month']\n",
            "Training CTGAN (this may take a few minutes)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sdv/single_table/base.py:168: FutureWarning: The 'SingleTableMetadata' is deprecated. Please use the new 'Metadata' class for synthesizers.\n",
            "  warnings.warn(DEPRECATION_MSG, FutureWarning)\n",
            "/usr/local/lib/python3.12/dist-packages/sdv/single_table/base.py:134: UserWarning: We strongly recommend saving the metadata using 'save_to_json' for replicability in future SDV versions.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/ctgan/synthesizers/_utils.py:16: FutureWarning: `cuda` parameter is deprecated and will be removed in a future release. Please use `enable_gpu` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved to ctgan_sdv_new.pkl\n",
            "Synthetic saved to synthetic.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "def main():\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--input', required=True)\n",
        "    parser.add_argument('--sheet', default=None)\n",
        "    parser.add_argument('--n-samples', type=int, default=10000)\n",
        "    parser.add_argument('--epochs', type=int, default=300)\n",
        "    parser.add_argument('--output', default='synthetic.csv')\n",
        "    parser.add_argument('--model-out', default='ctgan_sdv_new.pkl')\n",
        "    parser.add_argument('--label-col', default=None)\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    # Load\n",
        "    if args.sheet is None:\n",
        "        df = pd.read_excel(args.input)\n",
        "    else:\n",
        "        df = pd.read_excel(args.input, sheet_name=args.sheet)\n",
        "    print('Loaded', df.shape)\n",
        "\n",
        "    # Preprocess\n",
        "    proc, created_cols, cat_cols = preprocess_dataframe(df)\n",
        "    print('Created time cols:', created_cols)\n",
        "    print('Detected categorical cols:', cat_cols)\n",
        "\n",
        "    # Split for holdout\n",
        "    train_df, holdout_df = train_test_split(proc, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Build metadata\n",
        "    metadata = SingleTableMetadata()\n",
        "    metadata.detect_from_dataframe(train_df)\n",
        "    for c in cat_cols:\n",
        "        try:\n",
        "            metadata.update_column(c, sdtype='categorical')\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    # Train CTGAN\n",
        "    print('Training CTGAN (this may take a few minutes)...')\n",
        "    model = CTGANSynthesizer(metadata=metadata, epochs=args.epochs)\n",
        "    model.fit(train_df)\n",
        "\n",
        "    # Save model via SDV API\n",
        "    try:\n",
        "        model.save(args.model_out)\n",
        "        print('Model saved to', args.model_out)\n",
        "    except Exception:\n",
        "        # fallback: pickle\n",
        "        import pickle\n",
        "        with open(args.model_out, 'wb') as f:\n",
        "            pickle.dump(model, f)\n",
        "        print('Model pickled to', args.model_out)\n",
        "\n",
        "    # Sample\n",
        "    synth = sample_ctgan(model, n_samples=args.n_samples)\n",
        "    synth.to_csv(args.output, index=False)\n",
        "    print('Synthetic saved to', args.output)\n",
        "\n",
        "    # Optional TSTR\n",
        "    if args.label_col:\n",
        "        quick_tstr(synth, holdout_df, label_col=args.label_col)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "_P2GKBjIwKT0"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6Mbjb5jPwK55"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}