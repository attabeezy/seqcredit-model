{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c2bda65",
   "metadata": {},
   "source": "# Credit Risk Prediction\n\nLendingClub dataset — three approaches to binary default classification.\n\n| Version | Models | Key Features |\n|---------|--------|--------------|\n| `'a'` | LR + XGBoost + ANN | Paper replication, MSE/ROC comparison |\n| `'b'` | ANN + XGBoost + Random Forest | Full EDA, extensive feature engineering, outlier removal |\n| `'c'` | ANN with StratifiedKFold CV | Refined features, cross-validation, threshold analysis |\n\n**To switch versions:** change `VERSION` in the cell below and re-run all cells.\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a81bef4",
   "metadata": {},
   "outputs": [],
   "source": "VERSION = 'c'  # Options: 'a', 'b', 'c'\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e78c7a",
   "metadata": {},
   "outputs": [],
   "source": "import pandas as pd\nimport numpy as np\nimport kagglehub\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Input\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.metrics import AUC\n\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import (\n    accuracy_score, precision_score, recall_score, f1_score,\n    mean_squared_error, confusion_matrix, classification_report,\n    roc_auc_score, roc_curve, auc, precision_recall_curve,\n    ConfusionMatrixDisplay, RocCurveDisplay\n)\nimport xgboost as xgb\nfrom xgboost import XGBClassifier\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom imblearn.over_sampling import RandomOverSampler\n"
  },
  {
   "cell_type": "markdown",
   "id": "828f5aa6",
   "metadata": {},
   "source": "## Data Loading"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d935ac",
   "metadata": {},
   "outputs": [],
   "source": "dataset_path = kagglehub.dataset_download(\"jeandedieunyandwi/lending-club-dataset\")\ndf = pd.read_csv(f\"{dataset_path}/lending_club_loan_two.csv\")\nprint(f\"Dataset shape: {df.shape}\")\nprint(df[\"loan_status\"].value_counts())\n"
  },
  {
   "cell_type": "markdown",
   "id": "006b9524",
   "metadata": {},
   "source": "## Preprocessing & Feature Engineering"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ccf8bf",
   "metadata": {},
   "outputs": [],
   "source": "# ── VERSION a ─────────────────────────────────────────────────────────────\nif VERSION == \"a\":\n    df_clean = df[df[\"loan_status\"].isin([\"Fully Paid\", \"Charged Off\"])].copy()\n    df_clean[\"loan_status_binary\"] = df_clean[\"loan_status\"].apply(lambda x: 0 if x == \"Fully Paid\" else 1)\n\n    X_clean = df_clean.drop(\"loan_status_binary\", axis=1)\n    y_clean = df_clean[\"loan_status_binary\"]\n    X_train_full, X_test_full, y_train_full, y_test_full = train_test_split(\n        X_clean, y_clean, test_size=0.2, random_state=42, stratify=y_clean\n    )\n\n    SAMPLE_SIZE = 60000\n    if len(X_train_full) > SAMPLE_SIZE:\n        train_indices = X_train_full.sample(n=SAMPLE_SIZE, random_state=42).index\n        df_sampled = df_clean.loc[train_indices].copy()\n    else:\n        df_sampled = df_clean.loc[X_train_full.index].copy()\n\n    X_sampled = df_sampled.drop(\"loan_status_binary\", axis=1)\n    y_sampled = df_sampled[\"loan_status_binary\"]\n\n    FEATURE_COLUMNS = [\"loan_amnt\", \"funded_amnt\", \"funded_amnt_inv\", \"term\", \"int_rate\",\n                       \"installment\", \"grade\", \"sub_grade\", \"emp_title\", \"emp_length\",\n                       \"annual_inc\", \"application_type\"]\n    available_cols = [col for col in FEATURE_COLUMNS if col in X_sampled.columns]\n\n    if \"term\" in X_sampled.columns and X_sampled[\"term\"].dtype == \"object\":\n        X_sampled[\"term\"] = X_sampled[\"term\"].str.replace(\" months\", \"\", regex=False).astype(float)\n    if \"int_rate\" in X_sampled.columns and X_sampled[\"int_rate\"].dtype == \"object\":\n        X_sampled[\"int_rate\"] = X_sampled[\"int_rate\"].str.replace(\"%\", \"\", regex=False).astype(float)\n    if \"emp_length\" in X_sampled.columns and X_sampled[\"emp_length\"].dtype == \"object\":\n        X_sampled[\"emp_length\"] = X_sampled[\"emp_length\"].replace({\n            \"< 1 year\": \"0\", \"1 year\": \"1\", \"2 years\": \"2\", \"3 years\": \"3\",\n            \"4 years\": \"4\", \"5 years\": \"5\", \"6 years\": \"6\", \"7 years\": \"7\",\n            \"8 years\": \"8\", \"9 years\": \"9\", \"10+ years\": \"10\", \"n/a\": np.nan\n        }).astype(float)\n\n    categorical_features = [\"grade\", \"application_type\"]\n    final_features = [col for col in available_cols if X_sampled[col].dtype in [\"float64\", \"int64\"]]\n    final_features.extend([c for c in categorical_features if c in X_sampled.columns])\n    X_final = X_sampled[final_features].copy()\n    combined_df = pd.concat([X_final, y_sampled], axis=1).dropna()\n    X_final = combined_df.drop(\"loan_status_binary\", axis=1)\n    y_final = combined_df[\"loan_status_binary\"]\n\n    X_encoded = pd.get_dummies(X_final, columns=[c for c in categorical_features if c in X_final.columns], drop_first=True)\n    X_train, X_test, y_train, y_test = train_test_split(\n        X_encoded, y_final, test_size=0.2, random_state=42, stratify=y_final\n    )\n    scaler = MinMaxScaler()\n    X_train_scaled = scaler.fit_transform(X_train)\n    X_test_scaled  = scaler.transform(X_test)\n    print(f\"Train/Test: {X_train_scaled.shape[0]} / {X_test_scaled.shape[0]} | Features: {X_train_scaled.shape[1]}\")\n\n\n# ── VERSION b ─────────────────────────────────────────────────────────────\nelif VERSION == \"b\":\n    data = df[df[\"loan_status\"].isin([\"Fully Paid\", \"Charged Off\"])].copy()\n\n    # EDA\n    print(f\"Shape: {data.shape}\")\n    print(data[\"loan_status\"].value_counts())\n\n    plt.figure(figsize=(12, 8))\n    sns.heatmap(data.select_dtypes(include=np.number).corr(), annot=False, cmap=\"viridis\")\n    plt.title(\"Correlation Heatmap\"); plt.tight_layout(); plt.show()\n\n    plt.figure(figsize=(15, 6))\n    plt.subplot(1, 2, 1)\n    grade_order = sorted(data.grade.unique().tolist())\n    sns.countplot(x=\"grade\", data=data, hue=\"loan_status\", order=grade_order)\n    plt.title(\"Loan Status by Grade\")\n    plt.subplot(1, 2, 2)\n    sub_grade_order = sorted(data.sub_grade.unique().tolist())\n    g = sns.countplot(x=\"sub_grade\", data=data, hue=\"loan_status\", order=sub_grade_order)\n    g.set_xticklabels(g.get_xticklabels(), rotation=90)\n    plt.title(\"Loan Status by Sub-Grade\")\n    plt.tight_layout(); plt.show()\n\n    # Binary features\n    data[\"pub_rec\"] = data[\"pub_rec\"].apply(lambda x: 0 if x == 0.0 else 1)\n    data[\"mort_acc\"] = data[\"mort_acc\"].apply(lambda x: 0 if x == 0.0 else (1 if x >= 1.0 else x))\n    data[\"pub_rec_bankruptcies\"] = data[\"pub_rec_bankruptcies\"].apply(lambda x: 0 if x == 0.0 else (1 if x >= 1.0 else x))\n\n    # Target: 1=Fully Paid, 0=Charged Off\n    data[\"loan_status\"] = data[\"loan_status\"].map({\"Fully Paid\": 1, \"Charged Off\": 0})\n\n    # Handle missing values\n    data.drop([\"emp_title\", \"emp_length\", \"title\"], axis=1, inplace=True, errors=\"ignore\")\n    data[\"mort_acc\"] = pd.to_numeric(data[\"mort_acc\"], errors=\"coerce\")\n    total_acc_avg = data.groupby(\"total_acc\")[\"mort_acc\"].mean()\n    data[\"mort_acc\"] = data.apply(\n        lambda x: total_acc_avg[x[\"total_acc\"]] if np.isnan(x[\"mort_acc\"]) else x[\"mort_acc\"], axis=1\n    ).round()\n    data.dropna(inplace=True)\n    print(f\"Shape after cleaning: {data.shape}\")\n\n    # Feature engineering\n    data[\"term\"] = data[\"term\"].apply(lambda t: int(t.strip().replace(\" months\", \"\")))\n    data.drop(\"grade\", axis=1, inplace=True)\n    dummies = [\"sub_grade\", \"verification_status\", \"purpose\", \"initial_list_status\", \"application_type\"]\n    data = pd.get_dummies(data, columns=dummies + [\"home_ownership\"], drop_first=True)\n    data[\"zip_code\"] = data[\"address\"].apply(lambda x: x[-5:])\n    data = pd.get_dummies(data, columns=[\"zip_code\"], drop_first=True)\n    data.drop([\"address\", \"issue_d\"], axis=1, inplace=True, errors=\"ignore\")\n    data[\"earliest_cr_line\"] = pd.to_datetime(data[\"earliest_cr_line\"]).dt.year\n    print(f\"Shape after feature engineering: {data.shape}\")\n\n    # Split + outlier removal on train\n    train_b, test_b = train_test_split(data, test_size=0.33, random_state=42)\n    train_b = train_b[\n        (train_b[\"annual_inc\"] <= 250000) & (train_b[\"dti\"] <= 50) &\n        (train_b[\"open_acc\"] <= 40)       & (train_b[\"total_acc\"] <= 80) &\n        (train_b[\"revol_util\"] <= 120)    & (train_b[\"revol_bal\"] <= 250000)\n    ]\n    scaler_b = MinMaxScaler()\n    X_tr_b = np.array(scaler_b.fit_transform(train_b.drop(\"loan_status\", axis=1))).astype(np.float32)\n    y_tr_b = np.array(train_b[\"loan_status\"]).astype(np.float32)\n    X_te_b = np.array(scaler_b.transform(test_b.drop(\"loan_status\", axis=1))).astype(np.float32)\n    y_te_b = np.array(test_b[\"loan_status\"]).astype(np.float32)\n\n    # Balance: undersample majority, upsample minority\n    y_series = pd.Series(y_tr_b)\n    maj_cls = y_series.value_counts().idxmax()\n    min_cls = y_series.value_counts().idxmin()\n    rng = np.random.default_rng(42)\n    maj_idx = y_series[y_series == maj_cls].index.to_numpy()\n    min_idx = y_series[y_series == min_cls].index.to_numpy()\n    maj_under = rng.choice(maj_idx, size=len(maj_idx) // 2, replace=False)\n    min_over  = rng.choice(min_idx, size=len(maj_idx) // 2, replace=True)\n    bal_idx   = rng.permutation(np.concatenate([maj_under, min_over]))\n    X_train_scaled = X_tr_b[bal_idx]\n    y_train        = y_tr_b[bal_idx]\n    X_test_scaled  = X_te_b\n    y_test         = y_te_b\n    scores_dict    = {}\n    print(f\"Balanced train: {X_train_scaled.shape[0]} | Test: {X_test_scaled.shape[0]} | Features: {X_train_scaled.shape[1]}\")\n\n\n# ── VERSION c ─────────────────────────────────────────────────────────────\nelse:  # VERSION == \"c\"\n    df_c = df.copy()\n    df_c[\"loan_status_binary\"] = (df_c[\"loan_status\"] == \"Charged Off\").astype(int)\n    X_all = df_c.drop([\"loan_status\", \"loan_status_binary\"], axis=1)\n    y_all = df_c[\"loan_status_binary\"]\n    X_train_full, _, y_train_full, _ = train_test_split(\n        X_all, y_all, test_size=0.2, random_state=42, stratify=y_all\n    )\n\n    SAMPLE_SIZE = 60000\n    if len(X_train_full) > SAMPLE_SIZE:\n        idx = X_train_full.sample(n=SAMPLE_SIZE, random_state=42).index\n        X = df_c.loc[idx].drop([\"loan_status\", \"loan_status_binary\"], axis=1)\n        y = df_c.loc[idx, \"loan_status_binary\"]\n    else:\n        X, y = X_train_full, y_train_full\n\n    FEATURE_COLUMNS = [\"loan_amnt\", \"funded_amnt\", \"funded_amnt_inv\", \"term\", \"int_rate\",\n                       \"installment\", \"grade\", \"sub_grade\", \"emp_title\", \"emp_length\",\n                       \"annual_inc\", \"application_type\"]\n    X = X[[col for col in FEATURE_COLUMNS if col in X.columns]]\n\n    if \"term\" in X.columns and X[\"term\"].dtype == \"object\":\n        X[\"term\"] = X[\"term\"].str.replace(\" months\", \"\", regex=False).astype(float)\n    if \"int_rate\" in X.columns and X[\"int_rate\"].dtype == \"object\":\n        X[\"int_rate\"] = X[\"int_rate\"].str.replace(\"%\", \"\", regex=False).astype(float)\n    if \"emp_length\" in X.columns and X[\"emp_length\"].dtype == \"object\":\n        emp_map = {\"< 1 year\": \"0\", \"1 year\": \"1\", \"2 years\": \"2\", \"3 years\": \"3\",\n                   \"4 years\": \"4\", \"5 years\": \"5\", \"6 years\": \"6\", \"7 years\": \"7\",\n                   \"8 years\": \"8\", \"9 years\": \"9\", \"10+ years\": \"10\", \"n/a\": np.nan}\n        X[\"emp_length\"] = X[\"emp_length\"].replace(emp_map).astype(float)\n\n    categorical_features = [\"grade\", \"application_type\"]\n    numeric_features = [col for col in X.columns if X[col].dtype in [\"float64\", \"int64\"]]\n    X = X[numeric_features + [c for c in categorical_features if c in X.columns]]\n    combined = pd.concat([X, y], axis=1).dropna()\n    X = combined.drop(\"loan_status_binary\", axis=1)\n    y = combined[\"loan_status_binary\"]\n\n    X = pd.get_dummies(X, columns=[c for c in categorical_features if c in X.columns], drop_first=True)\n    X[\"annual_inc_to_loan_ratio\"] = X[\"annual_inc\"] / (X[\"loan_amnt\"] + 1e-6)\n    selected = [\"loan_amnt\", \"term\", \"int_rate\", \"emp_length\", \"annual_inc\", \"annual_inc_to_loan_ratio\"]\n    ohe_cols = [col for col in X.columns if col.startswith(\"grade_\") or col.startswith(\"application_type_\")]\n    selected += [f for f in ohe_cols if f in X.columns]\n    X = X[[f for f in selected if f in X.columns]]\n\n    maj = y.value_counts().idxmax()\n    min_ = y.value_counts().idxmin()\n    rus = RandomUnderSampler(sampling_strategy={maj: y.value_counts()[min_]}, random_state=42)\n    X_down, y_down = rus.fit_resample(X, y)\n    ros = RandomOverSampler(sampling_strategy={min_: len(y_down[y_down == maj])}, random_state=42)\n    X_balanced, y_balanced = ros.fit_resample(X_down, y_down)\n    combined = pd.concat([X_balanced, y_balanced], axis=1).sample(frac=1, random_state=42).reset_index(drop=True)\n    X_balanced = combined.drop(\"loan_status_binary\", axis=1)\n    y_balanced = combined[\"loan_status_binary\"]\n\n    X_train, X_test, y_train, y_test = train_test_split(\n        X_balanced, y_balanced, test_size=0.2, random_state=42, stratify=y_balanced\n    )\n    scaler = MinMaxScaler()\n    X_train_scaled = scaler.fit_transform(X_train)\n    X_test_scaled  = scaler.transform(X_test)\n    print(f\"Train/Test: {X_train_scaled.shape[0]} / {X_test_scaled.shape[0]} | Features: {X_train_scaled.shape[1]}\")\n"
  },
  {
   "cell_type": "markdown",
   "id": "ad7ed318",
   "metadata": {},
   "source": "## Model Building & Training"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f733402",
   "metadata": {},
   "outputs": [],
   "source": "# ── VERSION a ─────────────────────────────────────────────────────────────\nif VERSION == \"a\":\n    lr_model = LogisticRegression(random_state=42)\n    lr_model.fit(X_train_scaled, y_train)\n    y_pred_lr_proba = lr_model.predict_proba(X_test_scaled)[:, 1]\n    y_pred_lr = (y_pred_lr_proba > 0.5).astype(int)\n    lr_accuracy = accuracy_score(y_test, y_pred_lr)\n    lr_mse = mean_squared_error(y_test, y_pred_lr_proba)\n    print(f\"LR  Accuracy: {lr_accuracy:.4f} | MSE: {lr_mse:.6f}\")\n\n    xgb_model = xgb.XGBClassifier(random_state=42, eval_metric=\"logloss\")\n    xgb_model.fit(X_train_scaled, y_train)\n    y_pred_xgb_proba = xgb_model.predict_proba(X_test_scaled)[:, 1]\n    y_pred_xgb = (y_pred_xgb_proba > 0.5).astype(int)\n    xgb_accuracy = accuracy_score(y_test, y_pred_xgb)\n    xgb_mse = mean_squared_error(y_test, y_pred_xgb_proba)\n    print(f\"XGB Accuracy: {xgb_accuracy:.4f} | MSE: {xgb_mse:.6f}\")\n\n    ann = tf.keras.Sequential([\n        tf.keras.layers.Dense(7, activation=\"relu\", input_shape=(X_train_scaled.shape[1],)),\n        tf.keras.layers.Dense(1, activation=\"sigmoid\")\n    ])\n    ann.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n    ann.fit(X_train_scaled, y_train, epochs=50, batch_size=32,\n            validation_data=(X_test_scaled, y_test), verbose=0)\n    y_pred_ann_proba = ann.predict(X_test_scaled, verbose=0).flatten()\n    y_pred_ann = (y_pred_ann_proba > 0.5).astype(int)\n    ann_accuracy = accuracy_score(y_test, y_pred_ann)\n    ann_mse = mean_squared_error(y_test, y_pred_ann_proba)\n    print(f\"ANN Accuracy: {ann_accuracy:.4f} | MSE: {ann_mse:.6f}\")\n\n\n# ── VERSION b ─────────────────────────────────────────────────────────────\nelif VERSION == \"b\":\n    def print_score(true, pred, label=\"\"):\n        print(f\"\\n{label}\\n\" + \"=\"*50)\n        print(f\"Accuracy: {accuracy_score(true, pred)*100:.2f}%\")\n        print(classification_report(true, pred))\n\n    def plot_training(r):\n        plt.figure(figsize=(12, 4))\n        plt.subplot(1, 2, 1)\n        plt.plot(r.history[\"loss\"], label=\"Loss\")\n        plt.plot(r.history[\"val_loss\"], label=\"Val Loss\")\n        plt.title(\"Loss\"); plt.legend()\n        plt.subplot(1, 2, 2)\n        plt.plot(r.history[\"AUC\"], label=\"AUC\")\n        plt.plot(r.history[\"val_AUC\"], label=\"Val AUC\")\n        plt.title(\"AUC\"); plt.legend()\n        plt.tight_layout(); plt.show()\n\n    def nn_model_b(num_cols, hidden_units, dropout_rates, lr):\n        inp = Input(shape=(num_cols,))\n        x = BatchNormalization()(inp)\n        x = Dropout(dropout_rates[0])(x)\n        for i, units in enumerate(hidden_units):\n            x = Dense(units, activation=\"relu\")(x)\n            x = BatchNormalization()(x)\n            x = Dropout(dropout_rates[i + 1])(x)\n        x = Dense(1, activation=\"sigmoid\")(x)\n        m = Model(inputs=inp, outputs=x)\n        m.compile(optimizer=Adam(lr), loss=\"binary_crossentropy\", metrics=[AUC(name=\"AUC\")])\n        return m\n\n    ann_b = nn_model_b(X_train_scaled.shape[1], [150, 150, 150], [0.1, 0, 0.1, 0], 1e-3)\n    r = ann_b.fit(X_train_scaled, y_train, validation_data=(X_test_scaled, y_test),\n                  epochs=7, batch_size=32, verbose=1)\n    plot_training(r)\n    print_score(y_train, ann_b.predict(X_train_scaled, verbose=0).round(), label=\"ANN Train\")\n    print_score(y_test,  ann_b.predict(X_test_scaled,  verbose=0).round(), label=\"ANN Test\")\n    scores_dict[\"ANNs\"] = {\n        \"Train\": roc_auc_score(y_train, ann_b.predict(X_train_scaled, verbose=0)),\n        \"Test\":  roc_auc_score(y_test,  ann_b.predict(X_test_scaled,  verbose=0))\n    }\n\n    xgb_clf = XGBClassifier(eval_metric=\"logloss\")\n    xgb_clf.fit(X_train_scaled, y_train)\n    print_score(y_train, xgb_clf.predict(X_train_scaled), label=\"XGBoost Train\")\n    print_score(y_test,  xgb_clf.predict(X_test_scaled),  label=\"XGBoost Test\")\n    scores_dict[\"XGBoost\"] = {\n        \"Train\": roc_auc_score(y_train, xgb_clf.predict(X_train_scaled)),\n        \"Test\":  roc_auc_score(y_test,  xgb_clf.predict(X_test_scaled))\n    }\n\n    rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n    rf_clf.fit(X_train_scaled, y_train)\n    print_score(y_train, rf_clf.predict(X_train_scaled), label=\"Random Forest Train\")\n    print_score(y_test,  rf_clf.predict(X_test_scaled),  label=\"Random Forest Test\")\n    scores_dict[\"Random Forest\"] = {\n        \"Train\": roc_auc_score(y_train, rf_clf.predict(X_train_scaled)),\n        \"Test\":  roc_auc_score(y_test,  rf_clf.predict(X_test_scaled))\n    }\n\n\n# ── VERSION c ─────────────────────────────────────────────────────────────\nelse:  # VERSION == \"c\"\n    def build_model_c(input_dim):\n        m = tf.keras.Sequential([\n            tf.keras.layers.Dense(14, activation=\"tanh\", input_shape=(input_dim,)),\n            tf.keras.layers.Dense(7,  activation=\"tanh\"),\n            tf.keras.layers.Dense(1,  activation=\"sigmoid\")\n        ])\n        m.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n        return m\n\n    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n    cv_scores = {\"accuracy\": [], \"macro_f1\": [], \"recall_class_1\": [], \"roc_auc\": []}\n\n    for fold, (tr_idx, val_idx) in enumerate(skf.split(X_train_scaled, y_train)):\n        X_cv_tr, X_cv_val = X_train_scaled[tr_idx], X_train_scaled[val_idx]\n        y_cv_tr, y_cv_val = y_train.iloc[tr_idx],   y_train.iloc[val_idx]\n        m = build_model_c(X_train_scaled.shape[1])\n        m.fit(X_cv_tr, y_cv_tr, epochs=50, batch_size=32, verbose=0)\n        proba = m.predict(X_cv_val, verbose=0).flatten()\n        pred  = (proba > 0.5).astype(int)\n        cv_scores[\"accuracy\"].append(accuracy_score(y_cv_val, pred))\n        cv_scores[\"macro_f1\"].append(f1_score(y_cv_val, pred, average=\"macro\"))\n        cv_scores[\"recall_class_1\"].append(recall_score(y_cv_val, pred, pos_label=1))\n        cv_scores[\"roc_auc\"].append(roc_auc_score(y_cv_val, proba))\n        print(f\"Fold {fold+1} ROC-AUC: {cv_scores[\"roc_auc\"][-1]:.4f}\")\n\n    final_model = build_model_c(X_train_scaled.shape[1])\n    final_model.fit(X_train_scaled, y_train, epochs=50, batch_size=32, verbose=0)\n    y_pred_proba = final_model.predict(X_test_scaled, verbose=0).flatten()\n    y_pred = (y_pred_proba > 0.5).astype(int)\n    print(\"Final model trained.\")\n"
  },
  {
   "cell_type": "markdown",
   "id": "404098c7",
   "metadata": {},
   "source": "## Evaluation"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e29ea0c",
   "metadata": {},
   "outputs": [],
   "source": "# ── VERSION a ─────────────────────────────────────────────────────────────\nif VERSION == \"a\":\n    fpr_lr,  tpr_lr,  _ = roc_curve(y_test, y_pred_lr_proba)\n    fpr_xgb, tpr_xgb, _ = roc_curve(y_test, y_pred_xgb_proba)\n    fpr_ann, tpr_ann, _ = roc_curve(y_test, y_pred_ann_proba)\n\n    plt.figure(figsize=(8, 6))\n    plt.plot(fpr_lr,  tpr_lr,  label=f\"LR  (AUC={auc(fpr_lr,  tpr_lr):.4f})\")\n    plt.plot(fpr_xgb, tpr_xgb, label=f\"XGB (AUC={auc(fpr_xgb, tpr_xgb):.4f})\")\n    plt.plot(fpr_ann, tpr_ann, label=f\"ANN (AUC={auc(fpr_ann, tpr_ann):.4f})\")\n    plt.plot([0,1],[0,1],\"k--\")\n    plt.xlabel(\"FPR\"); plt.ylabel(\"TPR\"); plt.title(\"ROC Curves\")\n    plt.legend(); plt.show()\n\n    print(pd.DataFrame({\"Model\": [\"ANN\", \"LR\"], \"MSE\": [ann_mse, lr_mse]}).to_string(index=False))\n\n    for name, y_pred, y_proba in [(\"LR\", y_pred_lr, y_pred_lr_proba),\n                                   (\"XGBoost\", y_pred_xgb, y_pred_xgb_proba),\n                                   (\"ANN\", y_pred_ann, y_pred_ann_proba)]:\n        cm = confusion_matrix(y_test, y_pred)\n        plt.figure(figsize=(4, 3))\n        sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n                    xticklabels=[\"Fully Paid\", \"Charged Off\"],\n                    yticklabels=[\"Fully Paid\", \"Charged Off\"])\n        plt.title(f\"Confusion Matrix — {name}\")\n        plt.xlabel(\"Predicted\"); plt.ylabel(\"Actual\"); plt.show()\n        print(f\"\\nClassification Report — {name}:\")\n        print(classification_report(y_test, y_pred, zero_division=0))\n\n\n# ── VERSION b ─────────────────────────────────────────────────────────────\nelif VERSION == \"b\":\n    scores_df = pd.DataFrame(scores_dict)\n    print(\"\\nROC AUC Summary (Train / Test):\")\n    print(scores_df.to_string())\n\n    scores_df.T.plot(kind=\"bar\", figsize=(8, 5))\n    plt.title(\"ROC AUC — Train vs Test\")\n    plt.xlabel(\"Model\"); plt.ylabel(\"ROC AUC\")\n    plt.xticks(rotation=30); plt.legend(title=\"Dataset\")\n    plt.tight_layout(); plt.show()\n\n    for name, clf in [(\"XGBoost\", xgb_clf), (\"Random Forest\", rf_clf)]:\n        ConfusionMatrixDisplay.from_estimator(\n            clf, X_test_scaled, y_test, cmap=\"Blues\",\n            display_labels=[\"Charged Off\", \"Fully Paid\"]\n        )\n        plt.title(f\"{name} — Confusion Matrix\"); plt.show()\n\n    disp = RocCurveDisplay.from_estimator(xgb_clf, X_test_scaled, y_test, name=\"XGBoost\")\n    RocCurveDisplay.from_estimator(rf_clf, X_test_scaled, y_test, ax=disp.ax_, name=\"Random Forest\")\n    plt.title(\"ROC Curves\"); plt.show()\n\n\n# ── VERSION c ─────────────────────────────────────────────────────────────\nelse:  # VERSION == \"c\"\n    print(\"Cross-Validation Results:\")\n    for metric, scores in cv_scores.items():\n        print(f\"  {metric:20s}: {np.mean(scores):.4f} ± {np.std(scores):.4f}\")\n\n    print(\"\\nTest Set Results:\")\n    print(classification_report(y_test, y_pred, target_names=[\"Fully Paid\", \"Charged Off\"]))\n\n    fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n\n    fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n    axes[0, 0].plot(fpr, tpr, label=f\"AUC={auc(fpr, tpr):.4f}\", linewidth=2)\n    axes[0, 0].plot([0,1],[0,1],\"k--\", label=\"Random\")\n    axes[0, 0].set_xlabel(\"FPR\"); axes[0, 0].set_ylabel(\"TPR\")\n    axes[0, 0].set_title(\"ROC Curve\"); axes[0, 0].legend(); axes[0, 0].grid(alpha=0.3)\n\n    precision, recall, _ = precision_recall_curve(y_test, y_pred_proba)\n    no_skill = float(y_test.mean())\n    axes[0, 1].plot([0,1],[no_skill,no_skill],\"--\",label=\"Baseline\")\n    axes[0, 1].plot(recall, precision, label=\"Model\", linewidth=2)\n    axes[0, 1].set_xlabel(\"Recall\"); axes[0, 1].set_ylabel(\"Precision\")\n    axes[0, 1].set_title(\"Precision-Recall Curve\"); axes[0, 1].legend(); axes[0, 1].grid(alpha=0.3)\n\n    cm = confusion_matrix(y_test, y_pred)\n    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", ax=axes[1, 0],\n                xticklabels=[\"Fully Paid\",\"Charged Off\"],\n                yticklabels=[\"Fully Paid\",\"Charged Off\"])\n    axes[1, 0].set_xlabel(\"Predicted\"); axes[1, 0].set_ylabel(\"Actual\")\n    axes[1, 0].set_title(\"Confusion Matrix\")\n\n    thresholds = np.linspace(0, 1, 100)\n    p_scores, r_scores, f1_t, mf1_t = [], [], [], []\n    for t in thresholds:\n        yp = (y_pred_proba > t).astype(int)\n        p_scores.append(precision_score(y_test, yp, zero_division=0))\n        r_scores.append(recall_score(y_test, yp, zero_division=0))\n        f1_t.append(f1_score(y_test, yp, zero_division=0))\n        mf1_t.append(f1_score(y_test, yp, average=\"macro\", zero_division=0))\n    axes[1, 1].plot(thresholds, p_scores, label=\"Precision\", linewidth=2)\n    axes[1, 1].plot(thresholds, r_scores, label=\"Recall\",    linewidth=2)\n    axes[1, 1].plot(thresholds, f1_t,     label=\"F1\",        linewidth=2)\n    axes[1, 1].plot(thresholds, mf1_t,    label=\"Macro-F1\",  linewidth=2)\n    axes[1, 1].axvline(0.5, color=\"grey\", linestyle=\"--\", alpha=0.7)\n    axes[1, 1].set_xlabel(\"Threshold\"); axes[1, 1].set_ylabel(\"Score\")\n    axes[1, 1].set_title(\"Threshold Analysis\"); axes[1, 1].legend(); axes[1, 1].grid(alpha=0.3)\n\n    plt.tight_layout(); plt.show()\n"
  }
 ]
}