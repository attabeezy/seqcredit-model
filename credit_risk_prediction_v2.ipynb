{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPFLHR/5SiSFXlsz6fDn3sn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Attabeezy/sequential-crm-for-dce/blob/main/credit_risk_prediction_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "k-F2lZF6v5aZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15e2fbe6-759e-48a8-d274-8d04a28d654b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "CREDIT RISK PREDICTION - COMPLETE IMPLEMENTATION\n",
            "Phase 1: Data Balancing | Phase 2: Cross-Validation | Phase 3: Enhanced Metrics\n",
            "================================================================================\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Complete Integration: Phase 1 + Phase 2 + Phase 3\n",
        "Credit Risk Prediction - Final Implementation\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import kagglehub\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import xgboost as xgb\n",
        "import tensorflow as tf\n",
        "import warnings\n",
        "import os\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Create results directory\n",
        "os.makedirs('./results', exist_ok=True)\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"CREDIT RISK PREDICTION - COMPLETE IMPLEMENTATION\")\n",
        "print(\"Phase 1: Data Balancing | Phase 2: Cross-Validation | Phase 3: Enhanced Metrics\")\n",
        "print(\"=\"*80 + \"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# STEP 1: DATA LOADING AND PREPROCESSING"
      ],
      "metadata": {
        "id": "zuAtgaCIxi2D"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "71f10f3c"
      },
      "source": [
        "def create_binary_target(df, positive_class):\n",
        "    \"\"\"\n",
        "    Creates a binary target variable from 'loan_status'.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The input DataFrame with 'loan_status'.\n",
        "        positive_class (str): The value in 'loan_status' to be considered the positive class (1).\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing:\n",
        "            - pd.DataFrame: The DataFrame with the new 'loan_status_binary' column.\n",
        "            - dict: A dictionary mapping original loan status values to their binary encoding.\n",
        "    \"\"\"\n",
        "    df_clean = df.copy()\n",
        "    # Map the loan status to a binary target\n",
        "    # 1 for the positive class, 0 for others\n",
        "    df_clean['loan_status_binary'] = df_clean['loan_status'].apply(\n",
        "        lambda x: 1 if x == positive_class else 0\n",
        "    )\n",
        "\n",
        "    # Create encoding info\n",
        "    unique_statuses = df_clean['loan_status'].unique()\n",
        "    encoding_info = {status: (1 if status == positive_class else 0) for status in unique_statuses}\n",
        "\n",
        "    return df_clean, encoding_info"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download dataset\n",
        "dataset_path = kagglehub.dataset_download('jeandedieunyandwi/lending-club-dataset')\n",
        "file_path = f\"{dataset_path}/lending_club_loan_two.csv\"\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "print(f\"  Dataset shape: {df.shape}\")\n",
        "\n",
        "# PHASE 1: Create binary target\n",
        "df_clean, encoding_info = create_binary_target(df, positive_class='Charged Off')\n",
        "\n",
        "# Sample and preprocess (same as before)\n",
        "SAMPLE_SIZE = 60000\n",
        "X_clean = df_clean.drop(['loan_status', 'loan_status_binary'], axis=1)\n",
        "y_clean = df_clean['loan_status_binary']\n",
        "\n",
        "X_train_full, X_test_full, y_train_full, y_test_full = train_test_split(\n",
        "    X_clean, y_clean, test_size=0.2, random_state=42, stratify=y_clean\n",
        ")\n",
        "\n",
        "if len(X_train_full) > SAMPLE_SIZE:\n",
        "    train_indices = X_train_full.sample(n=SAMPLE_SIZE, random_state=42).index\n",
        "    df_sampled = df_clean.loc[train_indices].copy()\n",
        "else:\n",
        "    df_sampled = df_clean.loc[X_train_full.index].copy()\n",
        "\n",
        "X_sampled = df_sampled.drop(['loan_status', 'loan_status_binary'], axis=1)\n",
        "y_sampled = df_sampled['loan_status_binary']\n",
        "\n",
        "# Feature engineering\n",
        "FEATURE_COLUMNS = [\n",
        "    'loan_amnt', 'funded_amnt', 'funded_amnt_inv', 'term', 'int_rate',\n",
        "    'installment', 'grade', 'sub_grade', 'emp_title', 'emp_length', 'annual_inc', 'application_type'\n",
        "]\n",
        "\n",
        "available_cols = [col for col in FEATURE_COLUMNS if col in X_sampled.columns]\n",
        "\n",
        "# Clean string columns\n",
        "if 'term' in X_sampled.columns and X_sampled['term'].dtype == 'object':\n",
        "    X_sampled['term'] = X_sampled['term'].str.replace(' months', '', regex=False).astype(float)\n",
        "if 'int_rate' in X_sampled.columns and X_sampled['int_rate'].dtype == 'object':\n",
        "    X_sampled['int_rate'] = X_sampled['int_rate'].str.replace('%', '', regex=False).astype(float)\n",
        "if 'emp_length' in X_sampled.columns and X_sampled['emp_length'].dtype == 'object':\n",
        "    X_sampled['emp_length'] = X_sampled['emp_length'].replace({\n",
        "        '< 1 year': '0', '1 year': '1', '2 years': '2', '3 years': '3', '4 years': '4',\n",
        "        '5 years': '5', '6 years': '6', '7 years': '7', '8 years': '8', '9 years': '9',\n",
        "        '10+ years': '10', 'n/a': np.nan\n",
        "    }).astype(float)\n",
        "\n",
        "# Select features and one-hot encode\n",
        "final_features = [col for col in available_cols if X_sampled[col].dtype in ['float64', 'int64']]\n",
        "categorical_features = ['grade', 'application_type']\n",
        "final_features.extend([c for c in categorical_features if c in X_sampled.columns])\n",
        "\n",
        "X_final = X_sampled[final_features].copy()\n",
        "y_final = y_sampled.copy()\n",
        "\n",
        "combined_df = pd.concat([X_final, y_final], axis=1)\n",
        "combined_df.dropna(inplace=True)\n",
        "\n",
        "X_final = combined_df.drop('loan_status_binary', axis=1)\n",
        "y_final = combined_df['loan_status_binary']\n",
        "\n",
        "X_encoded = pd.get_dummies(X_final, columns=[c for c in categorical_features if c in X_final.columns], drop_first=True)\n",
        "X = X_encoded.copy()\n",
        "y = y_final.copy()\n",
        "\n",
        "print(f\"  Final data: {X.shape[0]} samples, {X.shape[1]} features\")\n",
        "# print(\"  ✓ Data preprocessing complete\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UVxefgPVw1w2",
        "outputId": "714bb39e-48a8-4717-877b-4c5fc7054a1c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Colab cache for faster access to the 'lending-club-dataset' dataset.\n",
            "  Dataset shape: (396030, 27)\n",
            "  Final data: 57265 samples, 14 features\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# STEP 2: SELECT BEST STRATEGY (or test multiple)"
      ],
      "metadata": {
        "id": "5AAREejEx4n5"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d2743b47"
      },
      "source": [
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def prepare_data_with_balancing(X, y, strategy='downsample_upsample', ratio=None, random_state=None):\n",
        "    \"\"\"\n",
        "    Applies different balancing strategies to the dataset.\n",
        "\n",
        "    Args:\n",
        "        X (pd.DataFrame): Feature DataFrame.\n",
        "        y (pd.Series): Target Series.\n",
        "        strategy (str): Balancing strategy ('downsample_upsample', 'ratio_based').\n",
        "        ratio (tuple, optional): Desired ratio of minority to majority class after balancing\n",
        "                                  if strategy is 'ratio_based'. Defaults to None.\n",
        "        random_state (int, optional): Random state for reproducibility. Defaults to None.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing:\n",
        "            - pd.DataFrame: Balanced feature DataFrame.\n",
        "            - pd.Series: Balanced target Series.\n",
        "            - dict: Information about the balancing applied.\n",
        "    \"\"\"\n",
        "    X_resampled, y_resampled = X.copy(), y.copy()\n",
        "    balance_info = {\"strategy_applied\": strategy}\n",
        "\n",
        "    if strategy == 'downsample_upsample':\n",
        "        # Separate majority and minority classes\n",
        "        majority_class = y_resampled.value_counts().idxmax()\n",
        "        minority_class = y_resampled.value_counts().idxmin()\n",
        "\n",
        "        # Downsample majority class\n",
        "        rus = RandomUnderSampler(sampling_strategy={majority_class: y_resampled.value_counts()[minority_class]}, random_state=random_state)\n",
        "        X_downsampled, y_downsampled = rus.fit_resample(X_resampled, y_resampled)\n",
        "\n",
        "        # Upsample minority class to match downsampled majority size (which is minority count)\n",
        "        ros = RandomOverSampler(sampling_strategy={minority_class: len(y_downsampled[y_downsampled == majority_class])}, random_state=random_state)\n",
        "        X_resampled, y_resampled = ros.fit_resample(X_downsampled, y_downsampled)\n",
        "\n",
        "        # Shuffle the combined data\n",
        "        combined_df = pd.concat([X_resampled, y_resampled], axis=1)\n",
        "        combined_df = combined_df.sample(frac=1, random_state=random_state).reset_index(drop=True)\n",
        "\n",
        "        X_resampled = combined_df.drop(y_resampled.name, axis=1)\n",
        "        y_resampled = combined_df[y_resampled.name]\n",
        "\n",
        "        balance_info['original_counts'] = y.value_counts().to_dict()\n",
        "        balance_info['resampled_counts'] = y_resampled.value_counts().to_dict()\n",
        "\n",
        "    elif strategy == 'ratio_based' and ratio is not None:\n",
        "        # Assuming ratio is (minority, majority) e.g., (0.6, 0.4) for 60% minority\n",
        "        # This strategy would typically involve SMOTE or similar, but for simplicity,\n",
        "        # let's assume it defines target sizes based on the original minority count\n",
        "        # target_minority = original_minority_count * (ratio[0] / original_minority_ratio)\n",
        "        # target_majority = target_minority * (ratio[1] / ratio[0])\n",
        "\n",
        "        # A simpler interpretation for demonstration: SMOTE the minority class to a certain ratio\n",
        "        # Note: This requires defining which class is minority/majority based on original data\n",
        "        from imblearn.over_sampling import SMOTE\n",
        "\n",
        "        original_counts = y.value_counts()\n",
        "        minority_class = original_counts.idxmin()\n",
        "        majority_class = original_counts.idxmax()\n",
        "        minority_count = original_counts[minority_class]\n",
        "        majority_count = original_counts[majority_class]\n",
        "\n",
        "        # Target number of minority samples after oversampling\n",
        "        target_minority_count = int(majority_count * (ratio[0] / ratio[1]))\n",
        "\n",
        "        if target_minority_count <= minority_count:\n",
        "             print(f\"Warning: Target minority count ({target_minority_count}) is less than or equal to original minority count ({minority_count}). No oversampling applied.\")\n",
        "             X_resampled, y_resampled = X.copy(), y.copy() # No change\n",
        "             balance_info['note'] = 'No oversampling as target ratio is not higher than original'\n",
        "        else:\n",
        "            sampling_strategy = {minority_class: target_minority_count, majority_class: majority_count}\n",
        "            smote = SMOTE(sampling_strategy=sampling_strategy, random_state=random_state)\n",
        "            X_resampled, y_resampled = smote.fit_resample(X, y)\n",
        "            balance_info['original_counts'] = original_counts.to_dict()\n",
        "            balance_info['resampled_counts'] = y_resampled.value_counts().to_dict()\n",
        "\n",
        "    else:\n",
        "        print(f\"Warning: Unknown strategy '{strategy}' or missing ratio for 'ratio_based'. No balancing applied.\")\n",
        "        balance_info['note'] = 'No balancing applied due to invalid strategy or ratio'\n",
        "\n",
        "\n",
        "    return X_resampled, y_resampled, balance_info"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For this demo, we'll use the best strategy from Phase 1+2\n",
        "# In practice, you'd run all strategies and pick the best one\n",
        "BEST_STRATEGY = 'downsample_upsample'  # Or '60-40', '70-30', etc.\n",
        "BEST_RATIO = None  # (0.6, 0.4) if using ratio_based\n",
        "\n",
        "print(f\"  Using strategy: {BEST_STRATEGY}\")\n",
        "\n",
        "# PHASE 1: Apply balancing\n",
        "X_balanced, y_balanced, balance_info = prepare_data_with_balancing(\n",
        "    X, y,\n",
        "    strategy=BEST_STRATEGY,\n",
        "    ratio=BEST_RATIO,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Split into train and test\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_balanced, y_balanced, test_size=0.2, random_state=42, stratify=y_balanced\n",
        ")\n",
        "\n",
        "# Scale features\n",
        "scaler = MinMaxScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "print(f\"  Train set: {len(y_train):,} samples\")\n",
        "print(f\"  Test set: {len(y_test):,} samples\")\n",
        "print(\"  ✓ Balancing and scaling complete\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gXS_mSrLw-li",
        "outputId": "6dd3473d-24f4-4040-d711-5712f457c3cf"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Using strategy: downsample_upsample\n",
            "  Train set: 17,686 samples\n",
            "  Test set: 4,422 samples\n",
            "  ✓ Balancing and scaling complete\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# STEP 3: TRAIN MODELS WITH CROSS-VALIDATION"
      ],
      "metadata": {
        "id": "Ep7ZWpLfyDYD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Storage for results\n",
        "all_models = {}\n",
        "all_cv_results = {}"
      ],
      "metadata": {
        "id": "lJ652kOfxEmu"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Logistic Regression"
      ],
      "metadata": {
        "id": "wqx_i8NvyNif"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1392f445"
      },
      "source": [
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def train_and_evaluate_with_cv(model, X, y, cv_folds=5, random_state=None, model_name=\"\", verbose=False):\n",
        "    \"\"\"\n",
        "    Trains and evaluates a model using Stratified K-Fold Cross-Validation,\n",
        "    calculating enhanced metrics relevant to credit risk (Macro-F1, Class 1 Recall, ROC-AUC).\n",
        "\n",
        "    Args:\n",
        "        model: The scikit-learn model to train.\n",
        "        X (np.ndarray or pd.DataFrame): The feature data.\n",
        "        y (np.ndarray or pd.Series): The target data.\n",
        "        cv_folds (int): Number of cross-validation folds. Defaults to 5.\n",
        "        random_state (int, optional): Random state for reproducibility. Defaults to None.\n",
        "        model_name (str): Name of the model for printing. Defaults to \"\".\n",
        "        verbose (bool): Whether to print results for each fold. Defaults to False.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing:\n",
        "            - dict: Dictionary of mean and std deviation for each metric across folds.\n",
        "            - list: List of dictionaries containing detailed results for each fold.\n",
        "    \"\"\"\n",
        "    skf = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=random_state)\n",
        "    fold_results = []\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"\\n--- {model_name} Cross-Validation ---\")\n",
        "\n",
        "    for fold, (train_index, val_index) in enumerate(skf.split(X, y)):\n",
        "        start_time = time.time()\n",
        "\n",
        "        # Use integer-based indexing for both X and y\n",
        "        # Check if X and y are pandas DataFrames/Series and use .iloc, otherwise use standard numpy indexing\n",
        "        if isinstance(X, (pd.DataFrame, pd.Series)):\n",
        "             X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
        "        else:\n",
        "             X_train, X_val = X[train_index], X[val_index]\n",
        "\n",
        "        if isinstance(y, (pd.DataFrame, pd.Series)):\n",
        "             y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
        "        else:\n",
        "             y_train, y_val = y[train_index], y[val_index]\n",
        "\n",
        "        # Train the model\n",
        "        # Handle potential use_label_encoder warning for XGBoost\n",
        "        if 'use_label_encoder' in model.get_params().keys() and model.__class__.__name__ == 'XGBClassifier':\n",
        "             # eval_metric is passed during model initialization, not fit\n",
        "             model.fit(X_train, y_train)\n",
        "        elif model.__class__.__name__ == 'Sequential': # Keras model\n",
        "             # Keras models handle their own training loop\n",
        "             model.fit(X_train, y_train, epochs=50, batch_size=32, verbose=0) # Using values from user's ANN cell\n",
        "        else:\n",
        "            model.fit(X_train, y_train)\n",
        "\n",
        "        # Predict probabilities for AUC and PR curves, and class 1 recall\n",
        "        if hasattr(model, 'predict_proba'):\n",
        "            y_pred_proba = model.predict_proba(X_val)[:, 1]\n",
        "        elif hasattr(model, 'predict'): # For Keras models\n",
        "             y_pred_proba = model.predict(X_val).flatten()\n",
        "        else:\n",
        "            y_pred_proba = None\n",
        "            if verbose:\n",
        "                print(f\"Fold {fold+1}: Model does not support probability prediction.\")\n",
        "\n",
        "        # Predict class labels (using default threshold 0.5 for standard metrics)\n",
        "        # y_pred = model.predict(X_val) # This line is not needed if using y_pred_proba\n",
        "        if hasattr(model, 'predict_proba') or hasattr(model, 'predict'):\n",
        "             y_pred = (y_pred_proba > 0.5).astype(int)\n",
        "\n",
        "\n",
        "        # Calculate metrics\n",
        "        accuracy = accuracy_score(y_val, y_pred)\n",
        "        precision = precision_score(y_val, y_pred, average='binary', zero_division=0)\n",
        "        recall = recall_score(y_val, y_pred, average='binary', zero_division=0)\n",
        "        f1 = f1_score(y_val, y_pred, average='binary', zero_division=0)\n",
        "\n",
        "        # Enhanced Metrics\n",
        "        # Macro F1 is the arithmetic mean of the F1 score for each class.\n",
        "        macro_f1 = f1_score(y_val, y_pred, average='macro', zero_division=0)\n",
        "        # Recall for class 1 (Charged Off)\n",
        "        recall_class_1 = recall_score(y_val, y_pred, pos_label=1, zero_division=0)\n",
        "        # ROC-AUC Score (requires probabilities)\n",
        "        roc_auc = roc_auc_score(y_val, y_pred_proba) if y_pred_proba is not None else np.nan\n",
        "\n",
        "        end_time = time.time()\n",
        "        fold_time = end_time - start_time\n",
        "\n",
        "        fold_results.append({\n",
        "            'fold': fold + 1,\n",
        "            'accuracy': accuracy,\n",
        "            'precision': precision,\n",
        "            'recall': recall,\n",
        "            'f1': f1,\n",
        "            'macro_f1': macro_f1,\n",
        "            'recall_class_1': recall_class_1,\n",
        "            'roc_auc': roc_auc,\n",
        "            'time': fold_time,\n",
        "            'y_true': y_val,\n",
        "            'y_pred': y_pred,\n",
        "            'y_pred_proba': y_pred_proba\n",
        "        })\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"  Fold {fold+1} completed in {fold_time:.2f} seconds.\")\n",
        "            print(f\"    Accuracy: {accuracy:.4f}, Macro-F1: {macro_f1:.4f}, Recall (Class 1): {recall_class_1:.4f}, ROC-AUC: {roc_auc:.4f}\")\n",
        "\n",
        "    # Calculate mean and std deviation across folds\n",
        "    mean_metrics = {}\n",
        "    std_metrics = {}\n",
        "    for metric in ['accuracy', 'precision', 'recall', 'f1', 'macro_f1', 'recall_class_1', 'roc_auc', 'time']:\n",
        "        mean_metrics[f'{metric}_mean'] = np.mean([res[metric] for res in fold_results])\n",
        "        std_metrics[f'{metric}_std'] = np.std([res[metric] for res in fold_results])\n",
        "\n",
        "    cv_summary = {**mean_metrics, **std_metrics}\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"\\n--- Average Results for {model_name} ---\")\n",
        "        print(f\"  Accuracy: {cv_summary['accuracy_mean']:.4f} ± {cv_summary['accuracy_std']:.4f}\")\n",
        "        print(f\"  Macro-F1: {cv_summary['macro_f1_mean']:.4f} ± {cv_summary['macro_f1_std']:.4f}\")\n",
        "        print(f\"  Recall (Class 1): {cv_summary['recall_class_1_mean']:.4f} ± {cv_summary['recall_class_1_std']:.4f}\")\n",
        "        print(f\"  ROC-AUC: {cv_summary['roc_auc_mean']:.4f} ± {cv_summary['roc_auc_std']:.4f}\") # Corrected variable name\n",
        "        print(\"-\" * (len(model_name) + 28)) # Match header length\n",
        "\n",
        "    return cv_summary, fold_results"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "73e38728"
      },
      "source": [
        "from sklearn.model_selection import StratifiedKFold\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "\n",
        "def train_and_evaluate_keras_with_cv(build_model_func, X, y, cv_folds=5, epochs=50, batch_size=32, random_state=None, model_name=\"\", verbose=False):\n",
        "    \"\"\"\n",
        "    Trains and evaluates a Keras model using Stratified K-Fold Cross-Validation,\n",
        "    calculating enhanced metrics relevant to credit risk.\n",
        "\n",
        "    Args:\n",
        "        build_model_func (function): A function that returns a compiled Keras model.\n",
        "        X (np.ndarray or pd.DataFrame): The feature data.\n",
        "        y (np.ndarray or pd.Series): The target data.\n",
        "        cv_folds (int): Number of cross-validation folds. Defaults to 5.\n",
        "        epochs (int): Number of epochs for training. Defaults to 50.\n",
        "        batch_size (int): Batch size for training. Defaults to 32.\n",
        "        random_state (int, optional): Random state for reproducibility. Defaults to None.\n",
        "        model_name (str): Name of the model for printing. Defaults to \"\".\n",
        "        verbose (bool): Whether to print results for each fold. Defaults to False.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing:\n",
        "            - dict: Dictionary of mean and std deviation for each metric across folds.\n",
        "            - list: List of dictionaries containing detailed results for each fold.\n",
        "    \"\"\"\n",
        "    skf = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=random_state)\n",
        "    fold_results = []\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"\\n--- {model_name} Cross-Validation ---\")\n",
        "\n",
        "    for fold, (train_index, val_index) in enumerate(skf.split(X, y)):\n",
        "        start_time = time.time()\n",
        "\n",
        "        # Use integer-based indexing for both X and y\n",
        "        # Check if X and y are pandas DataFrames/Series and use .iloc, otherwise use standard numpy indexing\n",
        "        if isinstance(X, (pd.DataFrame, pd.Series)):\n",
        "             X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
        "        else:\n",
        "             X_train, X_val = X[train_index], X[val_index]\n",
        "\n",
        "        if isinstance(y, (pd.DataFrame, pd.Series)):\n",
        "             y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
        "        else:\n",
        "             y_train, y_val = y[train_index], y[val_index]\n",
        "\n",
        "        # Build and compile a fresh model for each fold\n",
        "        model = build_model_func()\n",
        "\n",
        "        # Train the model\n",
        "        model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, verbose=0)\n",
        "\n",
        "        # Predict probabilities\n",
        "        y_pred_proba = model.predict(X_val, verbose=0).flatten()\n",
        "\n",
        "        # Predict class labels (using default threshold 0.5)\n",
        "        y_pred = (y_pred_proba > 0.5).astype(int)\n",
        "\n",
        "        # Calculate metrics\n",
        "        accuracy = accuracy_score(y_val, y_pred)\n",
        "        precision = precision_score(y_val, y_pred, average='binary', zero_division=0)\n",
        "        recall = recall_score(y_val, y_pred, average='binary', zero_division=0)\n",
        "        f1 = f1_score(y_val, y_pred, average='binary', zero_division=0)\n",
        "\n",
        "        # Enhanced Metrics\n",
        "        macro_f1 = f1_score(y_val, y_pred, average='macro', zero_division=0)\n",
        "        recall_class_1 = recall_score(y_val, y_pred, pos_label=1, zero_division=0)\n",
        "        roc_auc = roc_auc_score(y_val, y_pred_proba)\n",
        "\n",
        "        end_time = time.time()\n",
        "        fold_time = end_time - start_time\n",
        "\n",
        "        fold_results.append({\n",
        "            'fold': fold + 1,\n",
        "            'accuracy': accuracy,\n",
        "            'precision': precision,\n",
        "            'recall': recall,\n",
        "            'f1': f1,\n",
        "            'macro_f1': macro_f1,\n",
        "            'recall_class_1': recall_class_1,\n",
        "            'roc_auc': roc_auc,\n",
        "            'time': fold_time,\n",
        "            'y_true': y_val,\n",
        "            'y_pred': y_pred,\n",
        "            'y_pred_proba': y_pred_proba\n",
        "        })\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"  Fold {fold+1} completed in {fold_time:.2f} seconds.\")\n",
        "            print(f\"    Accuracy: {accuracy:.4f}, Macro-F1: {macro_f1:.4f}, Recall (Class 1): {recall_class_1:.4f}, ROC-AUC: {roc_auc:.4f}\")\n",
        "\n",
        "    # Calculate mean and std deviation across folds\n",
        "    mean_metrics = {}\n",
        "    std_metrics = {}\n",
        "    for metric in ['accuracy', 'precision', 'recall', 'f1', 'macro_f1', 'recall_class_1', 'roc_auc', 'time']:\n",
        "        mean_metrics[f'{metric}_mean'] = np.mean([res[metric] for res in fold_results])\n",
        "        std_metrics[f'{metric}_std'] = np.std([res[metric] for res in fold_results])\n",
        "\n",
        "    cv_summary = {**mean_metrics, **std_metrics}\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"\\n--- Average Results for {model_name} ---\")\n",
        "        print(f\"  Accuracy: {cv_summary['accuracy_mean']:.4f} ± {cv_summary['accuracy_std']:.4f}\")\n",
        "        print(f\"  Macro-F1: {cv_summary['macro_f1_mean']:.4f} ± {cv_summary['macro_f1_std']:.4f}\")\n",
        "        print(f\"  Recall (Class 1): {cv_summary['recall_class_1_mean']:.4f} ± {cv_summary['recall_class_1_std']:.4f}\")\n",
        "        print(f\"  ROC-AUC: {cv_summary['roc_auc_mean']:.4f} ± {cv_summary['roc_auc_std']:.4f}\")\n",
        "        print(\"-\" * (len(model_name) + 28))\n",
        "\n",
        "    return cv_summary, fold_results"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Training Logistic Regression with 5-fold CV...\")\n",
        "lr_model = LogisticRegression(random_state=42, max_iter=1000)\n",
        "lr_cv_results, lr_fold_details = train_and_evaluate_with_cv(\n",
        "    lr_model, X_train_scaled, y_train,\n",
        "    cv_folds=5, random_state=42, model_name='Logistic Regression', verbose=True\n",
        ")\n",
        "all_cv_results['Logistic Regression'] = lr_cv_results\n",
        "\n",
        "# Train final model on full training set for predictions\n",
        "lr_final = LogisticRegression(random_state=42, max_iter=1000)\n",
        "lr_final.fit(X_train_scaled, y_train)\n",
        "lr_test_proba = lr_final.predict_proba(X_test_scaled)[:, 1]\n",
        "all_models['Logistic Regression'] = {\n",
        "    'model': lr_final,\n",
        "    'y_true': y_test,\n",
        "    'y_pred_proba': lr_test_proba,\n",
        "    'cv_results': lr_cv_results\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RVmEcbsvyOHH",
        "outputId": "c004c40a-7ac6-424f-b94d-410186e75d4a"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Logistic Regression with 5-fold CV...\n",
            "\n",
            "--- Logistic Regression Cross-Validation ---\n",
            "  Fold 1 completed in 0.09 seconds.\n",
            "    Accuracy: 0.6328, Macro-F1: 0.6328, Recall (Class 1): 0.6456, ROC-AUC: 0.6866\n",
            "  Fold 2 completed in 0.08 seconds.\n",
            "    Accuracy: 0.6299, Macro-F1: 0.6295, Recall (Class 1): 0.6652, ROC-AUC: 0.6798\n",
            "  Fold 3 completed in 0.08 seconds.\n",
            "    Accuracy: 0.6378, Macro-F1: 0.6378, Recall (Class 1): 0.6448, ROC-AUC: 0.6988\n",
            "  Fold 4 completed in 0.10 seconds.\n",
            "    Accuracy: 0.6308, Macro-F1: 0.6307, Recall (Class 1): 0.6224, ROC-AUC: 0.6790\n",
            "  Fold 5 completed in 0.12 seconds.\n",
            "    Accuracy: 0.6373, Macro-F1: 0.6372, Recall (Class 1): 0.6529, ROC-AUC: 0.6940\n",
            "\n",
            "--- Average Results for Logistic Regression ---\n",
            "  Accuracy: 0.6337 ± 0.0033\n",
            "  Macro-F1: 0.6336 ± 0.0034\n",
            "  Recall (Class 1): 0.6462 ± 0.0140\n",
            "  ROC-AUC: 0.6876 ± 0.0078\n",
            "-----------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## XGBoost"
      ],
      "metadata": {
        "id": "kzCNP7f0ydIT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# setting up parameters\n",
        "xgb_model = xgb.XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss')\n",
        "xgb_cv_results, xgb_fold_details = train_and_evaluate_with_cv(\n",
        "    xgb_model, X_train_scaled, y_train,\n",
        "    cv_folds=5, random_state=42, model_name='XGBoost', verbose=True\n",
        ")\n",
        "all_cv_results['XGBoost'] = xgb_cv_results\n",
        "\n",
        "# Train final model\n",
        "xgb_final = xgb.XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss')\n",
        "xgb_final.fit(X_train_scaled, y_train)\n",
        "xgb_test_proba = xgb_final.predict_proba(X_test_scaled)[:, 1]\n",
        "all_models['XGBoost'] = {\n",
        "    'model': xgb_final,\n",
        "    'y_true': y_test,\n",
        "    'y_pred_proba': xgb_test_proba,\n",
        "    'cv_results': xgb_cv_results\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dDGDXJlvyWLN",
        "outputId": "fa103de3-68e3-4619-e36f-eeafd9347bea"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- XGBoost Cross-Validation ---\n",
            "  Fold 1 completed in 0.48 seconds.\n",
            "    Accuracy: 0.6244, Macro-F1: 0.6244, Recall (Class 1): 0.6218, ROC-AUC: 0.6702\n",
            "  Fold 2 completed in 0.17 seconds.\n",
            "    Accuracy: 0.6132, Macro-F1: 0.6132, Recall (Class 1): 0.6233, ROC-AUC: 0.6606\n",
            "  Fold 3 completed in 0.17 seconds.\n",
            "    Accuracy: 0.6350, Macro-F1: 0.6350, Recall (Class 1): 0.6352, ROC-AUC: 0.6762\n",
            "  Fold 4 completed in 0.16 seconds.\n",
            "    Accuracy: 0.6240, Macro-F1: 0.6240, Recall (Class 1): 0.6314, ROC-AUC: 0.6638\n",
            "  Fold 5 completed in 0.17 seconds.\n",
            "    Accuracy: 0.6305, Macro-F1: 0.6303, Recall (Class 1): 0.6495, ROC-AUC: 0.6763\n",
            "\n",
            "--- Average Results for XGBoost ---\n",
            "  Accuracy: 0.6254 ± 0.0073\n",
            "  Macro-F1: 0.6254 ± 0.0073\n",
            "  Recall (Class 1): 0.6323 ± 0.0100\n",
            "  ROC-AUC: 0.6694 ± 0.0064\n",
            "-----------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Artificial Neural Network"
      ],
      "metadata": {
        "id": "v1hZ8mZyyoij"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training ANN with 5-fold CV\n",
        "\n",
        "def build_ann():\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Dense(7, activation='relu', input_shape=(X_train_scaled.shape[1],)),\n",
        "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "ann_cv_results, ann_fold_details = train_and_evaluate_keras_with_cv(\n",
        "    build_ann, X_train_scaled, y_train,\n",
        "    cv_folds=5, epochs=50, batch_size=32, random_state=42,\n",
        "    model_name='ANN', verbose=True\n",
        ")\n",
        "all_cv_results['ANN'] = ann_cv_results\n",
        "\n",
        "# Train final ANN model\n",
        "ann_final = build_ann()\n",
        "ann_final.fit(X_train_scaled, y_train, epochs=50, batch_size=32, verbose=0)\n",
        "ann_test_proba = ann_final.predict(X_test_scaled, verbose=0).flatten()\n",
        "all_models['ANN'] = {\n",
        "    'model': ann_final,\n",
        "    'y_true': y_test,\n",
        "    'y_pred_proba': ann_test_proba,\n",
        "    'cv_results': ann_cv_results\n",
        "}\n",
        "\n",
        "\n",
        "print(\"\\nAll models trained successfully\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GbEYJpXjynC2",
        "outputId": "afcaf8cf-91b0-40e0-e2c8-584540aef681"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- ANN Cross-Validation ---\n",
            "  Fold 1 completed in 52.58 seconds.\n",
            "    Accuracy: 0.6374, Macro-F1: 0.6373, Recall (Class 1): 0.6546, ROC-AUC: 0.6877\n",
            "  Fold 2 completed in 49.43 seconds.\n",
            "    Accuracy: 0.6277, Macro-F1: 0.6273, Recall (Class 1): 0.6578, ROC-AUC: 0.6820\n",
            "  Fold 3 completed in 49.14 seconds.\n",
            "    Accuracy: 0.6407, Macro-F1: 0.6398, Recall (Class 1): 0.6895, ROC-AUC: 0.7043\n",
            "  Fold 4 completed in 52.56 seconds.\n",
            "    Accuracy: 0.6387, Macro-F1: 0.6373, Recall (Class 1): 0.6993, ROC-AUC: 0.6840\n",
            "  Fold 5 completed in 50.03 seconds.\n",
            "    Accuracy: 0.6395, Macro-F1: 0.6385, Recall (Class 1): 0.6919, ROC-AUC: 0.6971\n",
            "\n",
            "--- Average Results for ANN ---\n",
            "  Accuracy: 0.6368 ± 0.0047\n",
            "  Macro-F1: 0.6360 ± 0.0045\n",
            "  Recall (Class 1): 0.6786 ± 0.0186\n",
            "  ROC-AUC: 0.6910 ± 0.0084\n",
            "-------------------------------\n",
            "\n",
            "All models trained successfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# STEP 4: Cross-validation results comparison"
      ],
      "metadata": {
        "id": "JtslObsUy3Ql"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5512b5af"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "def compare_cv_results(all_cv_results):\n",
        "    \"\"\"\n",
        "    Compares the cross-validation results of different models.\n",
        "\n",
        "    Args:\n",
        "        all_cv_results (dict): A dictionary where keys are model names\n",
        "                               and values are dictionaries of CV results (mean and std).\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A DataFrame summarizing the mean CV metrics for each model.\n",
        "    \"\"\"\n",
        "    comparison_data = []\n",
        "    for model_name, cv_results in all_cv_results.items():\n",
        "        comparison_data.append({\n",
        "            'Model': model_name,\n",
        "            'Accuracy': f\"{cv_results['accuracy_mean']:.4f} ± {cv_results['accuracy_std']:.4f}\",\n",
        "            'Macro-F1': f\"{cv_results['macro_f1_mean']:.4f} ± {cv_results['macro_f1_std']:.4f}\",\n",
        "            'Recall (Class 1)': f\"{cv_results['recall_class_1_mean']:.4f} ± {cv_results['recall_class_1_std']:.4f}\",\n",
        "            'ROC-AUC': f\"{cv_results['roc_auc_mean']:.4f} ± {cv_results['roc_auc_std']:.4f}\",\n",
        "            'Avg Time (s)': f\"{cv_results['time_mean']:.2f} ± {cv_results['time_std']:.2f}\"\n",
        "        })\n",
        "\n",
        "    comparison_df = pd.DataFrame(comparison_data)\n",
        "    return comparison_df"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create comparison table\n",
        "comparison_df = compare_cv_results(all_cv_results)\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"CROSS-VALIDATION RESULTS COMPARISON\")\n",
        "print(\"=\"*80)\n",
        "print(comparison_df.to_string(index=False))\n",
        "print(\"=\"*80 + \"\\n\")\n",
        "\n",
        "# Save CV results\n",
        "comparison_df.to_csv('./results/cv_comparison.csv', index=False)\n",
        "print(\"✓ CV results saved to ./results/cv_comparison.csv\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ndA6-3IZxLoY",
        "outputId": "5c96c40f-d6f2-432c-ac05-65a4c8ce61b9"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "CROSS-VALIDATION RESULTS COMPARISON\n",
            "================================================================================\n",
            "              Model        Accuracy        Macro-F1 Recall (Class 1)         ROC-AUC Avg Time (s)\n",
            "Logistic Regression 0.6337 ± 0.0033 0.6336 ± 0.0034  0.6462 ± 0.0140 0.6876 ± 0.0078  0.09 ± 0.01\n",
            "            XGBoost 0.6254 ± 0.0073 0.6254 ± 0.0073  0.6323 ± 0.0100 0.6694 ± 0.0064  0.23 ± 0.12\n",
            "                ANN 0.6368 ± 0.0047 0.6360 ± 0.0045  0.6786 ± 0.0186 0.6910 ± 0.0084 50.75 ± 1.52\n",
            "================================================================================\n",
            "\n",
            "✓ CV results saved to ./results/cv_comparison.csv\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# STEP 5: PHASE 3 ENHANCED METRICS AND VISUALIZATIONS"
      ],
      "metadata": {
        "id": "HIKudFa-zAdX"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "57ae13e1"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "from sklearn.metrics import roc_curve, auc, precision_recall_curve\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def create_comprehensive_visualizations(results_dict, save_path='./results'):\n",
        "    \"\"\"\n",
        "    Generates and saves comprehensive visualizations for model evaluation,\n",
        "    including ROC curves, Precision-Recall curves, Confusion Matrices,\n",
        "    and a threshold analysis plot.\n",
        "\n",
        "    Args:\n",
        "        results_dict (dict): Nested dictionary with results:\n",
        "                             {strategy: {model: {y_true, y_pred_proba, cv_results}}}.\n",
        "        save_path (str): Directory to save the plots. Defaults to './results'.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing:\n",
        "            - dict: Dictionary of generated figures (can be None if not saving).\n",
        "            - dict: Dictionary containing analysis data (e.g., optimal thresholds).\n",
        "    \"\"\"\n",
        "    figures = {}\n",
        "    analysis_data = {}\n",
        "\n",
        "    os.makedirs(save_path, exist_ok=True)\n",
        "\n",
        "    # --- 1. ROC Curves ---\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    plt.plot([0, 1], [0, 1], 'k--', label='Random Guess')\n",
        "    print(\"\\nGenerating ROC Curves...\")\n",
        "    for strategy, models in results_dict.items():\n",
        "        for model_name, data in models.items():\n",
        "            y_true = data['y_true']\n",
        "            y_pred_proba = data['y_pred_proba']\n",
        "            fpr, tpr, _ = roc_curve(y_true, y_pred_proba)\n",
        "            roc_auc = auc(fpr, tpr)\n",
        "            plt.plot(fpr, tpr, label=f'{model_name} ({strategy}) (AUC = {roc_auc:.4f})')\n",
        "\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('ROC Curves')\n",
        "    plt.legend()\n",
        "    roc_fig = plt.gcf()\n",
        "    figures['roc_curves'] = roc_fig\n",
        "    roc_fig.savefig(os.path.join(save_path, 'roc_curves.png'))\n",
        "    print(\"  ✓ ROC Curves saved to ./results/roc_curves.png\")\n",
        "    plt.close(roc_fig) # Close the figure to free memory\n",
        "\n",
        "\n",
        "    # --- 2. Precision-Recall Curves ---\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    print(\"\\nGenerating Precision-Recall Curves...\")\n",
        "    for strategy, models in results_dict.items():\n",
        "        for model_name, data in models.items():\n",
        "            y_true = data['y_true']\n",
        "            y_pred_proba = data['y_pred_proba']\n",
        "            precision, recall, _ = precision_recall_curve(y_true, y_pred_proba)\n",
        "            # It's useful to show the baseline PR curve for imbalanced data\n",
        "            no_skill = len(y_true[y_true==1]) / len(y_true)\n",
        "            plt.plot([0, 1], [no_skill, no_skill], linestyle='--', label='No Skill Baseline')\n",
        "\n",
        "            plt.plot(recall, precision, label=f'{model_name} ({strategy})')\n",
        "\n",
        "    plt.xlabel('Recall')\n",
        "    plt.ylabel('Precision')\n",
        "    plt.title('Precision-Recall Curves')\n",
        "    plt.legend()\n",
        "    pr_fig = plt.gcf()\n",
        "    figures['precision_recall_curves'] = pr_fig\n",
        "    pr_fig.savefig(os.path.join(save_path, 'precision_recall_curves.png'))\n",
        "    print(\"  ✓ Precision-Recall Curves saved to ./results/precision_recall_curves.png\")\n",
        "    plt.close(pr_fig)\n",
        "\n",
        "\n",
        "    # --- 3. Confusion Matrices (using default threshold 0.5) ---\n",
        "    from sklearn.metrics import confusion_matrix\n",
        "    import itertools # For plotting\n",
        "\n",
        "    def plot_confusion_matrix(cm, classes, ax, title='Confusion matrix', cmap=plt.cm.Blues):\n",
        "        ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "        ax.set_title(title)\n",
        "        tick_marks = np.arange(len(classes))\n",
        "        ax.set_xticks(tick_marks)\n",
        "        ax.set_xticklabels(classes, rotation=45)\n",
        "        ax.set_yticks(tick_marks)\n",
        "        ax.set_yticklabels(classes)\n",
        "\n",
        "        fmt = 'd'\n",
        "        thresh = cm.max() / 2.\n",
        "        for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "            ax.text(j, i, format(cm[i, j], fmt),\n",
        "                    ha=\"center\", va=\"center\",\n",
        "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "        ax.set_ylabel('True label')\n",
        "        ax.set_xlabel('Predicted label')\n",
        "        plt.tight_layout()\n",
        "\n",
        "\n",
        "    print(\"\\nGenerating Confusion Matrices...\")\n",
        "    # Determine number of models to decide layout\n",
        "    num_models = sum(len(models) for models in results_dict.values())\n",
        "    if num_models > 0:\n",
        "        n_cols = min(num_models, 3) # Max 3 columns\n",
        "        n_rows = (num_models + n_cols - 1) // n_cols\n",
        "        fig_cm, axes_cm = plt.subplots(n_rows, n_cols, figsize=(n_cols * 6, n_rows * 6))\n",
        "        axes_cm = axes_cm.flatten() if num_models > 1 else [axes_cm] # Ensure axes_cm is iterable\n",
        "\n",
        "        ax_idx = 0\n",
        "        for strategy, models in results_dict.items():\n",
        "            for model_name, data in models.items():\n",
        "                y_true = data['y_true']\n",
        "                y_pred = (data['y_pred_proba'] > 0.5).astype(int) # Use default threshold\n",
        "                cm = confusion_matrix(y_true, y_pred)\n",
        "                plot_confusion_matrix(cm, classes=['Fully Paid (0)', 'Charged Off (1)'], ax=axes_cm[ax_idx], title=f'{model_name} ({strategy}) CM')\n",
        "                ax_idx += 1\n",
        "\n",
        "        # Hide any unused subplots\n",
        "        for i in range(ax_idx, len(axes_cm)):\n",
        "            fig_cm.delaxes(axes_cm[i])\n",
        "\n",
        "        figures['confusion_matrices'] = fig_cm\n",
        "        fig_cm.savefig(os.path.join(save_path, 'confusion_matrices.png'))\n",
        "        print(\"  ✓ Confusion Matrices saved to ./results/confusion_matrices.png\")\n",
        "        plt.close(fig_cm)\n",
        "\n",
        "\n",
        "    # --- 4. Threshold Analysis Plot (for the best performing model based on Macro-F1) ---\n",
        "    print(\"\\nPerforming Threshold Analysis...\")\n",
        "    # Find best model based on CV Macro-F1 (assuming results_dict contains 'cv_results')\n",
        "    best_model_name = None\n",
        "    best_macro_f1_cv = -1\n",
        "    best_strategy = None\n",
        "\n",
        "    for strategy, models in results_dict.items():\n",
        "        for model_name, data in models.items():\n",
        "            if 'cv_results' in data and data['cv_results']['macro_f1_mean'] > best_macro_f1_cv:\n",
        "                 best_macro_f1_cv = data['cv_results']['macro_f1_mean']\n",
        "                 best_model_name = model_name\n",
        "                 best_strategy = strategy\n",
        "\n",
        "    if best_model_name:\n",
        "        print(f\"  Analyzing thresholds for best model: {best_model_name} ({best_strategy})\")\n",
        "        y_true_best = results_dict[best_strategy][best_model_name]['y_true']\n",
        "        y_pred_proba_best = results_dict[best_strategy][best_model_name]['y_pred_proba']\n",
        "\n",
        "        thresholds = np.linspace(0, 1, 100) # Analyze 100 thresholds\n",
        "        precision_scores = []\n",
        "        recall_scores = []\n",
        "        f1_scores = []\n",
        "        macro_f1_scores = []\n",
        "\n",
        "        for thresh in thresholds:\n",
        "            y_pred_thresh = (y_pred_proba_best > thresh).astype(int)\n",
        "            precision_scores.append(precision_score(y_true_best, y_pred_thresh, zero_division=0))\n",
        "            recall_scores.append(recall_score(y_true_best, y_pred_thresh, zero_division=0))\n",
        "            f1_scores.append(f1_score(y_true_best, y_pred_thresh, zero_division=0))\n",
        "            macro_f1_scores.append(f1_score(y_true_best, y_pred_thresh, average='macro', zero_division=0))\n",
        "\n",
        "        # Find optimal thresholds\n",
        "        optimal_macro_f1_idx = np.argmax(macro_f1_scores)\n",
        "        optimal_recall_idx = np.argmax(recall_scores) # Often near 0 threshold\n",
        "\n",
        "        optimal_thresholds = {\n",
        "            'optimal_macro_f1': {\n",
        "                'threshold': thresholds[optimal_macro_f1_idx],\n",
        "                'macro_f1': macro_f1_scores[optimal_macro_f1_idx],\n",
        "                'precision': precision_scores[optimal_macro_f1_idx],\n",
        "                'recall': recall_scores[optimal_macro_f1_idx],\n",
        "            },\n",
        "            'optimal_recall': {\n",
        "                'threshold': thresholds[optimal_recall_idx],\n",
        "                'macro_f1': macro_f1_scores[optimal_recall_idx],\n",
        "                'precision': precision_scores[optimal_recall_idx],\n",
        "                'recall': recall_scores[optimal_recall_idx],\n",
        "            }\n",
        "        }\n",
        "        analysis_data['optimal_thresholds'] = optimal_thresholds\n",
        "\n",
        "        plt.figure(figsize=(12, 6))\n",
        "        plt.plot(thresholds, precision_scores, label='Precision')\n",
        "        plt.plot(thresholds, recall_scores, label='Recall')\n",
        "        plt.plot(thresholds, f1_scores, label='F1 Score (Binary)')\n",
        "        plt.plot(thresholds, macro_f1_scores, label='Macro F1 Score')\n",
        "        plt.axvline(0.5, color='grey', linestyle='--', label='Default Threshold (0.5)')\n",
        "        plt.axvline(optimal_thresholds['optimal_macro_f1']['threshold'], color='green', linestyle=':', label=f'Optimal Macro-F1 Threshold ({optimal_thresholds[\"optimal_macro_f1\"][\"threshold\"]:.3f})')\n",
        "        plt.axvline(optimal_thresholds['optimal_recall']['threshold'], color='red', linestyle=':', label=f'Optimal Recall Threshold ({optimal_thresholds[\"optimal_recall\"][\"threshold\"]:.3f})')\n",
        "\n",
        "\n",
        "        plt.xlabel('Threshold')\n",
        "        plt.ylabel('Score')\n",
        "        plt.title(f'Threshold Analysis for {best_model_name} ({best_strategy})')\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "        threshold_fig = plt.gcf()\n",
        "        figures['threshold_analysis'] = threshold_fig\n",
        "        threshold_fig.savefig(os.path.join(save_path, 'threshold_analysis.png'))\n",
        "        print(\"  ✓ Threshold Analysis plot saved to ./results/threshold_analysis.png\")\n",
        "        plt.close(threshold_fig)\n",
        "\n",
        "    else:\n",
        "        print(\"  No models found to perform threshold analysis.\")\n",
        "\n",
        "\n",
        "    print(\"\\n✓ Visualization generation complete\")\n",
        "\n",
        "\n",
        "    return figures, analysis_data"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare results dict for Phase 3\n",
        "# Phase 3 expects: {strategy: {model: {y_true, y_pred_proba, cv_results}}}\n",
        "phase3_results = {\n",
        "    BEST_STRATEGY: all_models\n",
        "}\n",
        "\n",
        "# Generate all Phase 3 visualizations\n",
        "figures, analysis_data = create_comprehensive_visualizations(\n",
        "    phase3_results,\n",
        "    save_path='./results'\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pEWmnPxUxPfF",
        "outputId": "49c6b9da-0783-4e1f-a1af-b8b2beeb6c16"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Generating ROC Curves...\n",
            "  ✓ ROC Curves saved to ./results/roc_curves.png\n",
            "\n",
            "Generating Precision-Recall Curves...\n",
            "  ✓ Precision-Recall Curves saved to ./results/precision_recall_curves.png\n",
            "\n",
            "Generating Confusion Matrices...\n",
            "  ✓ Confusion Matrices saved to ./results/confusion_matrices.png\n",
            "\n",
            "Performing Threshold Analysis...\n",
            "  Analyzing thresholds for best model: ANN (downsample_upsample)\n",
            "  ✓ Threshold Analysis plot saved to ./results/threshold_analysis.png\n",
            "\n",
            "✓ Visualization generation complete\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# STEP 6: FIND BEST MODEL AND OPTIMAL THRESHOLD"
      ],
      "metadata": {
        "id": "_Tv1farezHPG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Find best model based on CV Macro-F1\n",
        "best_model_name = None\n",
        "best_macro_f1 = -1\n",
        "\n",
        "for model_name, cv_results in all_cv_results.items():\n",
        "    if cv_results['macro_f1_mean'] > best_macro_f1:\n",
        "        best_macro_f1 = cv_results['macro_f1_mean']\n",
        "        best_model_name = model_name\n",
        "\n",
        "best_model_data = all_models[best_model_name]\n",
        "\n",
        "print(\"BEST MODEL (Based on Cross-Validation Macro-F1):\")\n",
        "print(f\"  Model: {best_model_name}\")\n",
        "print(f\"  Strategy: {BEST_STRATEGY}\")\n",
        "print(f\"  CV Macro-F1: {best_macro_f1:.4f} ± {all_cv_results[best_model_name]['macro_f1_std']:.4f}\")\n",
        "print(f\"  CV Class 1 Recall: {all_cv_results[best_model_name]['recall_class_1_mean']:.4f} ± {all_cv_results[best_model_name]['recall_class_1_std']:.4f}\")\n",
        "print(f\"  CV ROC-AUC: {all_cv_results[best_model_name]['roc_auc_mean']:.4f} ± {all_cv_results[best_model_name]['roc_auc_std']:.4f}\")\n",
        "\n",
        "# Optimal threshold recommendation\n",
        "if 'optimal_thresholds' in analysis_data:\n",
        "    opt_thresh = analysis_data['optimal_thresholds']\n",
        "    print(f\"\\nOPTIMAL THRESHOLD RECOMMENDATIONS:\")\n",
        "    print(f\"  For Best Macro-F1:  Use threshold = {opt_thresh['optimal_macro_f1']['threshold']:.3f}\")\n",
        "    print(f\"    → Macro-F1: {opt_thresh['optimal_macro_f1']['macro_f1']:.4f}\")\n",
        "    print(f\"    → Recall: {opt_thresh['optimal_macro_f1']['recall']:.4f}\")\n",
        "    print(f\"  For Best Recall:    Use threshold = {opt_thresh['optimal_recall']['threshold']:.3f}\")\n",
        "    print(f\"    → Recall: {opt_thresh['optimal_recall']['recall']:.4f}\")\n",
        "    print(f\"    → Macro-F1: {opt_thresh['optimal_recall']['macro_f1']:.4f}\")\n",
        "    print(f\"  Default (0.5):      Standard threshold\")\n",
        "\n",
        "print(f\"\\n{'='*80}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1B0pAqDexVRd",
        "outputId": "fce06d54-802a-434f-f5a9-1a7cfe9a2064"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BEST MODEL (Based on Cross-Validation Macro-F1):\n",
            "  Model: ANN\n",
            "  Strategy: downsample_upsample\n",
            "  CV Macro-F1: 0.6360 ± 0.0045\n",
            "  CV Class 1 Recall: 0.6786 ± 0.0186\n",
            "  CV ROC-AUC: 0.6910 ± 0.0084\n",
            "\n",
            "OPTIMAL THRESHOLD RECOMMENDATIONS:\n",
            "  For Best Macro-F1:  Use threshold = 0.485\n",
            "    → Macro-F1: 0.6385\n",
            "    → Recall: 0.7187\n",
            "  For Best Recall:    Use threshold = 0.000\n",
            "    → Recall: 1.0000\n",
            "    → Macro-F1: 0.3333\n",
            "  Default (0.5):      Standard threshold\n",
            "\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# STEP 7: FINAL EVALUATION ON TEST SET"
      ],
      "metadata": {
        "id": "3uFUuNxIzOVy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use default threshold (0.5) for final evaluation\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "for model_name, model_data in all_models.items():\n",
        "    y_true = model_data['y_true']\n",
        "    y_pred = (model_data['y_pred_proba'] > 0.5).astype(int)\n",
        "\n",
        "    print(f\"\\n{'-'*80}\")\n",
        "    print(f\"Final Test Set Results: {model_name}\")\n",
        "    print(f\"{'-'*80}\")\n",
        "\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(classification_report(\n",
        "        y_true, y_pred,\n",
        "        target_names=['Fully Paid (0)', 'Charged Off (1)'],\n",
        "        zero_division=0\n",
        "    ))\n",
        "\n",
        "    print(\"Confusion Matrix:\")\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    cm_df = pd.DataFrame(\n",
        "        cm,\n",
        "        index=['Actual Fully Paid', 'Actual Charged Off'],\n",
        "        columns=['Pred Fully Paid', 'Pred Charged Off']\n",
        "    )\n",
        "    print(cm_df)\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YWMK9fVrxYoY",
        "outputId": "1efd1bdb-b770-4765-f40c-77e710a77e3c"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Final Test Set Results: Logistic Regression\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Classification Report:\n",
            "                 precision    recall  f1-score   support\n",
            "\n",
            " Fully Paid (0)       0.64      0.63      0.63      2211\n",
            "Charged Off (1)       0.63      0.64      0.64      2211\n",
            "\n",
            "       accuracy                           0.64      4422\n",
            "      macro avg       0.64      0.64      0.63      4422\n",
            "   weighted avg       0.64      0.64      0.63      4422\n",
            "\n",
            "Confusion Matrix:\n",
            "                    Pred Fully Paid  Pred Charged Off\n",
            "Actual Fully Paid              1392               819\n",
            "Actual Charged Off              795              1416\n",
            "\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Final Test Set Results: XGBoost\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Classification Report:\n",
            "                 precision    recall  f1-score   support\n",
            "\n",
            " Fully Paid (0)       0.63      0.62      0.62      2211\n",
            "Charged Off (1)       0.62      0.63      0.63      2211\n",
            "\n",
            "       accuracy                           0.63      4422\n",
            "      macro avg       0.63      0.63      0.63      4422\n",
            "   weighted avg       0.63      0.63      0.63      4422\n",
            "\n",
            "Confusion Matrix:\n",
            "                    Pred Fully Paid  Pred Charged Off\n",
            "Actual Fully Paid              1372               839\n",
            "Actual Charged Off              814              1397\n",
            "\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Final Test Set Results: ANN\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Classification Report:\n",
            "                 precision    recall  f1-score   support\n",
            "\n",
            " Fully Paid (0)       0.65      0.60      0.62      2211\n",
            "Charged Off (1)       0.63      0.67      0.65      2211\n",
            "\n",
            "       accuracy                           0.64      4422\n",
            "      macro avg       0.64      0.64      0.64      4422\n",
            "   weighted avg       0.64      0.64      0.64      4422\n",
            "\n",
            "Confusion Matrix:\n",
            "                    Pred Fully Paid  Pred Charged Off\n",
            "Actual Fully Paid              1336               875\n",
            "Actual Charged Off              729              1482\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# STEP 8: SAVE ALL RESULTS"
      ],
      "metadata": {
        "id": "jBkhCm5-zSDA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save summary\n",
        "summary_data = []\n",
        "for model_name in all_models.keys():\n",
        "    cv_res = all_cv_results[model_name]\n",
        "    summary_data.append({\n",
        "        'Model': model_name,\n",
        "        'Strategy': BEST_STRATEGY,\n",
        "        'CV_Accuracy_Mean': cv_res['accuracy_mean'],\n",
        "        'CV_Accuracy_Std': cv_res['accuracy_std'],\n",
        "        'CV_MacroF1_Mean': cv_res['macro_f1_mean'],\n",
        "        'CV_MacroF1_Std': cv_res['macro_f1_std'],\n",
        "        'CV_Class1_Recall_Mean': cv_res['recall_class_1_mean'],\n",
        "        'CV_Class1_Recall_Std': cv_res['recall_class_1_std'],\n",
        "        'CV_ROC_AUC_Mean': cv_res['roc_auc_mean'],\n",
        "        'CV_ROC_AUC_Std': cv_res['roc_auc_std'],\n",
        "    })\n",
        "\n",
        "final_summary_df = pd.DataFrame(summary_data)\n",
        "final_summary_df.to_csv('./results/final_summary.csv', index=False)\n",
        "\n",
        "print(\"✓ Results saved:\")\n",
        "print(\"  - ./results/final_summary.csv\")\n",
        "print(\"  - ./results/cv_comparison.csv\")\n",
        "print(\"  - ./results/classification_reports_summary.csv\")\n",
        "print(\"  - ./results/roc_curves.png\")\n",
        "print(\"  - ./results/precision_recall_curves.png\")\n",
        "print(\"  - ./results/confusion_matrices.png\")\n",
        "print(\"  - ./results/threshold_analysis.png\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yW1lihuRxbp5",
        "outputId": "8f98b45b-f9ee-421d-cfa8-bdce77bd24c4"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Results saved:\n",
            "  - ./results/final_summary.csv\n",
            "  - ./results/cv_comparison.csv\n",
            "  - ./results/classification_reports_summary.csv\n",
            "  - ./results/roc_curves.png\n",
            "  - ./results/precision_recall_curves.png\n",
            "  - ./results/confusion_matrices.png\n",
            "  - ./results/threshold_analysis.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FINAL SUMMARY"
      ],
      "metadata": {
        "id": "TIG9NmlOzT_d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Summary of Achievements:\")\n",
        "print(f\"  ✓ Phase 1: Applied {BEST_STRATEGY} balancing strategy\")\n",
        "print(f\"  ✓ Phase 2: Performed 5-fold cross-validation for all models\")\n",
        "print(f\"  ✓ Phase 3: Generated comprehensive metrics and visualizations\")\n",
        "print(f\"  ✓ Best Model: {best_model_name}\")\n",
        "print(f\"  ✓ CV Macro-F1: {best_macro_f1:.4f}\")\n",
        "print(f\"  ✓ CV Class 1 Recall: {all_cv_results[best_model_name]['recall_class_1_mean']:.4f}\")\n",
        "\n",
        "print(\"\\nKey Improvements from Baseline:\")\n",
        "print(\"  Baseline (no balancing):\")\n",
        "print(\"    - Class 1 Recall: ~1-6% (POOR)\")\n",
        "print(\"    - Macro-F1: ~0.46 (POOR)\")\n",
        "print(f\"  {BEST_STRATEGY} + {best_model_name}:\")\n",
        "print(f\"    - Class 1 Recall: {all_cv_results[best_model_name]['recall_class_1_mean']*100:.1f}% (GOOD!)\")\n",
        "print(f\"    - Macro-F1: {best_macro_f1:.4f} (GOOD!)\")\n",
        "print(f\"    - Improvement: ~{(all_cv_results[best_model_name]['recall_class_1_mean'] - 0.03)*100:.0f} percentage points in recall\")\n",
        "\n",
        "print(\"\\nNext Steps for Research Paper:\")\n",
        "print(\"  1. Document the methodology (balancing + CV)\")\n",
        "print(\"  2. Include the visualizations (ROC, PR curves)\")\n",
        "print(\"  3. Discuss the trade-offs (accuracy vs recall)\")\n",
        "print(\"  4. Emphasize Macro-F1 for imbalanced data\")\n",
        "print(\"  5. Recommend optimal threshold for deployment\")\n",
        "\n",
        "print(\"\\nALL PHASES COMPLETE! ✓✓✓\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hTVkdZrzxftO",
        "outputId": "9f94e36d-00de-45db-f66f-556210756e5b"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Summary of Achievements:\n",
            "  ✓ Phase 1: Applied downsample_upsample balancing strategy\n",
            "  ✓ Phase 2: Performed 5-fold cross-validation for all models\n",
            "  ✓ Phase 3: Generated comprehensive metrics and visualizations\n",
            "  ✓ Best Model: ANN\n",
            "  ✓ CV Macro-F1: 0.6360\n",
            "  ✓ CV Class 1 Recall: 0.6786\n",
            "\n",
            "Key Improvements from Baseline:\n",
            "  Baseline (no balancing):\n",
            "    - Class 1 Recall: ~1-6% (POOR)\n",
            "    - Macro-F1: ~0.46 (POOR)\n",
            "  downsample_upsample + ANN:\n",
            "    - Class 1 Recall: 67.9% (GOOD!)\n",
            "    - Macro-F1: 0.6360 (GOOD!)\n",
            "    - Improvement: ~65 percentage points in recall\n",
            "\n",
            "Next Steps for Research Paper:\n",
            "  1. Document the methodology (balancing + CV)\n",
            "  2. Include the visualizations (ROC, PR curves)\n",
            "  3. Discuss the trade-offs (accuracy vs recall)\n",
            "  4. Emphasize Macro-F1 for imbalanced data\n",
            "  5. Recommend optimal threshold for deployment\n",
            "\n",
            "ALL PHASES COMPLETE! ✓✓✓\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bJsrtxNr41fw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9773ce9"
      },
      "source": [
        "# Task\n",
        "Improve the performance of the models, particularly the ANN, by implementing hyperparameter tuning, exploring different ANN architectures, applying advanced balancing techniques, and considering feature engineering and selection."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0b2c591"
      },
      "source": [
        "## Hyperparameter tuning\n",
        "\n",
        "### Subtask:\n",
        "Systematically search for the best combination of hyperparameters for each model (especially the ANN and XGBoost) using techniques like Grid Search or Random Search. This includes parameters like learning rate, number of layers/neurons (for ANN), tree-specific parameters (for XGBoost), regularization terms, etc.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e22b9a8e"
      },
      "source": [
        "**Reasoning**:\n",
        "Define parameter grids and perform hyperparameter tuning for Logistic Regression, XGBoost, and the ANN using GridSearchCV and RandomizedSearchCV respectively, then print the best parameters and scores.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 401
        },
        "id": "4888422c",
        "outputId": "f9191567-a460-4574-b056-eaae0e9f16c4"
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, StratifiedKFold\n",
        "from scikeras.wrappers import KerasClassifier\n",
        "import tensorflow as tf\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import xgboost as xgb\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import numpy as np\n",
        "\n",
        "# Define parameter grids/distributions\n",
        "# Logistic Regression\n",
        "lr_param_grid = {\n",
        "    'C': [0.01, 0.1, 1, 10, 100],\n",
        "    'penalty': ['l2'] # L1 penalty requires solver that supports it, keeping it simple with L2\n",
        "}\n",
        "\n",
        "# XGBoost\n",
        "xgb_param_dist = {\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    'learning_rate': [0.01, 0.05, 0.1],\n",
        "    'max_depth': [3, 5, 7],\n",
        "    'subsample': [0.7, 0.8, 0.9],\n",
        "    'colsample_bytree': [0.7, 0.8, 0.9],\n",
        "    'gamma': [0, 0.1, 0.2]\n",
        "}\n",
        "\n",
        "# ANN - using a simple architecture for tuning\n",
        "def build_ann_tune(n_neurons=7, l2_reg=0.0):\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Dense(n_neurons, activation='relu', input_shape=(X_train_scaled.shape[1],),\n",
        "                              kernel_regularizer=tf.keras.regularizers.l2(l2_reg)),\n",
        "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "ann_param_grid = {\n",
        "    'model__n_neurons': [5, 7, 10],\n",
        "    'model__l2_reg': [0.0, 0.01, 0.001],\n",
        "    'batch_size': [32, 64],\n",
        "    'epochs': [30, 50] # Reduce epochs for tuning speed\n",
        "}\n",
        "\n",
        "# Cross-validation strategy\n",
        "cv_strategy = StratifiedKFold(n_splits=3, shuffle=True, random_state=42) # Reduced folds for tuning speed\n",
        "scoring_metric = 'roc_auc' # Using ROC-AUC for tuning\n",
        "\n",
        "# --- Logistic Regression Tuning ---\n",
        "print(\"Starting Logistic Regression Hyperparameter Tuning...\")\n",
        "lr_model_tune = LogisticRegression(random_state=42, max_iter=1000) # Increased max_iter\n",
        "lr_grid_search = GridSearchCV(\n",
        "    lr_model_tune,\n",
        "    lr_param_grid,\n",
        "    scoring=scoring_metric,\n",
        "    cv=cv_strategy,\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "lr_grid_search.fit(X_train_scaled, y_train)\n",
        "\n",
        "print(\"\\nBest parameters for Logistic Regression:\")\n",
        "print(lr_grid_search.best_params_)\n",
        "print(f\"Best CV {scoring_metric} score for Logistic Regression: {lr_grid_search.best_score_:.4f}\")\n",
        "\n",
        "# Store the best model\n",
        "best_lr_model = lr_grid_search.best_estimator_\n",
        "all_models['Logistic Regression']['tuned_model'] = best_lr_model\n",
        "\n",
        "\n",
        "# --- XGBoost Tuning ---\n",
        "print(\"\\nStarting XGBoost Hyperparameter Tuning...\")\n",
        "xgb_model_tune = xgb.XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss')\n",
        "xgb_random_search = RandomizedSearchCV(\n",
        "    xgb_model_tune,\n",
        "    xgb_param_dist,\n",
        "    n_iter=20, # Number of parameter settings that are sampled\n",
        "    scoring=scoring_metric,\n",
        "    cv=cv_strategy,\n",
        "    random_state=42,\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "xgb_random_search.fit(X_train_scaled, y_train)\n",
        "\n",
        "print(\"\\nBest parameters for XGBoost:\")\n",
        "print(xgb_random_search.best_params_)\n",
        "print(f\"Best CV {scoring_metric} score for XGBoost: {xgb_random_search.best_score_:.4f}\")\n",
        "\n",
        "# Store the best model\n",
        "best_xgb_model = xgb_random_search.best_estimator_\n",
        "all_models['XGBoost']['tuned_model'] = best_xgb_model\n",
        "\n",
        "\n",
        "# --- ANN Tuning ---\n",
        "print(\"\\nStarting ANN Hyperparameter Tuning...\")\n",
        "\n",
        "# Note: KerasClassifier needs the build_fn, which is our build_ann_tune function\n",
        "ann_model_tune = KerasClassifier(model=build_ann_tune, verbose=0) # Pass the build function, verbose=0 to reduce output\n",
        "\n",
        "ann_grid_search = GridSearchCV(\n",
        "    ann_model_tune,\n",
        "    ann_param_grid,\n",
        "    scoring=scoring_metric,\n",
        "    cv=cv_strategy,\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Fit the GridSearchCV. It will call build_ann_tune internally with different params\n",
        "ann_grid_search.fit(X_train_scaled, y_train)\n",
        "\n",
        "\n",
        "print(\"\\nBest parameters for ANN:\")\n",
        "print(ann_grid_search.best_params_)\n",
        "print(f\"Best CV {scoring_metric} score for ANN: {ann_grid_search.best_score_:.4f}\")\n",
        "\n",
        "# Store the best model\n",
        "# GridSearchCV on KerasClassifier stores the best *parameters*, not a fitted Keras model directly.\n",
        "# We need to train the best model separately using the best parameters found.\n",
        "best_ann_params = ann_grid_search.best_params_\n",
        "best_ann_model = build_ann_tune(\n",
        "    n_neurons=best_ann_params['model__n_neurons'],\n",
        "    l2_reg=best_ann_params['model__l2_reg']\n",
        ")\n",
        "best_ann_model.fit(X_train_scaled, y_train, epochs=best_ann_params['epochs'], batch_size=best_ann_params['batch_size'], verbose=0)\n",
        "\n",
        "all_models['ANN']['tuned_model'] = best_ann_model\n",
        "\n",
        "print(\"\\nHyperparameter tuning complete for all models.\")"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'scikeras'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2304199163.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRandomizedSearchCV\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mStratifiedKFold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mscikeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrappers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mKerasClassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear_model\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mxgboost\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mxgb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'scikeras'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bf5253b0"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous command failed because the `scikeras` library is not installed. Install the `scikeras` library using pip.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lU4hbIc852gX",
        "outputId": "7cb0849b-96a0-437b-8f58-ee67f547098e"
      },
      "source": [
        "%pip install scikeras"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scikeras\n",
            "  Downloading scikeras-0.13.0-py3-none-any.whl.metadata (3.1 kB)\n",
            "Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.12/dist-packages (from scikeras) (3.10.0)\n",
            "Requirement already satisfied: scikit-learn>=1.4.2 in /usr/local/lib/python3.12/dist-packages (from scikeras) (1.6.1)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.12/dist-packages (from keras>=3.2.0->scikeras) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from keras>=3.2.0->scikeras) (2.0.2)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras>=3.2.0->scikeras) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras>=3.2.0->scikeras) (0.1.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.12/dist-packages (from keras>=3.2.0->scikeras) (3.15.1)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras>=3.2.0->scikeras) (0.17.0)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.12/dist-packages (from keras>=3.2.0->scikeras) (0.5.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from keras>=3.2.0->scikeras) (25.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.4.2->scikeras) (1.16.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.4.2->scikeras) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.4.2->scikeras) (3.6.0)\n",
            "Requirement already satisfied: typing-extensions>=4.6.0 in /usr/local/lib/python3.12/dist-packages (from optree->keras>=3.2.0->scikeras) (4.15.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.2.0->scikeras) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.2.0->scikeras) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->scikeras) (0.1.2)\n",
            "Downloading scikeras-0.13.0-py3-none-any.whl (26 kB)\n",
            "Installing collected packages: scikeras\n",
            "Successfully installed scikeras-0.13.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a82fdc28"
      },
      "source": [
        "**Reasoning**:\n",
        "The `scikeras` library has been successfully installed. Now, retry the hyperparameter tuning process for Logistic Regression, XGBoost, and the ANN.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 671
        },
        "id": "9a46d265",
        "outputId": "06dcfc81-8335-41b1-8065-69b8d97d185d"
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, StratifiedKFold\n",
        "from scikeras.wrappers import KerasClassifier\n",
        "import tensorflow as tf\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import xgboost as xgb\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import numpy as np\n",
        "\n",
        "# Define parameter grids/distributions\n",
        "# Logistic Regression\n",
        "lr_param_grid = {\n",
        "    'C': [0.01, 0.1, 1, 10, 100],\n",
        "    'penalty': ['l2'] # L1 penalty requires solver that supports it, keeping it simple with L2\n",
        "}\n",
        "\n",
        "# XGBoost\n",
        "xgb_param_dist = {\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    'learning_rate': [0.01, 0.05, 0.1],\n",
        "    'max_depth': [3, 5, 7],\n",
        "    'subsample': [0.7, 0.8, 0.9],\n",
        "    'colsample_bytree': [0.7, 0.8, 0.9],\n",
        "    'gamma': [0, 0.1, 0.2]\n",
        "}\n",
        "\n",
        "# ANN - using a simple architecture for tuning\n",
        "def build_ann_tune(n_neurons=7, l2_reg=0.0):\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Dense(n_neurons, activation='relu', input_shape=(X_train_scaled.shape[1],),\n",
        "                              kernel_regularizer=tf.keras.regularizers.l2(l2_reg)),\n",
        "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "ann_param_grid = {\n",
        "    'model__n_neurons': [5, 7, 10],\n",
        "    'model__l2_reg': [0.0, 0.01, 0.001],\n",
        "    'batch_size': [32, 64],\n",
        "    'epochs': [30, 50] # Reduce epochs for tuning speed\n",
        "}\n",
        "\n",
        "# Cross-validation strategy\n",
        "cv_strategy = StratifiedKFold(n_splits=3, shuffle=True, random_state=42) # Reduced folds for tuning speed\n",
        "scoring_metric = 'roc_auc' # Using ROC-AUC for tuning\n",
        "\n",
        "# --- Logistic Regression Tuning ---\n",
        "print(\"Starting Logistic Regression Hyperparameter Tuning...\")\n",
        "lr_model_tune = LogisticRegression(random_state=42, max_iter=1000) # Increased max_iter\n",
        "lr_grid_search = GridSearchCV(\n",
        "    lr_model_tune,\n",
        "    lr_param_grid,\n",
        "    scoring=scoring_metric,\n",
        "    cv=cv_strategy,\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "lr_grid_search.fit(X_train_scaled, y_train)\n",
        "\n",
        "print(\"\\nBest parameters for Logistic Regression:\")\n",
        "print(lr_grid_search.best_params_)\n",
        "print(f\"Best CV {scoring_metric} score for Logistic Regression: {lr_grid_search.best_score_:.4f}\")\n",
        "\n",
        "# Store the best model\n",
        "best_lr_model = lr_grid_search.best_estimator_\n",
        "all_models['Logistic Regression']['tuned_model'] = best_lr_model\n",
        "\n",
        "\n",
        "# --- XGBoost Tuning ---\n",
        "print(\"\\nStarting XGBoost Hyperparameter Tuning...\")\n",
        "xgb_model_tune = xgb.XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss')\n",
        "xgb_random_search = RandomizedSearchCV(\n",
        "    xgb_model_tune,\n",
        "    xgb_param_dist,\n",
        "    n_iter=20, # Number of parameter settings that are sampled\n",
        "    scoring=scoring_metric,\n",
        "    cv=cv_strategy,\n",
        "    random_state=42,\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "xgb_random_search.fit(X_train_scaled, y_train)\n",
        "\n",
        "print(\"\\nBest parameters for XGBoost:\")\n",
        "print(xgb_random_search.best_params_)\n",
        "print(f\"Best CV {scoring_metric} score for XGBoost: {xgb_random_search.best_score_:.4f}\")\n",
        "\n",
        "# Store the best model\n",
        "best_xgb_model = xgb_random_search.best_estimator_\n",
        "all_models['XGBoost']['tuned_model'] = best_xgb_model\n",
        "\n",
        "\n",
        "# --- ANN Tuning ---\n",
        "print(\"\\nStarting ANN Hyperparameter Tuning...\")\n",
        "\n",
        "# Note: KerasClassifier needs the build_fn, which is our build_ann_tune function\n",
        "ann_model_tune = KerasClassifier(model=build_ann_tune, verbose=0) # Pass the build function, verbose=0 to reduce output\n",
        "\n",
        "ann_grid_search = GridSearchCV(\n",
        "    ann_model_tune,\n",
        "    ann_param_grid,\n",
        "    scoring=scoring_metric,\n",
        "    cv=cv_strategy,\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Fit the GridSearchCV. It will call build_ann_tune internally with different params\n",
        "ann_grid_search.fit(X_train_scaled, y_train)\n",
        "\n",
        "\n",
        "print(\"\\nBest parameters for ANN:\")\n",
        "print(ann_grid_search.best_params_)\n",
        "print(f\"Best CV {scoring_metric} score for ANN: {ann_grid_search.best_score_:.4f}\")\n",
        "\n",
        "# Store the best model\n",
        "# GridSearchCV on KerasClassifier stores the best *parameters*, not a fitted Keras model directly.\n",
        "# We need to train the best model separately using the best parameters found.\n",
        "best_ann_params = ann_grid_search.best_params_\n",
        "best_ann_model = build_ann_tune(\n",
        "    n_neurons=best_ann_params['model__n_neurons'],\n",
        "    l2_reg=best_ann_params['model__l2_reg']\n",
        ")\n",
        "best_ann_model.fit(X_train_scaled, y_train, epochs=best_ann_params['epochs'], batch_size=best_ann_params['batch_size'], verbose=0)\n",
        "\n",
        "all_models['ANN']['tuned_model'] = best_ann_model\n",
        "\n",
        "print(\"\\nHyperparameter tuning complete for all models.\")"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Logistic Regression Hyperparameter Tuning...\n",
            "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n",
            "\n",
            "Best parameters for Logistic Regression:\n",
            "{'C': 100, 'penalty': 'l2'}\n",
            "Best CV roc_auc score for Logistic Regression: 0.6945\n",
            "\n",
            "Starting XGBoost Hyperparameter Tuning...\n",
            "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n",
            "\n",
            "Best parameters for XGBoost:\n",
            "{'subsample': 0.8, 'n_estimators': 200, 'max_depth': 3, 'learning_rate': 0.05, 'gamma': 0.1, 'colsample_bytree': 0.8}\n",
            "Best CV roc_auc score for XGBoost: 0.6961\n",
            "\n",
            "Starting ANN Hyperparameter Tuning...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'super' object has no attribute '__sklearn_tags__'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2304199163.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;31m# Fit the GridSearchCV. It will call build_ann_tune internally with different params\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m \u001b[0mann_grid_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_scaled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1387\u001b[0m                 )\n\u001b[1;32m   1388\u001b[0m             ):\n\u001b[0;32m-> 1389\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mrouted_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_routed_params_for_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 933\u001b[0;31m         \u001b[0mcv_orig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_cv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    934\u001b[0m         \u001b[0mn_splits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv_orig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_n_splits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mrouted_params\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplitter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mis_classifier\u001b[0;34m(estimator)\u001b[0m\n\u001b[1;32m   1235\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_estimator_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"classifier\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1237\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mget_tags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"classifier\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/utils/_tags.py\u001b[0m in \u001b[0;36mget_tags\u001b[0;34m(estimator)\u001b[0m\n\u001b[1;32m    428\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mklass\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreversed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmro\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m\"__sklearn_tags__\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mklass\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 430\u001b[0;31m                 \u001b[0msklearn_tags_provider\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mklass\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mklass\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__sklearn_tags__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    431\u001b[0m                 \u001b[0mclass_order\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mklass\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0;34m\"_more_tags\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mklass\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36m__sklearn_tags__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    538\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    539\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__sklearn_tags__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 540\u001b[0;31m         \u001b[0mtags\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__sklearn_tags__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    541\u001b[0m         \u001b[0mtags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"classifier\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    542\u001b[0m         \u001b[0mtags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassifier_tags\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mClassifierTags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'super' object has no attribute '__sklearn_tags__'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58573637"
      },
      "source": [
        "**Reasoning**:\n",
        "The error `AttributeError: 'super' object has no attribute '__sklearn_tags__'` during the ANN tuning with `KerasClassifier` suggests an incompatibility issue between scikeras and the scikit-learn version, or potentially how the Keras model is being wrapped. A common workaround is to explicitly define the `model_name` parameter in the `KerasClassifier` wrapper and potentially update scikit-learn if necessary. Since updating scikit-learn is outside the scope, try the `model_name` fix first. Also, ensure the build function is correctly defined and doesn't rely on external variables in a way that interferes with the wrapper. The build function itself seems correct, so focus on the `KerasClassifier` instantiation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 654
        },
        "id": "UJUR5OfD6HCS",
        "outputId": "29a60689-d50b-45cc-842c-a11a90fa9d9f"
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, StratifiedKFold\n",
        "from scikeras.wrappers import KerasClassifier\n",
        "import tensorflow as tf\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import xgboost as xgb\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import numpy as np\n",
        "\n",
        "# Define parameter grids/distributions\n",
        "# Logistic Regression\n",
        "lr_param_grid = {\n",
        "    'C': [0.01, 0.1, 1, 10, 100],\n",
        "    'penalty': ['l2']\n",
        "}\n",
        "\n",
        "# XGBoost\n",
        "xgb_param_dist = {\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    'learning_rate': [0.01, 0.05, 0.1],\n",
        "    'max_depth': [3, 5, 7],\n",
        "    'subsample': [0.7, 0.8, 0.9],\n",
        "    'colsample_bytree': [0.7, 0.8, 0.9],\n",
        "    'gamma': [0, 0.1, 0.2]\n",
        "}\n",
        "\n",
        "# ANN - using a simple architecture for tuning\n",
        "def build_ann_tune(meta, n_neurons=7, l2_reg=0.0): # Added 'meta' argument as required by KerasClassifier>=0.5.0\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Dense(n_neurons, activation='relu', input_shape=(meta[\"n_features_in_\"],), # Use meta[\"n_features_in_\"]\n",
        "                              kernel_regularizer=tf.keras.regularizers.l2(l2_reg)),\n",
        "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "ann_param_grid = {\n",
        "    'model__n_neurons': [5, 7, 10],\n",
        "    'model__l2_reg': [0.0, 0.01, 0.001],\n",
        "    'batch_size': [32, 64],\n",
        "    'epochs': [30, 50]\n",
        "}\n",
        "\n",
        "# Cross-validation strategy\n",
        "cv_strategy = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
        "scoring_metric = 'roc_auc'\n",
        "\n",
        "# --- Logistic Regression Tuning ---\n",
        "print(\"Starting Logistic Regression Hyperparameter Tuning...\")\n",
        "lr_model_tune = LogisticRegression(random_state=42, max_iter=1000)\n",
        "lr_grid_search = GridSearchCV(\n",
        "    lr_model_tune,\n",
        "    lr_param_grid,\n",
        "    scoring=scoring_metric,\n",
        "    cv=cv_strategy,\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "lr_grid_search.fit(X_train_scaled, y_train)\n",
        "\n",
        "print(\"\\nBest parameters for Logistic Regression:\")\n",
        "print(lr_grid_search.best_params_)\n",
        "print(f\"Best CV {scoring_metric} score for Logistic Regression: {lr_grid_search.best_score_:.4f}\")\n",
        "\n",
        "# Store the best model\n",
        "best_lr_model = lr_grid_search.best_estimator_\n",
        "all_models['Logistic Regression']['tuned_model'] = best_lr_model\n",
        "\n",
        "\n",
        "# --- XGBoost Tuning ---\n",
        "print(\"\\nStarting XGBoost Hyperparameter Tuning...\")\n",
        "xgb_model_tune = xgb.XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss')\n",
        "xgb_random_search = RandomizedSearchCV(\n",
        "    xgb_model_tune,\n",
        "    xgb_param_dist,\n",
        "    n_iter=20,\n",
        "    scoring=scoring_metric,\n",
        "    cv=cv_strategy,\n",
        "    random_state=42,\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "xgb_random_search.fit(X_train_scaled, y_train)\n",
        "\n",
        "print(\"\\nBest parameters for XGBoost:\")\n",
        "print(xgb_random_search.best_params_)\n",
        "print(f\"Best CV {scoring_metric} score for XGBoost: {xgb_random_search.best_score_:.4f}\")\n",
        "\n",
        "# Store the best model\n",
        "best_xgb_model = xgb_random_search.best_estimator_\n",
        "all_models['XGBoost']['tuned_model'] = best_xgb_model\n",
        "\n",
        "\n",
        "# --- ANN Tuning ---\n",
        "print(\"\\nStarting ANN Hyperparameter Tuning...\")\n",
        "\n",
        "# Explicitly define model_name and pass the build function\n",
        "ann_model_tune = KerasClassifier(model=build_ann_tune, verbose=0, model__name=\"tuned_ann\")\n",
        "\n",
        "ann_grid_search = GridSearchCV(\n",
        "    ann_model_tune,\n",
        "    ann_param_grid,\n",
        "    scoring=scoring_metric,\n",
        "    cv=cv_strategy,\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "ann_grid_search.fit(X_train_scaled, y_train)\n",
        "\n",
        "\n",
        "print(\"\\nBest parameters for ANN:\")\n",
        "print(ann_grid_search.best_params_)\n",
        "print(f\"Best CV {scoring_metric} score for ANN: {ann_grid_search.best_score_:.4f}\")\n",
        "\n",
        "# Store the best model\n",
        "best_ann_params = ann_grid_search.best_params_\n",
        "best_ann_model = build_ann_tune(\n",
        "     meta={\"n_features_in_\": X_train_scaled.shape[1]}, # Pass meta explicitly for the final build\n",
        "     n_neurons=best_ann_params['model__n_neurons'],\n",
        "     l2_reg=best_ann_params['model__l2_reg']\n",
        ")\n",
        "best_ann_model.fit(X_train_scaled, y_train, epochs=best_ann_params['epochs'], batch_size=best_ann_params['batch_size'], verbose=0)\n",
        "\n",
        "all_models['ANN']['tuned_model'] = best_ann_model\n",
        "\n",
        "print(\"\\nHyperparameter tuning complete for all models.\")"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Logistic Regression Hyperparameter Tuning...\n",
            "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n",
            "\n",
            "Best parameters for Logistic Regression:\n",
            "{'C': 100, 'penalty': 'l2'}\n",
            "Best CV roc_auc score for Logistic Regression: 0.6945\n",
            "\n",
            "Starting XGBoost Hyperparameter Tuning...\n",
            "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n",
            "\n",
            "Best parameters for XGBoost:\n",
            "{'subsample': 0.8, 'n_estimators': 200, 'max_depth': 3, 'learning_rate': 0.05, 'gamma': 0.1, 'colsample_bytree': 0.8}\n",
            "Best CV roc_auc score for XGBoost: 0.6961\n",
            "\n",
            "Starting ANN Hyperparameter Tuning...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'super' object has no attribute '__sklearn_tags__'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2885052155.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    106\u001b[0m )\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m \u001b[0mann_grid_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_scaled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1387\u001b[0m                 )\n\u001b[1;32m   1388\u001b[0m             ):\n\u001b[0;32m-> 1389\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mrouted_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_routed_params_for_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 933\u001b[0;31m         \u001b[0mcv_orig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_cv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    934\u001b[0m         \u001b[0mn_splits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv_orig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_n_splits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mrouted_params\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplitter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mis_classifier\u001b[0;34m(estimator)\u001b[0m\n\u001b[1;32m   1235\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_estimator_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"classifier\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1237\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mget_tags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"classifier\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/utils/_tags.py\u001b[0m in \u001b[0;36mget_tags\u001b[0;34m(estimator)\u001b[0m\n\u001b[1;32m    428\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mklass\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreversed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmro\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m\"__sklearn_tags__\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mklass\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 430\u001b[0;31m                 \u001b[0msklearn_tags_provider\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mklass\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mklass\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__sklearn_tags__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    431\u001b[0m                 \u001b[0mclass_order\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mklass\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0;34m\"_more_tags\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mklass\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36m__sklearn_tags__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    538\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    539\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__sklearn_tags__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 540\u001b[0;31m         \u001b[0mtags\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__sklearn_tags__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    541\u001b[0m         \u001b[0mtags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"classifier\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    542\u001b[0m         \u001b[0mtags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassifier_tags\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mClassifierTags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'super' object has no attribute '__sklearn_tags__'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ace3bcb"
      },
      "source": [
        "**Reasoning**:\n",
        "The same `AttributeError` persists, indicating a deeper compatibility issue between the installed scikeras version and the scikit-learn version in the environment, or potentially how the base Keras model is interacting with the scikit-learn wrapper. While updating scikit-learn is the most likely fix, it's outside the allowed operations. As a last attempt within the current constraints, explicitly set `model_name` in the `KerasClassifier` might resolve some internal tagging issues, though it's less likely than a version conflict. If this doesn't work, the task cannot be fully completed within the given constraints.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 654
        },
        "id": "oTJfTecB6P9E",
        "outputId": "c4350442-e637-4fee-fdc3-de0a584c2d51"
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, StratifiedKFold\n",
        "from scikeras.wrappers import KerasClassifier\n",
        "import tensorflow as tf\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import xgboost as xgb\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import numpy as np\n",
        "\n",
        "# Define parameter grids/distributions\n",
        "# Logistic Regression\n",
        "lr_param_grid = {\n",
        "    'C': [0.01, 0.1, 1, 10, 100],\n",
        "    'penalty': ['l2']\n",
        "}\n",
        "\n",
        "# XGBoost\n",
        "xgb_param_dist = {\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    'learning_rate': [0.01, 0.05, 0.1],\n",
        "    'max_depth': [3, 5, 7],\n",
        "    'subsample': [0.7, 0.8, 0.9],\n",
        "    'colsample_bytree': [0.7, 0.8, 0.9],\n",
        "    'gamma': [0, 0.1, 0.2]\n",
        "}\n",
        "\n",
        "# ANN - using a simple architecture for tuning\n",
        "def build_ann_tune(meta, n_neurons=7, l2_reg=0.0):\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Dense(n_neurons, activation='relu', input_shape=(meta[\"n_features_in_\"],),\n",
        "                              kernel_regularizer=tf.keras.regularizers.l2(l2_reg)),\n",
        "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "ann_param_grid = {\n",
        "    'model__n_neurons': [5, 7, 10],\n",
        "    'model__l2_reg': [0.0, 0.01, 0.001],\n",
        "    'batch_size': [32, 64],\n",
        "    'epochs': [30, 50]\n",
        "}\n",
        "\n",
        "# Cross-validation strategy\n",
        "cv_strategy = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
        "scoring_metric = 'roc_auc'\n",
        "\n",
        "# --- Logistic Regression Tuning ---\n",
        "print(\"Starting Logistic Regression Hyperparameter Tuning...\")\n",
        "lr_model_tune = LogisticRegression(random_state=42, max_iter=1000)\n",
        "lr_grid_search = GridSearchCV(\n",
        "    lr_model_tune,\n",
        "    lr_param_grid,\n",
        "    scoring=scoring_metric,\n",
        "    cv=cv_strategy,\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "lr_grid_search.fit(X_train_scaled, y_train)\n",
        "\n",
        "print(\"\\nBest parameters for Logistic Regression:\")\n",
        "print(lr_grid_search.best_params_)\n",
        "print(f\"Best CV {scoring_metric} score for Logistic Regression: {lr_grid_search.best_score_:.4f}\")\n",
        "\n",
        "# Store the best model\n",
        "best_lr_model = lr_grid_search.best_estimator_\n",
        "all_models['Logistic Regression']['tuned_model'] = best_lr_model\n",
        "\n",
        "\n",
        "# --- XGBoost Tuning ---\n",
        "print(\"\\nStarting XGBoost Hyperparameter Tuning...\")\n",
        "xgb_model_tune = xgb.XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss')\n",
        "xgb_random_search = RandomizedSearchCV(\n",
        "    xgb_model_tune,\n",
        "    xgb_param_dist,\n",
        "    n_iter=20,\n",
        "    scoring=scoring_metric,\n",
        "    cv=cv_strategy,\n",
        "    random_state=42,\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "xgb_random_search.fit(X_train_scaled, y_train)\n",
        "\n",
        "print(\"\\nBest parameters for XGBoost:\")\n",
        "print(xgb_random_search.best_params_)\n",
        "print(f\"Best CV {scoring_metric} score for XGBoost: {xgb_random_search.best_score_:.4f}\")\n",
        "\n",
        "# Store the best model\n",
        "best_xgb_model = xgb_random_search.best_estimator_\n",
        "all_models['XGBoost']['tuned_model'] = best_xgb_model\n",
        "\n",
        "\n",
        "# --- ANN Tuning ---\n",
        "print(\"\\nStarting ANN Hyperparameter Tuning...\")\n",
        "\n",
        "# Explicitly define model_name and pass the build function\n",
        "ann_model_tune = KerasClassifier(model=build_ann_tune, verbose=0, model__name=\"tuned_ann\")\n",
        "\n",
        "ann_grid_search = GridSearchCV(\n",
        "    ann_model_tune,\n",
        "    ann_param_grid,\n",
        "    scoring=scoring_metric,\n",
        "    cv=cv_strategy,\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "ann_grid_search.fit(X_train_scaled, y_train)\n",
        "\n",
        "\n",
        "print(\"\\nBest parameters for ANN:\")\n",
        "print(ann_grid_search.best_params_)\n",
        "print(f\"Best CV {scoring_metric} score for ANN: {ann_grid_search.best_score_:.4f}\")\n",
        "\n",
        "# Store the best model\n",
        "best_ann_params = ann_grid_search.best_params_\n",
        "best_ann_model = build_ann_tune(\n",
        "     meta={\"n_features_in_\": X_train_scaled.shape[1]}, # Pass meta explicitly for the final build\n",
        "     n_neurons=best_ann_params['model__n_neurons'],\n",
        "     l2_reg=best_ann_params['model__l2_reg']\n",
        ")\n",
        "best_ann_model.fit(X_train_scaled, y_train, epochs=best_ann_params['epochs'], batch_size=best_ann_params['batch_size'], verbose=0)\n",
        "\n",
        "all_models['ANN']['tuned_model'] = best_ann_model\n",
        "\n",
        "print(\"\\nHyperparameter tuning complete for all models.\")"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Logistic Regression Hyperparameter Tuning...\n",
            "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n",
            "\n",
            "Best parameters for Logistic Regression:\n",
            "{'C': 100, 'penalty': 'l2'}\n",
            "Best CV roc_auc score for Logistic Regression: 0.6945\n",
            "\n",
            "Starting XGBoost Hyperparameter Tuning...\n",
            "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n",
            "\n",
            "Best parameters for XGBoost:\n",
            "{'subsample': 0.8, 'n_estimators': 200, 'max_depth': 3, 'learning_rate': 0.05, 'gamma': 0.1, 'colsample_bytree': 0.8}\n",
            "Best CV roc_auc score for XGBoost: 0.6961\n",
            "\n",
            "Starting ANN Hyperparameter Tuning...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'super' object has no attribute '__sklearn_tags__'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-587106561.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    106\u001b[0m )\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m \u001b[0mann_grid_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_scaled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1387\u001b[0m                 )\n\u001b[1;32m   1388\u001b[0m             ):\n\u001b[0;32m-> 1389\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mrouted_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_routed_params_for_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 933\u001b[0;31m         \u001b[0mcv_orig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_cv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    934\u001b[0m         \u001b[0mn_splits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv_orig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_n_splits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mrouted_params\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplitter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mis_classifier\u001b[0;34m(estimator)\u001b[0m\n\u001b[1;32m   1235\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_estimator_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"classifier\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1237\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mget_tags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"classifier\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/utils/_tags.py\u001b[0m in \u001b[0;36mget_tags\u001b[0;34m(estimator)\u001b[0m\n\u001b[1;32m    428\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mklass\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreversed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmro\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m\"__sklearn_tags__\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mklass\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 430\u001b[0;31m                 \u001b[0msklearn_tags_provider\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mklass\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mklass\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__sklearn_tags__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    431\u001b[0m                 \u001b[0mclass_order\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mklass\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0;34m\"_more_tags\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mklass\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36m__sklearn_tags__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    538\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    539\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__sklearn_tags__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 540\u001b[0;31m         \u001b[0mtags\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__sklearn_tags__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    541\u001b[0m         \u001b[0mtags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"classifier\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    542\u001b[0m         \u001b[0mtags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassifier_tags\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mClassifierTags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'super' object has no attribute '__sklearn_tags__'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ce8b7de9"
      },
      "source": [
        "## Explore different ann architectures\n",
        "\n",
        "### Subtask:\n",
        "Explore different Artificial Neural Network (ANN) architectures by trying varying numbers of layers, neurons per layer, activation functions, and optimizers.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80a99607"
      },
      "source": [
        "**Reasoning**:\n",
        "Define functions for different ANN architectures and evaluate them using cross-validation, storing the results.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "56128fcf",
        "outputId": "ce4436d7-7ddd-4282-c090-a81463216cdf"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Define different ANN architectures\n",
        "def build_ann_arch1(input_shape):\n",
        "    \"\"\"Simple ANN with one hidden layer.\"\"\"\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Dense(10, activation='relu', input_shape=(input_shape,)),\n",
        "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "def build_ann_arch2(input_shape):\n",
        "    \"\"\"ANN with two hidden layers.\"\"\"\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Dense(14, activation='relu', input_shape=(input_shape,)),\n",
        "        tf.keras.layers.Dense(7, activation='relu'),\n",
        "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "def build_ann_arch3(input_shape):\n",
        "    \"\"\"ANN with two hidden layers and tanh activation.\"\"\"\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Dense(14, activation='tanh', input_shape=(input_shape,)),\n",
        "        tf.keras.layers.Dense(7, activation='tanh'),\n",
        "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "def build_ann_arch4(input_shape):\n",
        "    \"\"\"ANN with two hidden layers and RMSprop optimizer.\"\"\"\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Dense(14, activation='relu', input_shape=(input_shape,)),\n",
        "        tf.keras.layers.Dense(7, activation='relu'),\n",
        "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "    model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Evaluate each architecture using CV and store results\n",
        "ann_architectures = {\n",
        "    'ANN_Arch1_1HL_ReLU_Adam': build_ann_arch1,\n",
        "    'ANN_Arch2_2HL_ReLU_Adam': build_ann_arch2,\n",
        "    'ANN_Arch3_2HL_Tanh_Adam': build_ann_arch3,\n",
        "    'ANN_Arch4_2HL_ReLU_RMSprop': build_ann_arch4\n",
        "}\n",
        "\n",
        "print(\"\\nEvaluating different ANN architectures with 5-fold CV...\")\n",
        "\n",
        "for name, build_func in ann_architectures.items():\n",
        "    print(f\"\\n--- Evaluating {name} ---\")\n",
        "    # Pass input_shape to the build function\n",
        "    cv_results, fold_details = train_and_evaluate_keras_with_cv(\n",
        "        lambda: build_func(X_train_scaled.shape[1]), # Use lambda to pass input_shape\n",
        "        X_train_scaled,\n",
        "        y_train,\n",
        "        cv_folds=5,\n",
        "        epochs=50,\n",
        "        batch_size=32,\n",
        "        random_state=42,\n",
        "        model_name=name,\n",
        "        verbose=True\n",
        "    )\n",
        "    all_cv_results[name] = cv_results\n",
        "    # Store the last trained model for potential final evaluation if needed\n",
        "    # Note: In a full pipeline, you'd train the best architecture on the full train set here\n",
        "    # For this subtask, we just store CV results\n",
        "    # Example (if you wanted to store a final model for each):\n",
        "    # final_model = build_func(X_train_scaled.shape[1])\n",
        "    # final_model.fit(X_train_scaled, y_train, epochs=50, batch_size=32, verbose=0)\n",
        "    # all_models[name] = {'model': final_model, 'y_true': y_test, 'y_pred_proba': final_model.predict(X_test_scaled).flatten(), 'cv_results': cv_results}\n",
        "\n",
        "print(\"\\nFinished evaluating different ANN architectures.\")\n",
        "\n",
        "# Compare the results\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ANN ARCHITECTURE COMPARISON (CV Results)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "ann_comparison_data = []\n",
        "for model_name, cv_results in all_cv_results.items():\n",
        "    if model_name.startswith('ANN_Arch'): # Filter only the new ANN architectures\n",
        "         ann_comparison_data.append({\n",
        "            'Model': model_name,\n",
        "            'Accuracy': f\"{cv_results['accuracy_mean']:.4f} ± {cv_results['accuracy_std']:.4f}\",\n",
        "            'Macro-F1': f\"{cv_results['macro_f1_mean']:.4f} ± {cv_results['macro_f1_std']:.4f}\",\n",
        "            'Recall (Class 1)': f\"{cv_results['recall_class_1_mean']:.4f} ± {cv_results['recall_class_1_std']:.4f}\",\n",
        "            'ROC-AUC': f\"{cv_results['roc_auc_mean']:.4f} ± {cv_results['roc_auc_std']:.4f}\",\n",
        "            'Avg Time (s)': f\"{cv_results['time_mean']:.2f} ± {cv_results['time_std']:.2f}\"\n",
        "        })\n",
        "\n",
        "ann_comparison_df = pd.DataFrame(ann_comparison_data)\n",
        "print(ann_comparison_df.to_string(index=False))\n",
        "print(\"=\"*80 + \"\\n\")"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluating different ANN architectures with 5-fold CV...\n",
            "\n",
            "--- Evaluating ANN_Arch1_1HL_ReLU_Adam ---\n",
            "\n",
            "--- ANN_Arch1_1HL_ReLU_Adam Cross-Validation ---\n",
            "  Fold 1 completed in 53.37 seconds.\n",
            "    Accuracy: 0.6405, Macro-F1: 0.6404, Recall (Class 1): 0.6523, ROC-AUC: 0.6883\n",
            "  Fold 2 completed in 53.95 seconds.\n",
            "    Accuracy: 0.6279, Macro-F1: 0.6276, Recall (Class 1): 0.6601, ROC-AUC: 0.6794\n",
            "  Fold 3 completed in 56.24 seconds.\n",
            "    Accuracy: 0.6401, Macro-F1: 0.6392, Recall (Class 1): 0.6900, ROC-AUC: 0.7048\n",
            "  Fold 4 completed in 52.06 seconds.\n",
            "    Accuracy: 0.6313, Macro-F1: 0.6313, Recall (Class 1): 0.6388, ROC-AUC: 0.6826\n",
            "  Fold 5 completed in 50.52 seconds.\n",
            "    Accuracy: 0.6387, Macro-F1: 0.6352, Recall (Class 1): 0.7366, ROC-AUC: 0.6960\n",
            "\n",
            "--- Average Results for ANN_Arch1_1HL_ReLU_Adam ---\n",
            "  Accuracy: 0.6357 ± 0.0051\n",
            "  Macro-F1: 0.6347 ± 0.0048\n",
            "  Recall (Class 1): 0.6756 ± 0.0348\n",
            "  ROC-AUC: 0.6902 ± 0.0092\n",
            "---------------------------------------------------\n",
            "\n",
            "--- Evaluating ANN_Arch2_2HL_ReLU_Adam ---\n",
            "\n",
            "--- ANN_Arch2_2HL_ReLU_Adam Cross-Validation ---\n",
            "  Fold 1 completed in 52.44 seconds.\n",
            "    Accuracy: 0.6388, Macro-F1: 0.6383, Recall (Class 1): 0.6733, ROC-AUC: 0.6898\n",
            "  Fold 2 completed in 53.91 seconds.\n",
            "    Accuracy: 0.6296, Macro-F1: 0.6293, Recall (Class 1): 0.6589, ROC-AUC: 0.6810\n",
            "  Fold 3 completed in 54.02 seconds.\n",
            "    Accuracy: 0.6466, Macro-F1: 0.6465, Recall (Class 1): 0.6646, ROC-AUC: 0.7035\n",
            "  Fold 4 completed in 52.62 seconds.\n",
            "    Accuracy: 0.6344, Macro-F1: 0.6338, Recall (Class 1): 0.6772, ROC-AUC: 0.6829\n",
            "  Fold 5 completed in 51.93 seconds.\n",
            "    Accuracy: 0.6457, Macro-F1: 0.6453, Recall (Class 1): 0.6795, ROC-AUC: 0.6969\n",
            "\n",
            "--- Average Results for ANN_Arch2_2HL_ReLU_Adam ---\n",
            "  Accuracy: 0.6390 ± 0.0065\n",
            "  Macro-F1: 0.6386 ± 0.0066\n",
            "  Recall (Class 1): 0.6707 ± 0.0078\n",
            "  ROC-AUC: 0.6908 ± 0.0085\n",
            "---------------------------------------------------\n",
            "\n",
            "--- Evaluating ANN_Arch3_2HL_Tanh_Adam ---\n",
            "\n",
            "--- ANN_Arch3_2HL_Tanh_Adam Cross-Validation ---\n",
            "  Fold 1 completed in 52.89 seconds.\n",
            "    Accuracy: 0.6427, Macro-F1: 0.6421, Recall (Class 1): 0.6846, ROC-AUC: 0.6899\n",
            "  Fold 2 completed in 52.13 seconds.\n",
            "    Accuracy: 0.6353, Macro-F1: 0.6348, Recall (Class 1): 0.6702, ROC-AUC: 0.6832\n",
            "  Fold 3 completed in 52.62 seconds.\n",
            "    Accuracy: 0.6426, Macro-F1: 0.6421, Recall (Class 1): 0.6052, ROC-AUC: 0.7041\n",
            "  Fold 4 completed in 51.77 seconds.\n",
            "    Accuracy: 0.6370, Macro-F1: 0.6363, Recall (Class 1): 0.6812, ROC-AUC: 0.6851\n",
            "  Fold 5 completed in 52.16 seconds.\n",
            "    Accuracy: 0.6415, Macro-F1: 0.6414, Recall (Class 1): 0.6235, ROC-AUC: 0.6974\n",
            "\n",
            "--- Average Results for ANN_Arch3_2HL_Tanh_Adam ---\n",
            "  Accuracy: 0.6398 ± 0.0031\n",
            "  Macro-F1: 0.6393 ± 0.0031\n",
            "  Recall (Class 1): 0.6529 ± 0.0324\n",
            "  ROC-AUC: 0.6919 ± 0.0078\n",
            "---------------------------------------------------\n",
            "\n",
            "--- Evaluating ANN_Arch4_2HL_ReLU_RMSprop ---\n",
            "\n",
            "--- ANN_Arch4_2HL_ReLU_RMSprop Cross-Validation ---\n",
            "  Fold 1 completed in 50.61 seconds.\n",
            "    Accuracy: 0.6382, Macro-F1: 0.6381, Recall (Class 1): 0.6207, ROC-AUC: 0.6884\n",
            "  Fold 2 completed in 51.08 seconds.\n",
            "    Accuracy: 0.6291, Macro-F1: 0.6288, Recall (Class 1): 0.6561, ROC-AUC: 0.6797\n",
            "  Fold 3 completed in 49.82 seconds.\n",
            "    Accuracy: 0.6480, Macro-F1: 0.6461, Recall (Class 1): 0.7223, ROC-AUC: 0.7043\n",
            "  Fold 4 completed in 50.63 seconds.\n",
            "    Accuracy: 0.6308, Macro-F1: 0.6308, Recall (Class 1): 0.6269, ROC-AUC: 0.6837\n",
            "  Fold 5 completed in 51.64 seconds.\n",
            "    Accuracy: 0.6455, Macro-F1: 0.6453, Recall (Class 1): 0.6230, ROC-AUC: 0.6962\n",
            "\n",
            "--- Average Results for ANN_Arch4_2HL_ReLU_RMSprop ---\n",
            "  Accuracy: 0.6383 ± 0.0076\n",
            "  Macro-F1: 0.6378 ± 0.0071\n",
            "  Recall (Class 1): 0.6498 ± 0.0384\n",
            "  ROC-AUC: 0.6905 ± 0.0089\n",
            "------------------------------------------------------\n",
            "\n",
            "Finished evaluating different ANN architectures.\n",
            "\n",
            "================================================================================\n",
            "ANN ARCHITECTURE COMPARISON (CV Results)\n",
            "================================================================================\n",
            "                     Model        Accuracy        Macro-F1 Recall (Class 1)         ROC-AUC Avg Time (s)\n",
            "   ANN_Arch1_1HL_ReLU_Adam 0.6357 ± 0.0051 0.6347 ± 0.0048  0.6756 ± 0.0348 0.6902 ± 0.0092 53.23 ± 1.91\n",
            "   ANN_Arch2_2HL_ReLU_Adam 0.6390 ± 0.0065 0.6386 ± 0.0066  0.6707 ± 0.0078 0.6908 ± 0.0085 52.98 ± 0.83\n",
            "   ANN_Arch3_2HL_Tanh_Adam 0.6398 ± 0.0031 0.6393 ± 0.0031  0.6529 ± 0.0324 0.6919 ± 0.0078 52.32 ± 0.39\n",
            "ANN_Arch4_2HL_ReLU_RMSprop 0.6383 ± 0.0076 0.6378 ± 0.0071  0.6498 ± 0.0384 0.6905 ± 0.0089 50.75 ± 0.60\n",
            "================================================================================\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6cfdd3b6"
      },
      "source": [
        "## Advanced balancing techniques\n",
        "\n",
        "### Subtask:\n",
        "Investigate and implement more sophisticated data balancing methods beyond simple undersampling/oversampling, such as SMOTE variants (ADASYN, BorderlineSMOTE), or ensemble techniques like BalancedBaggingClassifier or BalancedRandomForestClassifier.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "228fc3c6"
      },
      "source": [
        "**Reasoning**:\n",
        "Import necessary libraries for advanced balancing techniques, including SMOTE variants and ensemble methods. Then, implement functions to apply these techniques to the training data. Finally, evaluate a model with these balanced datasets using cross-validation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 453
        },
        "id": "c70442aa",
        "outputId": "4ea0de70-c548-439f-8b92-1950df48430f"
      },
      "source": [
        "from imblearn.over_sampling import SMOTE, ADASYN, BorderlineSMOTE\n",
        "from imblearn.under_sampling import RandomUnderSampler, RandomOverSampler # Imported for clarity, already in previous cells\n",
        "from imblearn.ensemble import BalancedBaggingClassifier, BalancedRandomForestClassifier\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import LogisticRegression # Using LR as a representative model\n",
        "\n",
        "def apply_balancing_technique(X_train, y_train, method='SMOTE', random_state=None):\n",
        "    \"\"\"\n",
        "    Applies a specified advanced balancing technique to the training data.\n",
        "\n",
        "    Args:\n",
        "        X_train (pd.DataFrame or np.ndarray): Training features.\n",
        "        y_train (pd.Series or np.ndarray): Training target.\n",
        "        method (str): The balancing method to use ('SMOTE', 'ADASYN', 'BorderlineSMOTE',\n",
        "                      'BalancedBagging', 'BalancedRandomForest'). Note: Ensemble methods\n",
        "                      don't resample the data directly, they incorporate balancing\n",
        "                      into the model training. For these, the original data is returned.\n",
        "        random_state (int, optional): Random state for reproducibility. Defaults to None.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing:\n",
        "            - pd.DataFrame or np.ndarray: Resampled features.\n",
        "            - pd.Series or np.ndarray: Resampled target.\n",
        "            - dict: Information about the balancing applied.\n",
        "    \"\"\"\n",
        "    balance_info = {\"method_applied\": method}\n",
        "    X_resampled, y_resampled = X_train.copy(), y_train.copy()\n",
        "\n",
        "    if method == 'SMOTE':\n",
        "        sampler = SMOTE(random_state=random_state)\n",
        "        X_resampled, y_resampled = sampler.fit_resample(X_train, y_train)\n",
        "    elif method == 'ADASYN':\n",
        "        sampler = ADASYN(random_state=random_state)\n",
        "        X_resampled, y_resampled = sampler.fit_resample(X_train, y_train)\n",
        "    elif method == 'BorderlineSMOTE':\n",
        "        sampler = BorderlineSMOTE(random_state=random_state)\n",
        "        X_resampled, y_resampled = sampler.fit_resample(X_train, y_train)\n",
        "    elif method in ['BalancedBagging', 'BalancedRandomForest']:\n",
        "        # Ensemble methods handle balancing internally, return original data\n",
        "        print(f\"  {method} handles balancing internally. Returning original training data.\")\n",
        "        balance_info['note'] = f'{method} is an ensemble method and handles balancing internally.'\n",
        "        pass # X_resampled, y_resampled are already copies of original\n",
        "    else:\n",
        "        print(f\"Warning: Unknown balancing method '{method}'. No balancing applied.\")\n",
        "        balance_info['note'] = 'No balancing applied due to unknown method'\n",
        "\n",
        "    if method not in ['BalancedBagging', 'BalancedRandomForest']:\n",
        "        balance_info['original_counts'] = y_train.value_counts().to_dict()\n",
        "        balance_info['resampled_counts'] = pd.Series(y_resampled).value_counts().to_dict()\n",
        "\n",
        "\n",
        "    return X_resampled, y_resampled, balance_info\n",
        "\n",
        "# List of advanced balancing techniques to evaluate (excluding the previously used 'downsample_upsample')\n",
        "advanced_balancing_methods = ['SMOTE', 'ADASYN', 'BorderlineSMOTE', 'BalancedBagging', 'BalancedRandomForest']\n",
        "\n",
        "# Use Logistic Regression as a representative model for evaluation\n",
        "representative_model = LogisticRegression(random_state=42, max_iter=1000)\n",
        "model_name_eval = 'Logistic Regression' # Use LR as the comparison model\n",
        "\n",
        "print(f\"\\nEvaluating {model_name_eval} with advanced balancing techniques...\")\n",
        "\n",
        "for method in advanced_balancing_methods:\n",
        "    print(f\"\\n--- Evaluating with {method} ---\")\n",
        "\n",
        "    if method in ['BalancedBagging', 'BalancedRandomForest']:\n",
        "        # For ensemble methods, train the specific model\n",
        "        if method == 'BalancedBagging':\n",
        "            model_to_eval = BalancedBaggingClassifier(estimator=LogisticRegression(max_iter=1000),\n",
        "                                                      n_estimators=10, random_state=42, n_jobs=-1)\n",
        "            current_model_name = 'BalancedBagging (LR)'\n",
        "        else: # BalancedRandomForest\n",
        "            model_to_eval = BalancedRandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
        "            current_model_name = 'BalancedRandomForest'\n",
        "\n",
        "        # Ensemble methods don't need pre-resampling, so pass original X_train_scaled, y_train\n",
        "        # The function apply_balancing_technique will return original data for these methods\n",
        "        X_balanced_train, y_balanced_train, balance_info = apply_balancing_technique(\n",
        "            X_train_scaled, y_train, method=method, random_state=42\n",
        "        )\n",
        "\n",
        "        # Evaluate ensemble model with CV on the original training data (scaled)\n",
        "        # train_and_evaluate_with_cv handles the internal balancing for these models\n",
        "        cv_results, fold_details = train_and_evaluate_with_cv(\n",
        "            model_to_eval,\n",
        "            X_balanced_train, # Pass original scaled train data\n",
        "            y_balanced_train,\n",
        "            cv_folds=5,\n",
        "            random_state=42,\n",
        "            model_name=current_model_name,\n",
        "            verbose=True\n",
        "        )\n",
        "        all_cv_results[current_model_name] = cv_results\n",
        "\n",
        "    else:\n",
        "        # For SMOTE variants, resample the data first\n",
        "        X_balanced_train, y_balanced_train, balance_info = apply_balancing_technique(\n",
        "            X_train_scaled, y_train, method=method, random_state=42\n",
        "        )\n",
        "        print(f\"  Original counts: {balance_info['original_counts']}\")\n",
        "        print(f\"  Resampled counts: {balance_info['resampled_counts']}\")\n",
        "\n",
        "        # Evaluate the representative model (Logistic Regression) on the resampled data\n",
        "        current_model_name = f'{model_name_eval}_{method}'\n",
        "        cv_results, fold_details = train_and_evaluate_with_cv(\n",
        "            representative_model,\n",
        "            X_balanced_train, # Pass resampled data\n",
        "            y_balanced_train,\n",
        "            cv_folds=5,\n",
        "            random_state=42,\n",
        "            model_name=current_model_name,\n",
        "            verbose=True\n",
        "        )\n",
        "        all_cv_results[current_model_name] = cv_results\n",
        "\n",
        "print(\"\\nFinished evaluating models with advanced balancing techniques.\")\n",
        "\n",
        "# Compare the cross-validation results, including the previous 'downsample_upsample' LR result\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ADVANCED BALANCING TECHNIQUES COMPARISON (CV Results - Logistic Regression & Ensembles)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "balancing_comparison_data = []\n",
        "# Include the original LR result with downsample_upsample for comparison\n",
        "if 'Logistic Regression' in all_cv_results:\n",
        "    cv_res_lr_downup = all_cv_results['Logistic Regression']\n",
        "    balancing_comparison_data.append({\n",
        "        'Model + Balancing': f'Logistic Regression (downsample_upsample)',\n",
        "        'Macro-F1': f\"{cv_res_lr_downup['macro_f1_mean']:.4f} ± {cv_res_lr_downup['macro_f1_std']:.4f}\",\n",
        "        'Recall (Class 1)': f\"{cv_res_lr_downup['recall_class_1_mean']:.4f} ± {cv_res_lr_downup['recall_class_1_std']:.4f}\",\n",
        "        'ROC-AUC': f\"{cv_res_lr_downup['roc_auc_mean']:.4f} ± {cv_res_lr_downup['roc_auc_std']:.4f}\",\n",
        "        'Avg Time (s)': f\"{cv_res_lr_downup['time_mean']:.2f} ± {cv_res_lr_downup['time_std']:.2f}\"\n",
        "    })\n",
        "\n",
        "\n",
        "for model_name, cv_results in all_cv_results.items():\n",
        "    # Filter for the results of the advanced balancing techniques evaluated in this step\n",
        "    if model_name in ['Logistic Regression_SMOTE', 'Logistic Regression_ADASYN', 'Logistic Regression_BorderlineSMOTE',\n",
        "                      'BalancedBagging (LR)', 'BalancedRandomForest']:\n",
        "        balancing_comparison_data.append({\n",
        "            'Model + Balancing': model_name,\n",
        "            'Macro-F1': f\"{cv_results['macro_f1_mean']:.4f} ± {cv_results['macro_f1_std']:.4f}\",\n",
        "            'Recall (Class 1)': f\"{cv_results['recall_class_1_mean']:.4f} ± {cv_results['recall_class_1_std']:.4f}\",\n",
        "            'ROC-AUC': f\"{cv_results['roc_auc_mean']:.4f} ± {cv_results['roc_auc_std']:.4f}\",\n",
        "            'Avg Time (s)': f\"{cv_results['time_mean']:.2f} ± {cv_results['time_std']:.2f}\"\n",
        "        })\n",
        "\n",
        "balancing_comparison_df = pd.DataFrame(balancing_comparison_data)\n",
        "# Sort by Macro-F1 mean descending for easier comparison\n",
        "balancing_comparison_df = balancing_comparison_df.sort_values(by='Macro-F1', ascending=False)\n",
        "\n",
        "print(balancing_comparison_df.to_string(index=False))\n",
        "print(\"=\"*80 + \"\\n\")\n"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "cannot import name 'RandomOverSampler' from 'imblearn.under_sampling' (/usr/local/lib/python3.12/dist-packages/imblearn/under_sampling/__init__.py)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-399996287.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mimblearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mover_sampling\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSMOTE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mADASYN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBorderlineSMOTE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mimblearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munder_sampling\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRandomUnderSampler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRandomOverSampler\u001b[0m \u001b[0;31m# Imported for clarity, already in previous cells\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mimblearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensemble\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBalancedBaggingClassifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBalancedRandomForestClassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStratifiedKFold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'RandomOverSampler' from 'imblearn.under_sampling' (/usr/local/lib/python3.12/dist-packages/imblearn/under_sampling/__init__.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "961637df"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous command failed because `RandomOverSampler` is in `imblearn.over_sampling`, not `imblearn.under_sampling`. The corrected import statement is needed to proceed with defining and applying the advanced balancing techniques.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aUIYOHkU-sUv",
        "outputId": "b68a9744-70c2-46dc-b76c-8c959d953e71"
      },
      "source": [
        "from imblearn.over_sampling import SMOTE, ADASYN, BorderlineSMOTE, RandomOverSampler\n",
        "from imblearn.under_sampling import RandomUnderSampler # Keep RandomUnderSampler from under_sampling\n",
        "from imblearn.ensemble import BalancedBaggingClassifier, BalancedRandomForestClassifier\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import LogisticRegression # Using LR as a representative model\n",
        "\n",
        "def apply_balancing_technique(X_train, y_train, method='SMOTE', random_state=None):\n",
        "    \"\"\"\n",
        "    Applies a specified advanced balancing technique to the training data.\n",
        "\n",
        "    Args:\n",
        "        X_train (pd.DataFrame or np.ndarray): Training features.\n",
        "        y_train (pd.Series or np.ndarray): Training target.\n",
        "        method (str): The balancing method to use ('SMOTE', 'ADASYN', 'BorderlineSMOTE',\n",
        "                      'BalancedBagging', 'BalancedRandomForest'). Note: Ensemble methods\n",
        "                      don't resample the data directly, they incorporate balancing\n",
        "                      into the model training. For these, the original data is returned.\n",
        "        random_state (int, optional): Random state for reproducibility. Defaults to None.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing:\n",
        "            - pd.DataFrame or np.ndarray: Resampled features.\n",
        "            - pd.Series or np.ndarray: Resampled target.\n",
        "            - dict: Information about the balancing applied.\n",
        "    \"\"\"\n",
        "    balance_info = {\"method_applied\": method}\n",
        "    X_resampled, y_resampled = X_train.copy(), y_train.copy()\n",
        "\n",
        "    if method == 'SMOTE':\n",
        "        sampler = SMOTE(random_state=random_state)\n",
        "        X_resampled, y_resampled = sampler.fit_resample(X_train, y_train)\n",
        "    elif method == 'ADASYN':\n",
        "        sampler = ADASYN(random_state=random_state)\n",
        "        X_resampled, y_resampled = sampler.fit_resample(X_train, y_train)\n",
        "    elif method == 'BorderlineSMOTE':\n",
        "        sampler = BorderlineSMOTE(random_state=random_state)\n",
        "        X_resampled, y_resampled = sampler.fit_resample(X_train, y_train)\n",
        "    elif method in ['BalancedBagging', 'BalancedRandomForest']:\n",
        "        # Ensemble methods handle balancing internally, return original data\n",
        "        print(f\"  {method} handles balancing internally. Returning original training data.\")\n",
        "        balance_info['note'] = f'{method} is an ensemble method and handles balancing internally.'\n",
        "        pass # X_resampled, y_resampled are already copies of original\n",
        "    else:\n",
        "        print(f\"Warning: Unknown balancing method '{method}'. No balancing applied.\")\n",
        "        balance_info['note'] = 'No balancing applied due to unknown method'\n",
        "\n",
        "    if method not in ['BalancedBagging', 'BalancedRandomForest']:\n",
        "        balance_info['original_counts'] = pd.Series(y_train).value_counts().to_dict()\n",
        "        balance_info['resampled_counts'] = pd.Series(y_resampled).value_counts().to_dict()\n",
        "\n",
        "\n",
        "    return X_resampled, y_resampled, balance_info\n",
        "\n",
        "# List of advanced balancing techniques to evaluate (excluding the previously used 'downsample_upsample')\n",
        "advanced_balancing_methods = ['SMOTE', 'ADASYN', 'BorderlineSMOTE', 'BalancedBagging', 'BalancedRandomForest']\n",
        "\n",
        "# Use Logistic Regression as a representative model for evaluation\n",
        "representative_model = LogisticRegression(random_state=42, max_iter=1000)\n",
        "model_name_eval = 'Logistic Regression' # Use LR as the comparison model\n",
        "\n",
        "print(f\"\\nEvaluating {model_name_eval} with advanced balancing techniques...\")\n",
        "\n",
        "for method in advanced_balancing_methods:\n",
        "    print(f\"\\n--- Evaluating with {method} ---\")\n",
        "\n",
        "    if method in ['BalancedBagging', 'BalancedRandomForest']:\n",
        "        # For ensemble methods, train the specific model\n",
        "        if method == 'BalancedBagging':\n",
        "            model_to_eval = BalancedBaggingClassifier(estimator=LogisticRegression(max_iter=1000),\n",
        "                                                      n_estimators=10, random_state=42, n_jobs=-1)\n",
        "            current_model_name = 'BalancedBagging (LR)'\n",
        "        else: # BalancedRandomForest\n",
        "            model_to_eval = BalancedRandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
        "            current_model_name = 'BalancedRandomForest'\n",
        "\n",
        "        # Ensemble methods don't need pre-resampling, so pass original X_train_scaled, y_train\n",
        "        # The function apply_balancing_technique will return original data for these methods\n",
        "        X_balanced_train, y_balanced_train, balance_info = apply_balancing_technique(\n",
        "            X_train_scaled, y_train, method=method, random_state=42\n",
        "        )\n",
        "\n",
        "        # Evaluate ensemble model with CV on the original training data (scaled)\n",
        "        # train_and_evaluate_with_cv handles the internal balancing for these models\n",
        "        cv_results, fold_details = train_and_evaluate_with_cv(\n",
        "            model_to_eval,\n",
        "            X_balanced_train, # Pass original scaled train data\n",
        "            y_balanced_train,\n",
        "            cv_folds=5,\n",
        "            random_state=42,\n",
        "            model_name=current_model_name,\n",
        "            verbose=True\n",
        "        )\n",
        "        all_cv_results[current_model_name] = cv_results\n",
        "\n",
        "    else:\n",
        "        # For SMOTE variants, resample the data first\n",
        "        X_balanced_train, y_balanced_train, balance_info = apply_balancing_technique(\n",
        "            X_train_scaled, y_train, method=method, random_state=42\n",
        "        )\n",
        "        print(f\"  Original counts: {balance_info['original_counts']}\")\n",
        "        print(f\"  Resampled counts: {balance_info['resampled_counts']}\")\n",
        "\n",
        "        # Evaluate the representative model (Logistic Regression) on the resampled data\n",
        "        current_model_name = f'{model_name_eval}_{method}'\n",
        "        cv_results, fold_details = train_and_evaluate_with_cv(\n",
        "            representative_model,\n",
        "            X_balanced_train, # Pass resampled data\n",
        "            y_balanced_train,\n",
        "            cv_folds=5,\n",
        "            random_state=42,\n",
        "            model_name=current_model_name,\n",
        "            verbose=True\n",
        "        )\n",
        "        all_cv_results[current_model_name] = cv_results\n",
        "\n",
        "print(\"\\nFinished evaluating models with advanced balancing techniques.\")\n",
        "\n",
        "# Compare the cross-validation results, including the previous 'downsample_upsample' LR result\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ADVANCED BALANCING TECHNIQUES COMPARISON (CV Results - Logistic Regression & Ensembles)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "balancing_comparison_data = []\n",
        "# Include the original LR result with downsample_upsample for comparison\n",
        "if 'Logistic Regression' in all_cv_results:\n",
        "    cv_res_lr_downup = all_cv_results['Logistic Regression']\n",
        "    balancing_comparison_data.append({\n",
        "        'Model + Balancing': f'Logistic Regression (downsample_upsample)',\n",
        "        'Macro-F1': f\"{cv_res_lr_downup['macro_f1_mean']:.4f} ± {cv_res_lr_downup['macro_f1_std']:.4f}\",\n",
        "        'Recall (Class 1)': f\"{cv_res_lr_downup['recall_class_1_mean']:.4f} ± {cv_res_lr_downup['recall_class_1_std']:.4f}\",\n",
        "        'ROC-AUC': f\"{cv_res_lr_downup['roc_auc_mean']:.4f} ± {cv_res_lr_downup['roc_auc_std']:.4f}\",\n",
        "        'Avg Time (s)': f\"{cv_res_lr_downup['time_mean']:.2f} ± {cv_res_lr_downup['time_std']:.2f}\"\n",
        "    })\n",
        "\n",
        "\n",
        "for model_name, cv_results in all_cv_results.items():\n",
        "    # Filter for the results of the advanced balancing techniques evaluated in this step\n",
        "    if model_name in ['Logistic Regression_SMOTE', 'Logistic Regression_ADASYN', 'Logistic Regression_BorderlineSMOTE',\n",
        "                      'BalancedBagging (LR)', 'BalancedRandomForest']:\n",
        "        balancing_comparison_data.append({\n",
        "            'Model + Balancing': model_name,\n",
        "            'Macro-F1': f\"{cv_results['macro_f1_mean']:.4f} ± {cv_results['macro_f1_std']:.4f}\",\n",
        "            'Recall (Class 1)': f\"{cv_results['recall_class_1_mean']:.4f} ± {cv_results['recall_class_1_std']:.4f}\",\n",
        "            'ROC-AUC': f\"{cv_results['roc_auc_mean']:.4f} ± {cv_results['roc_auc_std']:.4f}\",\n",
        "            'Avg Time (s)': f\"{cv_results['time_mean']:.2f} ± {cv_results['time_std']:.2f}\"\n",
        "        })\n",
        "\n",
        "balancing_comparison_df = pd.DataFrame(balancing_comparison_data)\n",
        "# Sort by Macro-F1 mean descending for easier comparison\n",
        "balancing_comparison_df = balancing_comparison_df.sort_values(by='Macro-F1', ascending=False)\n",
        "\n",
        "print(balancing_comparison_df.to_string(index=False))\n",
        "print(\"=\"*80 + \"\\n\")"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluating Logistic Regression with advanced balancing techniques...\n",
            "\n",
            "--- Evaluating with SMOTE ---\n",
            "  Original counts: {0: 8843, 1: 8843}\n",
            "  Resampled counts: {0: 8843, 1: 8843}\n",
            "\n",
            "--- Logistic Regression_SMOTE Cross-Validation ---\n",
            "  Fold 1 completed in 0.17 seconds.\n",
            "    Accuracy: 0.6328, Macro-F1: 0.6328, Recall (Class 1): 0.6456, ROC-AUC: 0.6866\n",
            "  Fold 2 completed in 0.19 seconds.\n",
            "    Accuracy: 0.6299, Macro-F1: 0.6295, Recall (Class 1): 0.6652, ROC-AUC: 0.6798\n",
            "  Fold 3 completed in 0.33 seconds.\n",
            "    Accuracy: 0.6378, Macro-F1: 0.6378, Recall (Class 1): 0.6448, ROC-AUC: 0.6988\n",
            "  Fold 4 completed in 0.16 seconds.\n",
            "    Accuracy: 0.6308, Macro-F1: 0.6307, Recall (Class 1): 0.6224, ROC-AUC: 0.6790\n",
            "  Fold 5 completed in 0.20 seconds.\n",
            "    Accuracy: 0.6373, Macro-F1: 0.6372, Recall (Class 1): 0.6529, ROC-AUC: 0.6940\n",
            "\n",
            "--- Average Results for Logistic Regression_SMOTE ---\n",
            "  Accuracy: 0.6337 ± 0.0033\n",
            "  Macro-F1: 0.6336 ± 0.0034\n",
            "  Recall (Class 1): 0.6462 ± 0.0140\n",
            "  ROC-AUC: 0.6876 ± 0.0078\n",
            "-----------------------------------------------------\n",
            "\n",
            "--- Evaluating with ADASYN ---\n",
            "  Original counts: {0: 8843, 1: 8843}\n",
            "  Resampled counts: {0: 8843, 1: 8843}\n",
            "\n",
            "--- Logistic Regression_ADASYN Cross-Validation ---\n",
            "  Fold 1 completed in 0.30 seconds.\n",
            "    Accuracy: 0.6328, Macro-F1: 0.6328, Recall (Class 1): 0.6456, ROC-AUC: 0.6866\n",
            "  Fold 2 completed in 0.24 seconds.\n",
            "    Accuracy: 0.6299, Macro-F1: 0.6295, Recall (Class 1): 0.6652, ROC-AUC: 0.6798\n",
            "  Fold 3 completed in 0.17 seconds.\n",
            "    Accuracy: 0.6378, Macro-F1: 0.6378, Recall (Class 1): 0.6448, ROC-AUC: 0.6988\n",
            "  Fold 4 completed in 0.15 seconds.\n",
            "    Accuracy: 0.6308, Macro-F1: 0.6307, Recall (Class 1): 0.6224, ROC-AUC: 0.6790\n",
            "  Fold 5 completed in 0.28 seconds.\n",
            "    Accuracy: 0.6373, Macro-F1: 0.6372, Recall (Class 1): 0.6529, ROC-AUC: 0.6940\n",
            "\n",
            "--- Average Results for Logistic Regression_ADASYN ---\n",
            "  Accuracy: 0.6337 ± 0.0033\n",
            "  Macro-F1: 0.6336 ± 0.0034\n",
            "  Recall (Class 1): 0.6462 ± 0.0140\n",
            "  ROC-AUC: 0.6876 ± 0.0078\n",
            "------------------------------------------------------\n",
            "\n",
            "--- Evaluating with BorderlineSMOTE ---\n",
            "  Original counts: {0: 8843, 1: 8843}\n",
            "  Resampled counts: {0: 8843, 1: 8843}\n",
            "\n",
            "--- Logistic Regression_BorderlineSMOTE Cross-Validation ---\n",
            "  Fold 1 completed in 0.18 seconds.\n",
            "    Accuracy: 0.6328, Macro-F1: 0.6328, Recall (Class 1): 0.6456, ROC-AUC: 0.6866\n",
            "  Fold 2 completed in 0.25 seconds.\n",
            "    Accuracy: 0.6299, Macro-F1: 0.6295, Recall (Class 1): 0.6652, ROC-AUC: 0.6798\n",
            "  Fold 3 completed in 0.19 seconds.\n",
            "    Accuracy: 0.6378, Macro-F1: 0.6378, Recall (Class 1): 0.6448, ROC-AUC: 0.6988\n",
            "  Fold 4 completed in 0.14 seconds.\n",
            "    Accuracy: 0.6308, Macro-F1: 0.6307, Recall (Class 1): 0.6224, ROC-AUC: 0.6790\n",
            "  Fold 5 completed in 0.12 seconds.\n",
            "    Accuracy: 0.6373, Macro-F1: 0.6372, Recall (Class 1): 0.6529, ROC-AUC: 0.6940\n",
            "\n",
            "--- Average Results for Logistic Regression_BorderlineSMOTE ---\n",
            "  Accuracy: 0.6337 ± 0.0033\n",
            "  Macro-F1: 0.6336 ± 0.0034\n",
            "  Recall (Class 1): 0.6462 ± 0.0140\n",
            "  ROC-AUC: 0.6876 ± 0.0078\n",
            "---------------------------------------------------------------\n",
            "\n",
            "--- Evaluating with BalancedBagging ---\n",
            "  BalancedBagging handles balancing internally. Returning original training data.\n",
            "\n",
            "--- BalancedBagging (LR) Cross-Validation ---\n",
            "  Fold 1 completed in 2.57 seconds.\n",
            "    Accuracy: 0.6331, Macro-F1: 0.6330, Recall (Class 1): 0.6501, ROC-AUC: 0.6870\n",
            "  Fold 2 completed in 0.46 seconds.\n",
            "    Accuracy: 0.6282, Macro-F1: 0.6275, Recall (Class 1): 0.6714, ROC-AUC: 0.6796\n",
            "  Fold 3 completed in 0.57 seconds.\n",
            "    Accuracy: 0.6401, Macro-F1: 0.6401, Recall (Class 1): 0.6493, ROC-AUC: 0.6997\n",
            "  Fold 4 completed in 0.47 seconds.\n",
            "    Accuracy: 0.6291, Macro-F1: 0.6290, Recall (Class 1): 0.6167, ROC-AUC: 0.6794\n",
            "  Fold 5 completed in 0.58 seconds.\n",
            "    Accuracy: 0.6364, Macro-F1: 0.6364, Recall (Class 1): 0.6422, ROC-AUC: 0.6942\n",
            "\n",
            "--- Average Results for BalancedBagging (LR) ---\n",
            "  Accuracy: 0.6334 ± 0.0045\n",
            "  Macro-F1: 0.6332 ± 0.0046\n",
            "  Recall (Class 1): 0.6459 ± 0.0176\n",
            "  ROC-AUC: 0.6880 ± 0.0080\n",
            "------------------------------------------------\n",
            "\n",
            "--- Evaluating with BalancedRandomForest ---\n",
            "  BalancedRandomForest handles balancing internally. Returning original training data.\n",
            "\n",
            "--- BalancedRandomForest Cross-Validation ---\n",
            "  Fold 1 completed in 2.84 seconds.\n",
            "    Accuracy: 0.6023, Macro-F1: 0.6023, Recall (Class 1): 0.6105, ROC-AUC: 0.6506\n",
            "  Fold 2 completed in 2.51 seconds.\n",
            "    Accuracy: 0.6022, Macro-F1: 0.6022, Recall (Class 1): 0.6103, ROC-AUC: 0.6446\n",
            "  Fold 3 completed in 2.16 seconds.\n",
            "    Accuracy: 0.6064, Macro-F1: 0.6064, Recall (Class 1): 0.6109, ROC-AUC: 0.6501\n",
            "  Fold 4 completed in 2.17 seconds.\n",
            "    Accuracy: 0.6135, Macro-F1: 0.6135, Recall (Class 1): 0.6133, ROC-AUC: 0.6543\n",
            "  Fold 5 completed in 2.15 seconds.\n",
            "    Accuracy: 0.6172, Macro-F1: 0.6172, Recall (Class 1): 0.6235, ROC-AUC: 0.6654\n",
            "\n",
            "--- Average Results for BalancedRandomForest ---\n",
            "  Accuracy: 0.6083 ± 0.0060\n",
            "  Macro-F1: 0.6083 ± 0.0060\n",
            "  Recall (Class 1): 0.6137 ± 0.0050\n",
            "  ROC-AUC: 0.6530 ± 0.0069\n",
            "------------------------------------------------\n",
            "\n",
            "Finished evaluating models with advanced balancing techniques.\n",
            "\n",
            "================================================================================\n",
            "ADVANCED BALANCING TECHNIQUES COMPARISON (CV Results - Logistic Regression & Ensembles)\n",
            "================================================================================\n",
            "                        Model + Balancing        Macro-F1 Recall (Class 1)         ROC-AUC Avg Time (s)\n",
            "Logistic Regression (downsample_upsample) 0.6336 ± 0.0034  0.6462 ± 0.0140 0.6876 ± 0.0078  0.09 ± 0.01\n",
            "                Logistic Regression_SMOTE 0.6336 ± 0.0034  0.6462 ± 0.0140 0.6876 ± 0.0078  0.21 ± 0.06\n",
            "               Logistic Regression_ADASYN 0.6336 ± 0.0034  0.6462 ± 0.0140 0.6876 ± 0.0078  0.23 ± 0.06\n",
            "      Logistic Regression_BorderlineSMOTE 0.6336 ± 0.0034  0.6462 ± 0.0140 0.6876 ± 0.0078  0.17 ± 0.05\n",
            "                     BalancedBagging (LR) 0.6332 ± 0.0046  0.6459 ± 0.0176 0.6880 ± 0.0080  0.93 ± 0.82\n",
            "                     BalancedRandomForest 0.6083 ± 0.0060  0.6137 ± 0.0050 0.6530 ± 0.0069  2.37 ± 0.27\n",
            "================================================================================\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3adba359"
      },
      "source": [
        "## Feature engineering & selection\n",
        "\n",
        "### Subtask:\n",
        "Explore creating new features from existing ones or select a subset of the most important features to potentially improve model performance and reduce training time.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47eeac74"
      },
      "source": [
        "**Reasoning**:\n",
        "Calculate and visualize the correlation matrix for numerical features to identify potential collinearity and inform feature selection.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 585
        },
        "id": "ddefd048",
        "outputId": "79a3fe2a-285c-475a-af5b-cc41f0dd5df7"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Select only numerical columns from X\n",
        "X_numerical = X.select_dtypes(include=np.number)\n",
        "\n",
        "# Calculate the correlation matrix\n",
        "correlation_matrix = X_numerical.corr()\n",
        "\n",
        "# Visualize the correlation matrix\n",
        "plt.figure(figsize=(12, 10))\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=.5)\n",
        "plt.title('Correlation Matrix of Numerical Features')\n",
        "plt.show()\n",
        "\n",
        "print(\"✓ Correlation matrix calculated and visualized.\")"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x1000 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Correlation matrix calculated and visualized.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3523ea1b"
      },
      "source": [
        "**Reasoning**:\n",
        "Create a new feature (annual_inc_to_loan_amnt_ratio), select a subset of features based on the correlation analysis and domain knowledge, and then train a representative model (ANN) on the new feature set using cross-validation to evaluate its impact on performance.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "da750b1b",
        "outputId": "f5819150-62e8-41b8-9fea-14758fde372e"
      },
      "source": [
        "# Create a new feature: ratio of annual income to loan amount\n",
        "X_engineered = X.copy()\n",
        "# Add a small constant to loan_amnt to avoid division by zero, although loan_amnt is unlikely to be 0\n",
        "X_engineered['annual_inc_to_loan_amnt_ratio'] = X_engineered['annual_inc'] / (X_engineered['loan_amnt'] + 1e-6)\n",
        "\n",
        "# Select a subset of features\n",
        "# Based on correlation matrix: 'loan_amnt' and 'installment' are highly correlated.\n",
        "# Keep 'loan_amnt' and drop 'installment'.\n",
        "# Keep 'term', 'int_rate', 'emp_length', 'annual_inc'.\n",
        "# Keep the one-hot encoded 'grade' and 'application_type' features.\n",
        "# Include the new engineered feature.\n",
        "\n",
        "selected_features = [\n",
        "    'loan_amnt',\n",
        "    'term',\n",
        "    'int_rate',\n",
        "    'emp_length',\n",
        "    'annual_inc',\n",
        "    'annual_inc_to_loan_amnt_ratio'\n",
        "]\n",
        "\n",
        "# Add all one-hot encoded categorical features\n",
        "# Identify columns that are not in the initial selected_features and are from one-hot encoding\n",
        "ohe_cols = [col for col in X_engineered.columns if col.startswith('grade_') or col.startswith('application_type_')]\n",
        "selected_features.extend(ohe_cols)\n",
        "\n",
        "# Ensure all selected features exist in the DataFrame\n",
        "selected_features = [f for f in selected_features if f in X_engineered.columns]\n",
        "\n",
        "X_selected = X_engineered[selected_features].copy()\n",
        "\n",
        "print(f\"Original feature shape: {X.shape}\")\n",
        "print(f\"Selected feature shape: {X_selected.shape}\")\n",
        "print(\"Selected features:\", selected_features)\n",
        "\n",
        "# Re-run balancing and train the best performing ANN model on the selected features\n",
        "print(\"\\nEvaluating the best ANN model with selected features using 5-fold CV...\")\n",
        "\n",
        "# We need to re-split the data using the selected features before balancing\n",
        "X_train_selected_full, X_test_selected_full, y_train_full, y_test_full = train_test_split(\n",
        "    X_selected, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Apply the best balancing strategy ('downsample_upsample' as determined previously)\n",
        "X_train_selected_balanced, y_train_selected_balanced, balance_info_selected = prepare_data_with_balancing(\n",
        "    X_train_selected_full, y_train_full,\n",
        "    strategy=BEST_STRATEGY, # Use the best strategy found in previous steps\n",
        "    ratio=BEST_RATIO,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Scale the selected and balanced training data\n",
        "scaler_selected = MinMaxScaler()\n",
        "X_train_selected_scaled = scaler_selected.fit_transform(X_train_selected_balanced)\n",
        "# Scale the corresponding test data for potential final evaluation later\n",
        "X_test_selected_scaled = scaler_selected.transform(X_test_selected_full)\n",
        "\n",
        "\n",
        "# Train the best ANN architecture (ANN_Arch3_2HL_Tanh_Adam based on previous step) on the new data\n",
        "# Need to define the build function for the best ANN architecture again or pass the actual function\n",
        "def build_best_ann(input_shape):\n",
        "    \"\"\"Best performing ANN architecture from previous exploration.\"\"\"\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Dense(14, activation='tanh', input_shape=(input_shape,)),\n",
        "        tf.keras.layers.Dense(7, activation='tanh'),\n",
        "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "\n",
        "best_ann_selected_cv_results, best_ann_selected_fold_details = train_and_evaluate_keras_with_cv(\n",
        "    lambda: build_best_ann(X_train_selected_scaled.shape[1]), # Use lambda to pass input_shape\n",
        "    X_train_selected_scaled,\n",
        "    y_train_selected_balanced,\n",
        "    cv_folds=5,\n",
        "    epochs=50,\n",
        "    batch_size=32,\n",
        "    random_state=42,\n",
        "    model_name=f'ANN ({BEST_STRATEGY} + Selected Features)',\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "# Store the CV results\n",
        "all_cv_results[f'ANN ({BEST_STRATEGY} + Selected Features)'] = best_ann_selected_cv_results\n",
        "\n",
        "# Compare with previous ANN results\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ANN PERFORMANCE COMPARISON (Original vs. Selected Features)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "ann_comparison_data = []\n",
        "# Add the original ANN result (from the 'ANN' key which used the base ANN architecture)\n",
        "if 'ANN' in all_cv_results:\n",
        "     cv_res_original_ann = all_cv_results['ANN']\n",
        "     ann_comparison_data.append({\n",
        "        'Model + Features': f'ANN ({BEST_STRATEGY} + Original Features)',\n",
        "        'Macro-F1': f\"{cv_res_original_ann['macro_f1_mean']:.4f} ± {cv_res_original_ann['macro_f1_std']:.4f}\",\n",
        "        'Recall (Class 1)': f\"{cv_res_original_ann['recall_class_1_mean']:.4f} ± {cv_res_original_ann['recall_class_1_std']:.4f}\",\n",
        "        'ROC-AUC': f\"{cv_res_original_ann['roc_auc_mean']:.4f} ± {cv_res_original_ann['roc_auc_std']:.4f}\",\n",
        "        'Avg Time (s)': f\"{cv_res_original_ann['time_mean']:.2f} ± {cv_res_original_ann['time_std']:.2f}\"\n",
        "     })\n",
        "\n",
        "# Add the result with selected features\n",
        "ann_comparison_data.append({\n",
        "    'Model + Features': f'ANN ({BEST_STRATEGY} + Selected Features)',\n",
        "    'Macro-F1': f\"{best_ann_selected_cv_results['macro_f1_mean']:.4f} ± {best_ann_selected_cv_results['macro_f1_std']:.4f}\",\n",
        "    'Recall (Class 1)': f\"{best_ann_selected_cv_results['recall_class_1_mean']:.4f} ± {best_ann_selected_cv_results['recall_class_1_std']:.4f}\",\n",
        "    'ROC-AUC': f\"{best_ann_selected_cv_results['roc_auc_mean']:.4f} ± {best_ann_selected_cv_results['roc_auc_std']:.4f}\",\n",
        "    'Avg Time (s)': f\"{best_ann_selected_cv_results['time_mean']:.2f} ± {best_ann_selected_cv_results['time_std']:.2f}\"\n",
        "})\n",
        "\n",
        "ann_features_comparison_df = pd.DataFrame(ann_comparison_data)\n",
        "print(ann_features_comparison_df.to_string(index=False))\n",
        "print(\"=\"*80 + \"\\n\")\n",
        "\n",
        "print(\"✓ Feature engineering and selection evaluation complete.\")"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original feature shape: (57265, 14)\n",
            "Selected feature shape: (57265, 14)\n",
            "Selected features: ['loan_amnt', 'term', 'int_rate', 'emp_length', 'annual_inc', 'annual_inc_to_loan_amnt_ratio', 'grade_B', 'grade_C', 'grade_D', 'grade_E', 'grade_F', 'grade_G', 'application_type_INDIVIDUAL', 'application_type_JOINT']\n",
            "\n",
            "Evaluating the best ANN model with selected features using 5-fold CV...\n",
            "\n",
            "--- ANN (downsample_upsample + Selected Features) Cross-Validation ---\n",
            "  Fold 1 completed in 53.98 seconds.\n",
            "    Accuracy: 0.6461, Macro-F1: 0.6441, Recall (Class 1): 0.7224, ROC-AUC: 0.6957\n",
            "  Fold 2 completed in 56.65 seconds.\n",
            "    Accuracy: 0.6367, Macro-F1: 0.6339, Recall (Class 1): 0.7241, ROC-AUC: 0.6894\n",
            "  Fold 3 completed in 54.79 seconds.\n",
            "    Accuracy: 0.6358, Macro-F1: 0.6333, Recall (Class 1): 0.7191, ROC-AUC: 0.6948\n",
            "  Fold 4 completed in 53.36 seconds.\n",
            "    Accuracy: 0.6443, Macro-F1: 0.6439, Recall (Class 1): 0.6804, ROC-AUC: 0.6971\n",
            "  Fold 5 completed in 52.76 seconds.\n",
            "    Accuracy: 0.6429, Macro-F1: 0.6421, Recall (Class 1): 0.6900, ROC-AUC: 0.6917\n",
            "\n",
            "--- Average Results for ANN (downsample_upsample + Selected Features) ---\n",
            "  Accuracy: 0.6412 ± 0.0041\n",
            "  Macro-F1: 0.6394 ± 0.0048\n",
            "  Recall (Class 1): 0.7072 ± 0.0183\n",
            "  ROC-AUC: 0.6937 ± 0.0028\n",
            "-------------------------------------------------------------------------\n",
            "\n",
            "================================================================================\n",
            "ANN PERFORMANCE COMPARISON (Original vs. Selected Features)\n",
            "================================================================================\n",
            "                             Model + Features        Macro-F1 Recall (Class 1)         ROC-AUC Avg Time (s)\n",
            "ANN (downsample_upsample + Original Features) 0.6360 ± 0.0045  0.6786 ± 0.0186 0.6910 ± 0.0084 50.75 ± 1.52\n",
            "ANN (downsample_upsample + Selected Features) 0.6394 ± 0.0048  0.7072 ± 0.0183 0.6937 ± 0.0028 54.31 ± 1.35\n",
            "================================================================================\n",
            "\n",
            "✓ Feature engineering and selection evaluation complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1a3c12f1"
      },
      "source": [
        "## Ensemble methods\n",
        "\n",
        "### Subtask:\n",
        "Combine predictions from multiple models (e.g., Logistic Regression, XGBoost, ANN) using techniques like bagging, boosting, or stacking to potentially achieve better overall performance than any single model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "45a29d41"
      },
      "source": [
        "**Reasoning**:\n",
        "Identify the best individual models and train them on the full training data. Then, implement and evaluate a stacking ensemble model using these trained models as base estimators and a Logistic Regression as the final estimator. Store and compare the results.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 460
        },
        "id": "122c0379",
        "outputId": "80e2b39f-0d2e-40b2-8891-4bb1f3c49c35"
      },
      "source": [
        "from sklearn.ensemble import StackingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# 1. Identify the best performing individual models from the previous steps\n",
        "# Based on the previous steps, the best models for each type were:\n",
        "# Logistic Regression: The tuned version (if successful) or the base one.\n",
        "# XGBoost: The tuned version (if successful) or the base one.\n",
        "# ANN: The best architecture found (ANN_Arch3_2HL_Tanh_Adam) trained on selected features.\n",
        "\n",
        "# Check if tuned models exist, otherwise use base models\n",
        "lr_best = all_models['Logistic Regression'].get('tuned_model', all_models['Logistic Regression']['model'])\n",
        "xgb_best = all_models['XGBoost'].get('tuned_model', all_models['XGBoost']['model'])\n",
        "\n",
        "# The best ANN was evaluated with selected features, let's use that setup\n",
        "# We need to retrain the best ANN architecture on the full selected and balanced training data\n",
        "# Define the build function for the best ANN architecture again\n",
        "def build_best_ann_final(input_shape):\n",
        "    \"\"\"Best performing ANN architecture from previous exploration.\"\"\"\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Dense(14, activation='tanh', input_shape=(input_shape,)),\n",
        "        tf.keras.layers.Dense(7, activation='tanh'),\n",
        "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Retrain the best ANN on the full X_train_selected_scaled and y_train_selected_balanced\n",
        "best_ann_final = build_best_ann_final(X_train_selected_scaled.shape[1])\n",
        "best_ann_final.fit(X_train_selected_scaled, y_train_selected_balanced, epochs=50, batch_size=32, verbose=0)\n",
        "\n",
        "\n",
        "# 2. Train these best individual models on the full training dataset\n",
        "# LR and XGBoost were already trained on X_train_scaled, y_train in previous steps\n",
        "# The best ANN was trained on X_train_selected_scaled, y_train_selected_balanced\n",
        "\n",
        "# The base estimators for stacking should be trained on the same data\n",
        "# Let's use X_train_scaled and y_train for the base estimators for simplicity in stacking\n",
        "# Retrain LR and XGBoost on X_train_scaled if tuned models were not trained on it\n",
        "lr_best.fit(X_train_scaled, y_train)\n",
        "xgb_best.fit(X_train_scaled, y_train)\n",
        "\n",
        "# For ANN in stacking, we need a scikit-learn compatible wrapper.\n",
        "# We will use the KerasClassifier wrapper and train it on X_train_scaled, y_train\n",
        "# This might perform slightly differently than the ANN on selected features,\n",
        "# but it allows combining with sklearn models easily.\n",
        "# If we wanted to use the ANN on selected features, we would need to scale X_test_scaled\n",
        "# using the scaler from selected features and pass it to the stacking predict method.\n",
        "# For this step, let's stick to using X_train_scaled for all base estimators for simplicity.\n",
        "\n",
        "# Build the ANN model using the best architecture for the KerasClassifier wrapper\n",
        "def build_best_ann_wrapper(meta):\n",
        "     model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Dense(14, activation='tanh', input_shape=(meta[\"n_features_in_\"],)),\n",
        "        tf.keras.layers.Dense(7, activation='tanh'),\n",
        "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "     model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "     return model\n",
        "\n",
        "# Wrap the ANN model\n",
        "ann_wrapper = KerasClassifier(model=build_best_ann_wrapper, epochs=50, batch_size=32, verbose=0)\n",
        "# Fit the wrapper model (necessary before using in stacking, although stacking refits)\n",
        "ann_wrapper.fit(X_train_scaled, y_train)\n",
        "\n",
        "\n",
        "# 3. Implement an ensemble method (StackingClassifier)\n",
        "# Define base estimators - using the trained models\n",
        "estimators = [\n",
        "    ('lr', lr_best),\n",
        "    ('xgb', xgb_best),\n",
        "    ('ann', ann_wrapper) # Using the wrapper here\n",
        "]\n",
        "\n",
        "# Define the final estimator\n",
        "final_estimator = LogisticRegression(random_state=42, max_iter=1000)\n",
        "\n",
        "# Create the Stacking Classifier\n",
        "stacking_model = StackingClassifier(\n",
        "    estimators=estimators,\n",
        "    final_estimator=final_estimator,\n",
        "    cv='auto', # Use cross-validation internally for training the final estimator\n",
        "    stack_method='predict_proba', # Use probabilities for stacking\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# 4. Train the ensemble model on the full training dataset\n",
        "print(\"\\nTraining Stacking Ensemble Model...\")\n",
        "stacking_model.fit(X_train_scaled, y_train)\n",
        "print(\"✓ Stacking Ensemble Model trained.\")\n",
        "\n",
        "# 5. Evaluate the ensemble model's performance using CV\n",
        "print(\"\\nEvaluating Stacking Ensemble Model with 5-fold CV...\")\n",
        "stacking_cv_results, stacking_fold_details = train_and_evaluate_with_cv(\n",
        "    stacking_model,\n",
        "    X_train_scaled,\n",
        "    y_train,\n",
        "    cv_folds=5,\n",
        "    random_state=42,\n",
        "    model_name='Stacking Ensemble',\n",
        "    verbose=True # Set to True to see fold results\n",
        ")\n",
        "\n",
        "# 6. Store the ensemble model and its CV results\n",
        "all_models['Stacking Ensemble'] = {\n",
        "    'model': stacking_model,\n",
        "    # Note: StackingClassifier doesn't have a single predict_proba for the whole test set like individual models\n",
        "    # We would typically evaluate the stacking model on the held-out test set later.\n",
        "    # For now, store the CV results and the trained model.\n",
        "    'y_true': y_test, # Store the test set true labels for later evaluation\n",
        "    'y_pred_proba': stacking_model.predict_proba(X_test_scaled)[:, 1], # Predict on test set for later\n",
        "    'cv_results': stacking_cv_results\n",
        "}\n",
        "all_cv_results['Stacking Ensemble'] = stacking_cv_results\n",
        "\n",
        "# 7. Print and compare results\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"STACKING ENSEMBLE VS BEST INDIVIDUAL MODELS (CV Results)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "comparison_data = []\n",
        "# Add best individual models (based on Macro-F1 from previous steps)\n",
        "# Find the best non-ensemble, non-tuned model based on Macro-F1\n",
        "best_individual_macro_f1 = -1\n",
        "best_individual_model_name = None\n",
        "for model_name, cv_results in all_cv_results.items():\n",
        "    # Exclude ensemble models and tuned models from 'best individual' comparison in this context\n",
        "    if model_name not in ['Stacking Ensemble', 'BalancedBagging (LR)', 'BalancedRandomForest'] and \\\n",
        "       not model_name.startswith('ANN_Arch') and \\\n",
        "       'tuned' not in model_name.lower():\n",
        "         if cv_results['macro_f1_mean'] > best_individual_macro_f1:\n",
        "             best_individual_macro_f1 = cv_results['macro_f1_mean']\n",
        "             best_individual_model_name = model_name\n",
        "\n",
        "# Add the best individual models evaluated previously\n",
        "if best_individual_model_name:\n",
        "     cv_results_best_individual = all_cv_results[best_individual_model_name]\n",
        "     comparison_data.append({\n",
        "        'Model': best_individual_model_name,\n",
        "        'Macro-F1': f\"{cv_results_best_individual['macro_f1_mean']:.4f} ± {cv_results_best_individual['macro_f1_std']:.4f}\",\n",
        "        'Recall (Class 1)': f\"{cv_results_best_individual['recall_class_1_mean']:.4f} ± {cv_results_best_individual['recall_class_1_std']:.4f}\",\n",
        "        'ROC-AUC': f\"{cv_results_best_individual['roc_auc_mean']:.4f} ± {cv_results_best_individual['roc_auc_std']:.4f}\",\n",
        "        'Avg Time (s)': f\"{cv_results_best_individual['time_mean']:.2f} ± {cv_results_best_individual['time_std']:.2f}\"\n",
        "     })\n",
        "\n",
        "# Add the best ANN architecture on selected features\n",
        "if f'ANN ({BEST_STRATEGY} + Selected Features)' in all_cv_results:\n",
        "     cv_results_best_ann_selected = all_cv_results[f'ANN ({BEST_STRATEGY} + Selected Features)']\n",
        "     comparison_data.append({\n",
        "        'Model': f'ANN ({BEST_STRATEGY} + Selected Features)',\n",
        "        'Macro-F1': f\"{cv_results_best_ann_selected['macro_f1_mean']:.4f} ± {cv_results_best_ann_selected['macro_f1_std']:.4f}\",\n",
        "        'Recall (Class 1)': f\"{cv_results_best_ann_selected['recall_class_1_mean']:.4f} ± {cv_results_best_ann_selected['recall_class_1_std']:.4f}\",\n",
        "        'ROC-AUC': f\"{cv_results_best_ann_selected['roc_auc_mean']:.4f} ± {cv_results_best_ann_selected['roc_auc_std']:.4f}\",\n",
        "        'Avg Time (s)': f\"{cv_results_best_ann_selected['time_mean']:.2f} ± {cv_results_best_ann_selected['time_std']:.2f}\"\n",
        "     })\n",
        "\n",
        "\n",
        "# Add the Stacking Ensemble results\n",
        "comparison_data.append({\n",
        "    'Model': 'Stacking Ensemble',\n",
        "    'Macro-F1': f\"{stacking_cv_results['macro_f1_mean']:.4f} ± {stacking_cv_results['macro_f1_std']:.4f}\",\n",
        "    'Recall (Class 1)': f\"{stacking_cv_results['recall_class_1_mean']:.4f} ± {stacking_cv_results['recall_class_1_std']:.4f}\",\n",
        "    'ROC-AUC': f\"{stacking_cv_results['roc_auc_mean']:.4f} ± {stacking_cv_results['roc_auc_std']:.4f}\",\n",
        "    'Avg Time (s)': f\"{stacking_cv_results['time_mean']:.2f} ± {stacking_cv_results['time_std']:.2f}\"\n",
        "})\n",
        "\n",
        "comparison_df_ensemble = pd.DataFrame(comparison_data)\n",
        "# Sort by Macro-F1 mean descending\n",
        "comparison_df_ensemble = comparison_df_ensemble.sort_values(by='Macro-F1', ascending=False)\n",
        "\n",
        "print(comparison_df_ensemble.to_string(index=False))\n",
        "print(\"=\"*80 + \"\\n\")\n",
        "\n",
        "print(\"✓ Ensemble modeling and evaluation complete.\")"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training Stacking Ensemble Model...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "InvalidParameterError",
          "evalue": "The 'cv' parameter of StackingClassifier must be an int in the range [2, inf), an object implementing 'split' and 'get_n_splits', an iterable or None or a str among {'prefit'}. Got 'auto' instead.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidParameterError\u001b[0m                     Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4242278226.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;31m# 4. Train the ensemble model on the full training dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nTraining Stacking Ensemble Model...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m \u001b[0mstacking_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_scaled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"✓ Stacking Ensemble Model trained.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/ensemble/_stacking.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, **fit_params)\u001b[0m\n\u001b[1;32m    715\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    716\u001b[0m             \u001b[0mfit_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"sample_weight\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 717\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_encoded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    718\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    719\u001b[0m     @available_if(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1380\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1381\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mglobal_skip_validation\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mpartial_fit_and_fitted\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1382\u001b[0;31m                 \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1384\u001b[0m             with config_context(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_params\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    434\u001b[0m         \u001b[0maccepted\u001b[0m \u001b[0mconstraints\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m         \"\"\"\n\u001b[0;32m--> 436\u001b[0;31m         validate_parameter_constraints(\n\u001b[0m\u001b[1;32m    437\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parameter_constraints\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36mvalidate_parameter_constraints\u001b[0;34m(parameter_constraints, params, caller_name)\u001b[0m\n\u001b[1;32m     96\u001b[0m                 )\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m             raise InvalidParameterError(\n\u001b[0m\u001b[1;32m     99\u001b[0m                 \u001b[0;34mf\"The {param_name!r} parameter of {caller_name} must be\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0;34mf\" {constraints_str}. Got {param_val!r} instead.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidParameterError\u001b[0m: The 'cv' parameter of StackingClassifier must be an int in the range [2, inf), an object implementing 'split' and 'get_n_splits', an iterable or None or a str among {'prefit'}. Got 'auto' instead."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2aaacd55"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous command failed because 'auto' is not a valid value for the `cv` parameter in StackingClassifier. I need to provide a valid cross-validation strategy, such as an integer number of folds. I will fix the `cv` parameter in the StackingClassifier constructor and rerun the training and evaluation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "OJjMVgb6A0lH",
        "outputId": "1abc4897-17fa-459b-980e-bb3480e87066"
      },
      "source": [
        "from sklearn.ensemble import StackingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scikeras.wrappers import KerasClassifier # Import KerasClassifier\n",
        "\n",
        "\n",
        "# 1. Identify the best performing individual models from the previous steps\n",
        "# Based on the previous steps, the best models for each type were:\n",
        "# Logistic Regression: The tuned version (if successful) or the base one.\n",
        "# XGBoost: The tuned version (if successful) or the base one.\n",
        "# ANN: The best architecture found (ANN_Arch3_2HL_Tanh_Adam) trained on selected features.\n",
        "\n",
        "# Check if tuned models exist, otherwise use base models\n",
        "lr_best = all_models['Logistic Regression'].get('tuned_model', all_models['Logistic Regression']['model'])\n",
        "xgb_best = all_models['XGBoost'].get('tuned_model', all_models['XGBoost']['model'])\n",
        "\n",
        "\n",
        "# Retrain LR and XGBoost on X_train_scaled if tuned models were not trained on it\n",
        "lr_best.fit(X_train_scaled, y_train)\n",
        "xgb_best.fit(X_train_scaled, y_train)\n",
        "\n",
        "# For ANN in stacking, we need a scikit-learn compatible wrapper.\n",
        "# We will use the KerasClassifier wrapper and train it on X_train_scaled, y_train\n",
        "# This might perform slightly differently than the ANN on selected features,\n",
        "# but it allows combining with sklearn models easily.\n",
        "# If we wanted to use the ANN on selected features, we would need to scale X_test_scaled\n",
        "# using the scaler from selected features and pass it to the stacking predict method.\n",
        "# For this step, let's stick to using X_train_scaled for all base estimators for simplicity.\n",
        "\n",
        "# Build the ANN model using the best architecture for the KerasClassifier wrapper\n",
        "def build_best_ann_wrapper(meta):\n",
        "     model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Dense(14, activation='tanh', input_shape=(meta[\"n_features_in_\"],)),\n",
        "        tf.keras.layers.Dense(7, activation='tanh'),\n",
        "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "     model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "     return model\n",
        "\n",
        "# Wrap the ANN model\n",
        "ann_wrapper = KerasClassifier(model=build_best_ann_wrapper, epochs=50, batch_size=32, verbose=0)\n",
        "# Fit the wrapper model (necessary before using in stacking, although stacking refits)\n",
        "# Ensure X_train_scaled is a numpy array or pandas DataFrame\n",
        "if isinstance(X_train_scaled, np.ndarray):\n",
        "    ann_wrapper.fit(X_train_scaled, y_train.values)\n",
        "else:\n",
        "    ann_wrapper.fit(X_train_scaled, y_train)\n",
        "\n",
        "\n",
        "# 3. Implement an ensemble method (StackingClassifier)\n",
        "# Define base estimators - using the trained models\n",
        "estimators = [\n",
        "    ('lr', lr_best),\n",
        "    ('xgb', xgb_best),\n",
        "    ('ann', ann_wrapper) # Using the wrapper here\n",
        "]\n",
        "\n",
        "# Define the final estimator\n",
        "final_estimator = LogisticRegression(random_state=42, max_iter=1000)\n",
        "\n",
        "# Create the Stacking Classifier - FIX: Set a valid integer for cv\n",
        "stacking_model = StackingClassifier(\n",
        "    estimators=estimators,\n",
        "    final_estimator=final_estimator,\n",
        "    cv=5, # Use 5-fold cross-validation internally for training the final estimator\n",
        "    stack_method='predict_proba', # Use probabilities for stacking\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# 4. Train the ensemble model on the full training dataset\n",
        "print(\"\\nTraining Stacking Ensemble Model...\")\n",
        "stacking_model.fit(X_train_scaled, y_train)\n",
        "print(\"✓ Stacking Ensemble Model trained.\")\n",
        "\n",
        "# 5. Evaluate the ensemble model's performance using CV\n",
        "print(\"\\nEvaluating Stacking Ensemble Model with 5-fold CV...\")\n",
        "stacking_cv_results, stacking_fold_details = train_and_evaluate_with_cv(\n",
        "    stacking_model,\n",
        "    X_train_scaled,\n",
        "    y_train,\n",
        "    cv_folds=5,\n",
        "    random_state=42,\n",
        "    model_name='Stacking Ensemble',\n",
        "    verbose=True # Set to True to see fold results\n",
        ")\n",
        "\n",
        "# 6. Store the ensemble model and its CV results\n",
        "all_models['Stacking Ensemble'] = {\n",
        "    'model': stacking_model,\n",
        "    # Note: StackingClassifier doesn't have a single predict_proba for the whole test set like individual models\n",
        "    # We would typically evaluate the stacking model on the held-out test set later.\n",
        "    # For now, store the CV results and the trained model.\n",
        "    'y_true': y_test, # Store the test set true labels for later evaluation\n",
        "    'y_pred_proba': stacking_model.predict_proba(X_test_scaled)[:, 1], # Predict on test set for later\n",
        "    'cv_results': stacking_cv_results\n",
        "}\n",
        "all_cv_results['Stacking Ensemble'] = stacking_cv_results\n",
        "\n",
        "# 7. Print and compare results\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"STACKING ENSEMBLE VS BEST INDIVIDUAL MODELS (CV Results)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "comparison_data = []\n",
        "# Add best individual models (based on Macro-F1 from previous steps)\n",
        "# Find the best non-ensemble, non-tuned model based on Macro-F1\n",
        "best_individual_macro_f1 = -1\n",
        "best_individual_model_name = None\n",
        "for model_name, cv_results in all_cv_results.items():\n",
        "    # Exclude ensemble models and tuned models from 'best individual' comparison in this context\n",
        "    if model_name not in ['Stacking Ensemble', 'BalancedBagging (LR)', 'BalancedRandomForest'] and \\\n",
        "       not model_name.startswith('ANN_Arch') and \\\n",
        "       'tuned' not in model_name.lower():\n",
        "         if cv_results['macro_f1_mean'] > best_individual_macro_f1:\n",
        "             best_individual_macro_f1 = cv_results['macro_f1_mean']\n",
        "             best_individual_model_name = model_name\n",
        "\n",
        "# Add the best individual models evaluated previously\n",
        "if best_individual_model_name:\n",
        "     cv_results_best_individual = all_cv_results[best_individual_model_name]\n",
        "     comparison_data.append({\n",
        "        'Model': best_individual_model_name,\n",
        "        'Macro-F1': f\"{cv_results_best_individual['macro_f1_mean']:.4f} ± {cv_results_best_individual['macro_f1_std']:.4f}\",\n",
        "        'Recall (Class 1)': f\"{cv_results_best_individual['recall_class_1_mean']:.4f} ± {cv_results_best_individual['recall_class_1_std']:.4f}\",\n",
        "        'ROC-AUC': f\"{cv_results_best_individual['roc_auc_mean']:.4f} ± {cv_results_best_individual['roc_auc_std']:.4f}\",\n",
        "        'Avg Time (s)': f\"{cv_results_best_individual['time_mean']:.2f} ± {cv_results_best_individual['time_std']:.2f}\"\n",
        "     })\n",
        "\n",
        "# Add the best ANN architecture on selected features (if it exists in results)\n",
        "# The key might vary based on previous runs, check for its existence\n",
        "ann_selected_key = f'ANN ({BEST_STRATEGY} + Selected Features)'\n",
        "if ann_selected_key in all_cv_results:\n",
        "     cv_results_best_ann_selected = all_cv_results[ann_selected_key]\n",
        "     comparison_data.append({\n",
        "        'Model': ann_selected_key,\n",
        "        'Macro-F1': f\"{cv_results_best_ann_selected['macro_f1_mean']:.4f} ± {cv_results_best_ann_selected['macro_f1_std']:.4f}\",\n",
        "        'Recall (Class 1)': f\"{cv_results_best_ann_selected['recall_class_1_mean']:.4f} ± {cv_results_best_ann_selected['recall_class_1_std']:.4f}\",\n",
        "        'ROC-AUC': f\"{cv_results_best_ann_selected['roc_auc_mean']:.4f} ± {cv_results_best_ann_selected['roc_auc_std']:.4f}\",\n",
        "        'Avg Time (s)': f\"{cv_results_best_ann_selected['time_mean']:.2f} ± {cv_results_best_ann_selected['time_std']:.2f}\"\n",
        "     })\n",
        "\n",
        "\n",
        "# Add the Stacking Ensemble results\n",
        "comparison_data.append({\n",
        "    'Model': 'Stacking Ensemble',\n",
        "    'Macro-F1': f\"{stacking_cv_results['macro_f1_mean']:.4f} ± {stacking_cv_results['macro_f1_std']:.4f}\",\n",
        "    'Recall (Class 1)': f\"{stacking_cv_results['recall_class_1_mean']:.4f} ± {stacking_cv_results['recall_class_1_std']:.4f}\",\n",
        "    'ROC-AUC': f\"{stacking_cv_results['roc_auc_mean']:.4f} ± {stacking_cv_results['roc_auc_std']:.4f}\",\n",
        "    'Avg Time (s)': f\"{stacking_cv_results['time_mean']:.2f} ± {stacking_cv_results['time_std']:.2f}\"\n",
        "})\n",
        "\n",
        "comparison_df_ensemble = pd.DataFrame(comparison_data)\n",
        "# Sort by Macro-F1 mean descending\n",
        "comparison_df_ensemble = comparison_df_ensemble.sort_values(by='Macro-F1', ascending=False)\n",
        "\n",
        "print(comparison_df_ensemble.to_string(index=False))\n",
        "print(\"=\"*80 + \"\\n\")\n",
        "\n",
        "print(\"✓ Ensemble modeling and evaluation complete.\")"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training Stacking Ensemble Model...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'super' object has no attribute '__sklearn_tags__'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.12/dist-packages/joblib/externals/loky/process_executor.py\", line 490, in _process_worker\n    r = call_item()\n        ^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/joblib/externals/loky/process_executor.py\", line 291, in __call__\n    return self.fn(*self.args, **self.kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/joblib/parallel.py\", line 607, in __call__\n    return [func(*args, **kwargs) for func, args, kwargs in self.items]\n            ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/sklearn/utils/parallel.py\", line 139, in __call__\n    return self.function(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/sklearn/utils/_param_validation.py\", line 216, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/sklearn/model_selection/_validation.py\", line 1220, in cross_val_predict\n    cv = check_cv(cv, y, classifier=is_classifier(estimator))\n                                    ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/sklearn/base.py\", line 1237, in is_classifier\n    return get_tags(estimator).estimator_type == \"classifier\"\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/sklearn/utils/_tags.py\", line 430, in get_tags\n    sklearn_tags_provider[klass] = klass.__sklearn_tags__(estimator)  # type: ignore[attr-defined]\n                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/sklearn/base.py\", line 540, in __sklearn_tags__\n    tags = super().__sklearn_tags__()\n           ^^^^^^^^^^^^^^^^^^^^^^^^\nAttributeError: 'super' object has no attribute '__sklearn_tags__'\n\"\"\"",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3753878016.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;31m# 4. Train the ensemble model on the full training dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nTraining Stacking Ensemble Model...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m \u001b[0mstacking_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_scaled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"✓ Stacking Ensemble Model trained.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/ensemble/_stacking.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, **fit_params)\u001b[0m\n\u001b[1;32m    715\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    716\u001b[0m             \u001b[0mfit_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"sample_weight\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 717\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_encoded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    718\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    719\u001b[0m     @available_if(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1387\u001b[0m                 )\n\u001b[1;32m   1388\u001b[0m             ):\n\u001b[0;32m-> 1389\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/ensemble/_stacking.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    252\u001b[0m                 \u001b[0mcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRandomState\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 254\u001b[0;31m             predictions = Parallel(n_jobs=self.n_jobs)(\n\u001b[0m\u001b[1;32m    255\u001b[0m                 delayed(cross_val_predict)(\n\u001b[1;32m    256\u001b[0m                     \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdelayed_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         )\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable_with_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   2070\u001b[0m         \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2071\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2072\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_generator\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2073\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2074\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1680\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1681\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1682\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_retrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1683\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1684\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mGeneratorExit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1782\u001b[0m             \u001b[0;31m# worker traceback.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_aborting\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1784\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_error_fast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_raise_error_fast\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1857\u001b[0m         \u001b[0;31m# called directly or if the generator is gc'ed.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1858\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0merror_job\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1859\u001b[0;31m             \u001b[0merror_job\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1860\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1861\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_warn_exit_early\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mget_result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    756\u001b[0m             \u001b[0;31m# callback thread, and is stored internally. It's just waiting to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    757\u001b[0m             \u001b[0;31m# be returned.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 758\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_return_or_raise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    759\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    760\u001b[0m         \u001b[0;31m# For other backends, the main thread needs to run the retrieval step.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_return_or_raise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    771\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    772\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mTASK_ERROR\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 773\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    774\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    775\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'super' object has no attribute '__sklearn_tags__'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "de4dea6c"
      },
      "source": [
        "## Adjust threshold\n",
        "\n",
        "### Subtask:\n",
        "Adjust the classification threshold for the best performing model based on the threshold analysis performed in the previous steps to optimize for different business objectives (e.g., maximizing recall for identifying risky loans).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7476d2c4"
      },
      "source": [
        "**Reasoning**:\n",
        "Identify the best performing model, retrieve optimal thresholds from analysis_data, choose a threshold based on the business objective (maximizing recall), apply the chosen threshold to make predictions on the test set using the best model's probabilities, and evaluate the model's performance with the new threshold by calculating and printing the classification report and confusion matrix, and finally compare it to the default threshold results.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3af94ba2",
        "outputId": "5a5f0d6a-930e-4b49-b8a4-af9070955ca8"
      },
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import numpy as np\n",
        "\n",
        "# 1. Identify the best performing model based on CV Macro-F1\n",
        "best_model_name = None\n",
        "best_macro_f1_cv = -1\n",
        "\n",
        "for model_name, cv_results in all_cv_results.items():\n",
        "    # Consider only the main models evaluated, not intermediate architectures or balancing variants in this step\n",
        "    if model_name in ['Logistic Regression', 'XGBoost', 'ANN']:\n",
        "        if cv_results['macro_f1_mean'] > best_macro_f1_cv:\n",
        "            best_macro_f1_cv = cv_results['macro_f1_mean']\n",
        "            best_model_name = model_name\n",
        "\n",
        "print(f\"Best model based on CV Macro-F1: {best_model_name}\")\n",
        "\n",
        "# Get the best model and its test set predictions from the all_models dictionary\n",
        "best_model_data = all_models[best_model_name]\n",
        "y_true_test = best_model_data['y_true']\n",
        "y_pred_proba_test = best_model_data['y_pred_proba']\n",
        "\n",
        "# 2. Access the analysis_data dictionary to retrieve optimal thresholds\n",
        "if 'optimal_thresholds' in analysis_data:\n",
        "    opt_thresh = analysis_data['optimal_thresholds']\n",
        "\n",
        "    # 3. Choose a threshold based on the desired business objective\n",
        "    # For this example, let's prioritize maximizing recall (identifying risky loans)\n",
        "    chosen_threshold = opt_thresh['optimal_recall']['threshold']\n",
        "    objective = 'Maximizing Recall'\n",
        "    print(f\"\\nBusiness objective: {objective}\")\n",
        "    print(f\"Chosen threshold: {chosen_threshold:.3f}\")\n",
        "\n",
        "    # 4. Use the chosen threshold to make binary predictions on the test set\n",
        "    y_pred_test_tuned_threshold = (y_pred_proba_test > chosen_threshold).astype(int)\n",
        "\n",
        "    # 5. Evaluate the performance of the best model on the test set using the chosen threshold\n",
        "    print(f\"\\n{'-'*80}\")\n",
        "    print(f\"Final Test Set Results for {best_model_name} (Threshold = {chosen_threshold:.3f})\")\n",
        "    print(f\"{'-'*80}\")\n",
        "\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(classification_report(\n",
        "        y_true_test, y_pred_test_tuned_threshold,\n",
        "        target_names=['Fully Paid (0)', 'Charged Off (1)'],\n",
        "        zero_division=0\n",
        "    ))\n",
        "\n",
        "    print(\"Confusion Matrix:\")\n",
        "    cm_tuned = confusion_matrix(y_true_test, y_pred_test_tuned_threshold)\n",
        "    cm_tuned_df = pd.DataFrame(\n",
        "        cm_tuned,\n",
        "        index=['Actual Fully Paid', 'Actual Charged Off'],\n",
        "        columns=['Pred Fully Paid', 'Pred Charged Off']\n",
        "    )\n",
        "    print(cm_tuned_df)\n",
        "    print()\n",
        "\n",
        "    # 6. Compare to the default threshold results (already printed in STEP 7)\n",
        "    print(\"Comparison to Default Threshold (0.5) results can be seen in the output of STEP 7.\")\n",
        "\n",
        "else:\n",
        "    print(\"\\nanalysis_data with optimal thresholds not found. Cannot perform threshold adjustment analysis.\")"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best model based on CV Macro-F1: ANN\n",
            "\n",
            "Business objective: Maximizing Recall\n",
            "Chosen threshold: 0.000\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Final Test Set Results for ANN (Threshold = 0.000)\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Classification Report:\n",
            "                 precision    recall  f1-score   support\n",
            "\n",
            " Fully Paid (0)       0.00      0.00      0.00      2211\n",
            "Charged Off (1)       0.50      1.00      0.67      2211\n",
            "\n",
            "       accuracy                           0.50      4422\n",
            "      macro avg       0.25      0.50      0.33      4422\n",
            "   weighted avg       0.25      0.50      0.33      4422\n",
            "\n",
            "Confusion Matrix:\n",
            "                    Pred Fully Paid  Pred Charged Off\n",
            "Actual Fully Paid                 0              2211\n",
            "Actual Charged Off                0              2211\n",
            "\n",
            "Comparison to Default Threshold (0.5) results can be seen in the output of STEP 7.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "996df026"
      },
      "source": [
        "## Evaluate other models\n",
        "\n",
        "### Subtask:\n",
        "Evaluate other suitable classification algorithms like Support Vector Machines (SVM), Random Forests, or Gradient Boosting Machines (other than XGBoost).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dbcc1f89"
      },
      "source": [
        "**Reasoning**:\n",
        "Import the necessary libraries for the new models and instantiate them.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b87d1ea8"
      },
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "\n",
        "# Instantiate the new models\n",
        "svc_model = SVC(random_state=42, probability=True)\n",
        "rf_model = RandomForestClassifier(random_state=42)\n",
        "gb_model = GradientBoostingClassifier(random_state=42)"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3c9add3a"
      },
      "source": [
        "**Reasoning**:\n",
        "Train and evaluate each of the new models using cross-validation and store the results.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0c8e8e68",
        "outputId": "3fbcaf91-861b-4b4d-b8b4-628b59dba389"
      },
      "source": [
        "print(\"\\nTraining Support Vector Machine with 5-fold CV...\")\n",
        "svc_cv_results, svc_fold_details = train_and_evaluate_with_cv(\n",
        "    svc_model, X_train_scaled, y_train,\n",
        "    cv_folds=5, random_state=42, model_name='SVC', verbose=True\n",
        ")\n",
        "all_cv_results['SVC'] = svc_cv_results\n",
        "\n",
        "# Train final model on full training set for predictions\n",
        "svc_final = SVC(random_state=42, probability=True)\n",
        "svc_final.fit(X_train_scaled, y_train)\n",
        "svc_test_proba = svc_final.predict_proba(X_test_scaled)[:, 1]\n",
        "all_models['SVC'] = {\n",
        "    'model': svc_final,\n",
        "    'y_true': y_test,\n",
        "    'y_pred_proba': svc_test_proba,\n",
        "    'cv_results': svc_cv_results\n",
        "}\n",
        "\n",
        "\n",
        "print(\"\\nTraining Random Forest with 5-fold CV...\")\n",
        "rf_cv_results, rf_fold_details = train_and_evaluate_with_cv(\n",
        "    rf_model, X_train_scaled, y_train,\n",
        "    cv_folds=5, random_state=42, model_name='Random Forest', verbose=True\n",
        ")\n",
        "all_cv_results['Random Forest'] = rf_cv_results\n",
        "\n",
        "# Train final model on full training set for predictions\n",
        "rf_final = RandomForestClassifier(random_state=42)\n",
        "rf_final.fit(X_train_scaled, y_train)\n",
        "rf_test_proba = rf_final.predict_proba(X_test_scaled)[:, 1]\n",
        "all_models['Random Forest'] = {\n",
        "    'model': rf_final,\n",
        "    'y_true': y_test,\n",
        "    'y_pred_proba': rf_test_proba,\n",
        "    'cv_results': rf_cv_results\n",
        "}\n",
        "\n",
        "print(\"\\nTraining Gradient Boosting with 5-fold CV...\")\n",
        "gb_cv_results, gb_fold_details = train_and_evaluate_with_cv(\n",
        "    gb_model, X_train_scaled, y_train,\n",
        "    cv_folds=5, random_state=42, model_name='Gradient Boosting', verbose=True\n",
        ")\n",
        "all_cv_results['Gradient Boosting'] = gb_cv_results\n",
        "\n",
        "# Train final model on full training set for predictions\n",
        "gb_final = GradientBoostingClassifier(random_state=42)\n",
        "gb_final.fit(X_train_scaled, y_train)\n",
        "gb_test_proba = gb_final.predict_proba(X_test_scaled)[:, 1]\n",
        "all_models['Gradient Boosting'] = {\n",
        "    'model': gb_final,\n",
        "    'y_true': y_test,\n",
        "    'y_pred_proba': gb_test_proba,\n",
        "    'cv_results': gb_cv_results\n",
        "}\n",
        "\n",
        "print(\"\\nAll new models trained successfully.\")"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training Support Vector Machine with 5-fold CV...\n",
            "\n",
            "--- SVC Cross-Validation ---\n",
            "  Fold 1 completed in 71.69 seconds.\n",
            "    Accuracy: 0.6382, Macro-F1: 0.6380, Recall (Class 1): 0.6648, ROC-AUC: 0.6821\n",
            "  Fold 2 completed in 67.70 seconds.\n",
            "    Accuracy: 0.6316, Macro-F1: 0.6315, Recall (Class 1): 0.6516, ROC-AUC: 0.6695\n",
            "  Fold 3 completed in 68.33 seconds.\n",
            "    Accuracy: 0.6401, Macro-F1: 0.6399, Recall (Class 1): 0.6652, ROC-AUC: 0.6871\n",
            "  Fold 4 completed in 66.21 seconds.\n",
            "    Accuracy: 0.6251, Macro-F1: 0.6240, Recall (Class 1): 0.5698, ROC-AUC: 0.6759\n",
            "  Fold 5 completed in 67.46 seconds.\n",
            "    Accuracy: 0.6370, Macro-F1: 0.6365, Recall (Class 1): 0.6716, ROC-AUC: 0.6771\n",
            "\n",
            "--- Average Results for SVC ---\n",
            "  Accuracy: 0.6344 ± 0.0054\n",
            "  Macro-F1: 0.6340 ± 0.0057\n",
            "  Recall (Class 1): 0.6446 ± 0.0379\n",
            "  ROC-AUC: 0.6783 ± 0.0059\n",
            "-------------------------------\n",
            "\n",
            "Training Random Forest with 5-fold CV...\n",
            "\n",
            "--- Random Forest Cross-Validation ---\n",
            "  Fold 1 completed in 2.70 seconds.\n",
            "    Accuracy: 0.6009, Macro-F1: 0.6009, Recall (Class 1): 0.6066, ROC-AUC: 0.6477\n",
            "  Fold 2 completed in 2.00 seconds.\n",
            "    Accuracy: 0.6011, Macro-F1: 0.6010, Recall (Class 1): 0.6160, ROC-AUC: 0.6463\n",
            "  Fold 3 completed in 1.99 seconds.\n",
            "    Accuracy: 0.6141, Macro-F1: 0.6141, Recall (Class 1): 0.6160, ROC-AUC: 0.6522\n",
            "  Fold 4 completed in 1.97 seconds.\n",
            "    Accuracy: 0.6172, Macro-F1: 0.6172, Recall (Class 1): 0.6071, ROC-AUC: 0.6561\n",
            "  Fold 5 completed in 1.99 seconds.\n",
            "    Accuracy: 0.6245, Macro-F1: 0.6245, Recall (Class 1): 0.6337, ROC-AUC: 0.6664\n",
            "\n",
            "--- Average Results for Random Forest ---\n",
            "  Accuracy: 0.6116 ± 0.0093\n",
            "  Macro-F1: 0.6115 ± 0.0093\n",
            "  Recall (Class 1): 0.6159 ± 0.0098\n",
            "  ROC-AUC: 0.6537 ± 0.0072\n",
            "-----------------------------------------\n",
            "\n",
            "Training Gradient Boosting with 5-fold CV...\n",
            "\n",
            "--- Gradient Boosting Cross-Validation ---\n",
            "  Fold 1 completed in 2.27 seconds.\n",
            "    Accuracy: 0.6518, Macro-F1: 0.6517, Recall (Class 1): 0.6670, ROC-AUC: 0.6989\n",
            "  Fold 2 completed in 1.79 seconds.\n",
            "    Accuracy: 0.6319, Macro-F1: 0.6316, Recall (Class 1): 0.6606, ROC-AUC: 0.6832\n",
            "  Fold 3 completed in 1.78 seconds.\n",
            "    Accuracy: 0.6525, Macro-F1: 0.6525, Recall (Class 1): 0.6589, ROC-AUC: 0.7096\n",
            "  Fold 4 completed in 1.79 seconds.\n",
            "    Accuracy: 0.6392, Macro-F1: 0.6392, Recall (Class 1): 0.6495, ROC-AUC: 0.6843\n",
            "  Fold 5 completed in 1.82 seconds.\n",
            "    Accuracy: 0.6409, Macro-F1: 0.6409, Recall (Class 1): 0.6546, ROC-AUC: 0.7032\n",
            "\n",
            "--- Average Results for Gradient Boosting ---\n",
            "  Accuracy: 0.6433 ± 0.0079\n",
            "  Macro-F1: 0.6432 ± 0.0079\n",
            "  Recall (Class 1): 0.6581 ± 0.0059\n",
            "  ROC-AUC: 0.6959 ± 0.0105\n",
            "---------------------------------------------\n",
            "\n",
            "All new models trained successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RptFDxltEImn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}